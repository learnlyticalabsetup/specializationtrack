<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Azure Practical Assignment Solutions - Complete Guide</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            background: linear-gradient(135deg, #0078d4, #106ebe, #005a9e);
            min-height: 100vh;
            color: #333;
            line-height: 1.6;
        }

        .header {
            background: rgba(255, 255, 255, 0.95);
            padding: 20px 0;
            text-align: center;
            box-shadow: 0 4px 20px rgba(0,0,0,0.1);
            backdrop-filter: blur(10px);
            margin-bottom: 30px;
        }

        .container {
            max-width: 1400px;
            margin: 0 auto;
            padding: 0 20px;
        }

        h1 {
            color: #0078d4;
            font-size: 2.5em;
            margin-bottom: 10px;
        }

        .subtitle {
            color: #666;
            font-size: 1.2em;
            margin-bottom: 20px;
        }

        .nav-pills {
            display: flex;
            background: white;
            border-radius: 15px;
            padding: 5px;
            margin-bottom: 30px;
            box-shadow: 0 4px 15px rgba(0,0,0,0.1);
            overflow-x: auto;
        }

        .nav-pill {
            padding: 12px 20px;
            border-radius: 10px;
            background: transparent;
            border: none;
            cursor: pointer;
            font-weight: bold;
            transition: all 0.3s ease;
            white-space: nowrap;
            margin: 2px;
        }

        .nav-pill.active {
            background: #0078d4;
            color: white;
        }

        .nav-pill:hover {
            background: #e1f5fe;
        }

        .solution-card {
            background: white;
            border-radius: 15px;
            padding: 30px;
            margin-bottom: 30px;
            box-shadow: 0 8px 25px rgba(0,0,0,0.15);
            display: none;
        }

        .solution-card.active {
            display: block;
        }

        .solution-header {
            border-bottom: 3px solid #0078d4;
            padding-bottom: 20px;
            margin-bottom: 30px;
        }

        .solution-title {
            font-size: 2em;
            font-weight: bold;
            color: #0078d4;
            margin-bottom: 10px;
        }

        .solution-meta {
            display: flex;
            gap: 20px;
            flex-wrap: wrap;
        }

        .meta-tag {
            background: #e1f5fe;
            color: #0078d4;
            padding: 8px 16px;
            border-radius: 20px;
            font-size: 0.9em;
            font-weight: bold;
        }

        .step-section {
            margin-bottom: 40px;
        }

        .step-header {
            background: #f8f9fa;
            padding: 15px 20px;
            border-radius: 10px;
            margin-bottom: 20px;
            border-left: 5px solid #0078d4;
        }

        .step-title {
            font-size: 1.4em;
            font-weight: bold;
            color: #2c3e50;
            margin-bottom: 5px;
        }

        .step-description {
            color: #666;
            font-size: 1.1em;
        }

        .step-content {
            padding-left: 20px;
        }

        .substep {
            background: white;
            border: 1px solid #e0e0e0;
            border-radius: 8px;
            padding: 20px;
            margin-bottom: 15px;
            position: relative;
        }

        .substep-number {
            position: absolute;
            top: -10px;
            left: 20px;
            background: #0078d4;
            color: white;
            width: 25px;
            height: 25px;
            border-radius: 50%;
            display: flex;
            align-items: center;
            justify-content: center;
            font-weight: bold;
            font-size: 0.9em;
        }

        .substep-title {
            font-weight: bold;
            color: #2c3e50;
            margin-bottom: 10px;
            margin-top: 5px;
        }

        .code-block {
            background: #2d3748;
            color: #e2e8f0;
            padding: 20px;
            border-radius: 8px;
            font-family: 'Courier New', monospace;
            font-size: 0.9em;
            overflow-x: auto;
            margin: 15px 0;
            border-left: 4px solid #0078d4;
        }

        .command {
            background: #1a202c;
            color: #68d391;
            padding: 8px 12px;
            border-radius: 4px;
            font-family: 'Courier New', monospace;
            font-size: 0.9em;
            display: inline-block;
            margin: 5px 0;
        }

        .warning {
            background: #fff3cd;
            border: 1px solid #ffeaa7;
            color: #856404;
            padding: 15px;
            border-radius: 8px;
            margin: 15px 0;
            border-left: 4px solid #f39c12;
        }

        .tip {
            background: #d1ecf1;
            border: 1px solid #bee5eb;
            color: #0c5460;
            padding: 15px;
            border-radius: 8px;
            margin: 15px 0;
            border-left: 4px solid #17a2b8;
        }

        .success {
            background: #d4edda;
            border: 1px solid #c3e6cb;
            color: #155724;
            padding: 15px;
            border-radius: 8px;
            margin: 15px 0;
            border-left: 4px solid #28a745;
        }

        .screenshot-placeholder {
            background: #f8f9fa;
            border: 2px dashed #dee2e6;
            padding: 40px;
            text-align: center;
            color: #6c757d;
            border-radius: 8px;
            margin: 15px 0;
        }

        .resource-links {
            background: #f8f9fa;
            padding: 20px;
            border-radius: 8px;
            margin-top: 30px;
        }

        .resource-links h4 {
            color: #2c3e50;
            margin-bottom: 15px;
        }

        .resource-links ul {
            list-style: none;
            padding: 0;
        }

        .resource-links li {
            margin-bottom: 8px;
        }

        .resource-links a {
            color: #0078d4;
            text-decoration: none;
            font-weight: 500;
        }

        .resource-links a:hover {
            text-decoration: underline;
        }

        .time-estimate {
            background: #e8f4fd;
            padding: 15px;
            border-radius: 8px;
            margin-bottom: 20px;
            border-left: 4px solid #3498db;
        }

        .prerequisites {
            background: #f1f3f4;
            padding: 20px;
            border-radius: 8px;
            margin-bottom: 30px;
        }

        .prerequisites h4 {
            color: #2c3e50;
            margin-bottom: 15px;
        }

        .prerequisites ul {
            margin-left: 20px;
        }

        .prerequisites li {
            margin-bottom: 8px;
        }

        .validation-section {
            background: #f8f9fa;
            padding: 20px;
            border-radius: 8px;
            margin-top: 30px;
            border-left: 4px solid #28a745;
        }

        .troubleshooting {
            background: #fff5f5;
            padding: 20px;
            border-radius: 8px;
            margin-top: 20px;
            border-left: 4px solid #e53e3e;
        }

        @media (max-width: 768px) {
            .solution-meta {
                flex-direction: column;
            }
            
            .nav-pills {
                flex-direction: column;
                gap: 5px;
            }
            
            .substep {
                padding: 15px;
            }
        }
    </style>
</head>
<body>
    <div class="header">
        <div class="container">
            <h1>🎯 Azure Practical Assignment Solutions</h1>
            <p class="subtitle">Complete Step-by-Step Guide for Beginners</p>
        </div>
    </div>

    <div class="container">
        <!-- Navigation Pills -->
        <div class="nav-pills">
            <button class="nav-pill active" onclick="showSolution(1)">Assignment 1: Identity Management</button>
            <button class="nav-pill" onclick="showSolution(2)">Assignment 2: Virtual Networks</button>
            <button class="nav-pill" onclick="showSolution(3)">Assignment 3: Storage Solutions</button>
            <button class="nav-pill" onclick="showSolution(4)">Assignment 4: App Services</button>
            <button class="nav-pill" onclick="showSolution(5)">Assignment 5: Database Setup</button>
            <button class="nav-pill" onclick="showSolution(6)">Assignment 6: Key Vault</button>
            <button class="nav-pill" onclick="showSolution(7)">Assignment 7: Monitor & Alerts</button>
            <button class="nav-pill" onclick="showSolution(8)">Assignment 8: Backup Solutions</button>
            <button class="nav-pill" onclick="showSolution(9)">Assignment 9: Load Balancer</button>
            <button class="nav-pill" onclick="showSolution(10)">Assignment 10: CDN Setup</button>
            <button class="nav-pill" onclick="showSolution(11)">Assignment 11: DevOps Pipeline</button>
            <button class="nav-pill" onclick="showSolution(12)">Assignment 12: Container Apps</button>
            <button class="nav-pill" onclick="showSolution(13)">Assignment 13: Kubernetes</button>
            <button class="nav-pill" onclick="showSolution(14)">Assignment 14: Functions</button>
            <button class="nav-pill" onclick="showSolution(15)">Assignment 15: Logic Apps</button>
            <button class="nav-pill" onclick="showSolution(16)">Assignment 16: Event Grid</button>
            <button class="nav-pill" onclick="showSolution(17)">Assignment 17: API Management</button>
            <button class="nav-pill" onclick="showSolution(18)">Assignment 18: Service Bus</button>
            <button class="nav-pill" onclick="showSolution(19)">Assignment 19: Cosmos DB</button>
            <button class="nav-pill" onclick="showSolution(20)">Assignment 20: Data Factory</button>
            <button class="nav-pill" onclick="showSolution(21)">Assignment 21: Synapse Analytics</button>
            <button class="nav-pill" onclick="showSolution(22)">Assignment 22: Cosmos DB</button>
            <button class="nav-pill" onclick="showSolution(23)">Assignment 23: Redis Cache</button>
            <button class="nav-pill" onclick="showSolution(24)">Assignment 24: Search Service</button>
            <button class="nav-pill" onclick="showSolution(25)">Assignment 25: Machine Learning</button>
            <button class="nav-pill" onclick="showSolution(26)">Assignment 26: Cognitive Services</button>
            <button class="nav-pill" onclick="showSolution(27)">Assignment 27: Bot Framework</button>
            <button class="nav-pill" onclick="showSolution(28)">Assignment 28: IoT Hub</button>
            <button class="nav-pill" onclick="showSolution(29)">Assignment 29: Stream Analytics</button>
            <button class="nav-pill" onclick="showSolution(30)">Assignment 30: Power BI</button>
            <button class="nav-pill" onclick="showSolution(31)">Assignment 31: Cost Management</button>
            <button class="nav-pill" onclick="showSolution(32)">Assignment 32: Governance</button>
            <button class="nav-pill" onclick="showSolution(33)">Assignment 33: Security Center</button>
        </div>

        <!-- Assignment 1: Identity and Access Management -->
        <div id="solution1" class="solution-card active">
            <div class="solution-header">
                <div class="solution-title">Assignment 1: Azure Active Directory Identity Management</div>
                <div class="solution-meta">
                    <span class="meta-tag">🔐 Identity & Governance</span>
                    <span class="meta-tag">⏱️ 120 minutes</span>
                    <span class="meta-tag">📊 Beginner Friendly</span>
                </div>
            </div>

            <div class="time-estimate">
                <strong>⏱️ Estimated Time:</strong> 2 hours<br>
                <strong>🎯 Learning Objective:</strong> Set up comprehensive identity management for enterprise users
            </div>

            <div class="prerequisites">
                <h4>📋 Prerequisites</h4>
                <ul>
                    <li>Active Azure subscription (Free tier is sufficient)</li>
                    <li>Global Administrator access to Azure AD tenant</li>
                    <li>Basic understanding of user management concepts</li>
                    <li>Web browser (Chrome, Edge, or Firefox recommended)</li>
                </ul>
            </div>

            <!-- Step 1 -->
            <div class="step-section">
                <div class="step-header">
                    <div class="step-title">Step 1: Setting Up Azure Active Directory</div>
                    <div class="step-description">Create and configure your Azure AD tenant for the organization</div>
                </div>
                <div class="step-content">
                    <div class="substep">
                        <div class="substep-number">1</div>
                        <div class="substep-title">Access Azure Portal</div>
                        <p>Open your web browser and navigate to the Azure Portal:</p>
                        <div class="command">https://portal.azure.com</div>
                        <p>Sign in with your Azure account credentials.</p>
                        
                        <div class="tip">
                            <strong>💡 Beginner Tip:</strong> If you don't have an Azure account, you can create a free one at https://azure.microsoft.com/free/. The free tier includes $200 credit for 30 days.
                        </div>
                    </div>

                    <div class="substep">
                        <div class="substep-number">2</div>
                        <div class="substep-title">Navigate to Azure Active Directory</div>
                        <p>In the Azure Portal:</p>
                        <ul>
                            <li>Click on the menu button (☰) in the top-left corner</li>
                            <li>Search for "Azure Active Directory" in the search box</li>
                            <li>Click on "Azure Active Directory" from the results</li>
                        </ul>
                        
                        <div class="screenshot-placeholder">
                            📸 Screenshot: Azure Portal dashboard with Azure AD highlighted
                        </div>
                    </div>

                    <div class="substep">
                        <div class="substep-number">3</div>
                        <div class="substep-title">Verify Tenant Information</div>
                        <p>In the Azure AD Overview page, verify:</p>
                        <ul>
                            <li><strong>Tenant ID:</strong> Note down your unique tenant identifier</li>
                            <li><strong>Domain Name:</strong> Your default domain (e.g., yourname.onmicrosoft.com)</li>
                            <li><strong>License Type:</strong> Should show "Free" or "Premium" based on your subscription</li>
                        </ul>
                        
                        <div class="warning">
                            <strong>⚠️ Important:</strong> Keep your Tenant ID secure as it's used to identify your organization in Azure AD.
                        </div>
                    </div>
                </div>
            </div>

            <!-- Step 2 -->
            <div class="step-section">
                <div class="step-header">
                    <div class="step-title">Step 2: Creating User Accounts</div>
                    <div class="step-description">Set up individual user accounts for enterprise employees</div>
                </div>
                <div class="step-content">
                    <div class="substep">
                        <div class="substep-number">1</div>
                        <div class="substep-title">Access Users Management</div>
                        <p>In the Azure AD dashboard:</p>
                        <ul>
                            <li>Click on "Users" in the left navigation menu</li>
                            <li>You'll see the "All users" page with existing users (probably just your admin account)</li>
                        </ul>
                    </div>

                    <div class="substep">
                        <div class="substep-number">2</div>
                        <div class="substep-title">Create New User - Marketing Manager</div>
                        <p>Let's create our first user:</p>
                        <ol>
                            <li>Click the "New user" button at the top</li>
                            <li>Select "Create user" from the dropdown</li>
                            <li>Fill in the user details:</li>
                        </ol>
                        
                        <div class="code-block">
User principal name: sarah.marketing@yourtenantname.onmicrosoft.com
Display name: Sarah Johnson
First name: Sarah
Last name: Johnson
Job title: Marketing Manager
Department: Marketing
Password: (Auto-generate or create a temporary password)
                        </div>
                        
                        <p>Click "Create" to save the user.</p>
                        
                        <div class="tip">
                            <strong>💡 Best Practice:</strong> Always use a consistent naming convention for user accounts (e.g., firstname.department@domain.com).
                        </div>
                    </div>

                    <div class="substep">
                        <div class="substep-number">3</div>
                        <div class="substep-title">Create Additional Users</div>
                        <p>Repeat the process to create these additional users:</p>
                        
                        <div class="code-block">
User 2:
  Name: john.it@yourtenantname.onmicrosoft.com
  Display: John Smith
  Department: IT
  Job Title: IT Administrator

User 3:
  Name: mike.sales@yourtenantname.onmicrosoft.com
  Display: Mike Davis
  Department: Sales
  Job Title: Sales Representative

User 4:
  Name: lisa.hr@yourtenantname.onmicrosoft.com
  Display: Lisa Wilson
  Department: HR
  Job Title: HR Specialist
                        </div>
                    </div>
                </div>
            </div>

            <!-- Step 3 -->
            <div class="step-section">
                <div class="step-header">
                    <div class="step-title">Step 3: Creating Security Groups</div>
                    <div class="step-description">Organize users into logical groups for easier permission management</div>
                </div>
                <div class="step-content">
                    <div class="substep">
                        <div class="substep-number">1</div>
                        <div class="substep-title">Navigate to Groups</div>
                        <p>In Azure AD:</p>
                        <ul>
                            <li>Click on "Groups" in the left navigation menu</li>
                            <li>Click "New group" to create your first group</li>
                        </ul>
                    </div>

                    <div class="substep">
                        <div class="substep-number">2</div>
                        <div class="substep-title">Create Marketing Department Group</div>
                        <p>Configure the group settings:</p>
                        
                        <div class="code-block">
Group type: Security
Group name: Marketing-Department
Group description: All users in the Marketing department
Membership type: Assigned
Owners: (Select your admin account)
Members: (Add sarah.marketing@yourtenantname.onmicrosoft.com)
                        </div>
                        
                        <p>Click "Create" to save the group.</p>
                    </div>

                    <div class="substep">
                        <div class="substep-number">3</div>
                        <div class="substep-title">Create Additional Department Groups</div>
                        <p>Create these additional groups:</p>
                        
                        <div class="code-block">
Group 1: IT-Department
  Members: john.it@yourtenantname.onmicrosoft.com
  Description: IT Department staff with administrative access

Group 2: Sales-Department  
  Members: mike.sales@yourtenantname.onmicrosoft.com
  Description: Sales team members

Group 3: HR-Department
  Members: lisa.hr@yourtenantname.onmicrosoft.com
  Description: Human Resources department staff
                        </div>
                    </div>
                </div>
            </div>

            <!-- Step 4 -->
            <div class="step-section">
                <div class="step-header">
                    <div class="step-title">Step 4: Configuring Multi-Factor Authentication (MFA)</div>
                    <div class="step-description">Enhance security by enabling MFA for all users</div>
                </div>
                <div class="step-content">
                    <div class="substep">
                        <div class="substep-number">1</div>
                        <div class="substep-title">Access Security Settings</div>
                        <p>In Azure AD:</p>
                        <ul>
                            <li>Click on "Security" in the left navigation menu</li>
                            <li>Click on "MFA" under "Manage"</li>
                            <li>Select "Getting started"</li>
                        </ul>
                    </div>

                    <div class="substep">
                        <div class="substep-number">2</div>
                        <div class="substep-title">Configure MFA Settings</div>
                        <p>Set up MFA requirements:</p>
                        <ol>
                            <li>Click on "Additional cloud-based MFA settings"</li>
                            <li>Under "Verification options", enable:
                                <ul>
                                    <li>✅ Call to phone</li>
                                    <li>✅ Text message to phone</li>
                                    <li>✅ Mobile app notification</li>
                                </ul>
                            </li>
                            <li>Click "Save"</li>
                        </ol>
                    </div>

                    <div class="substep">
                        <div class="substep-number">3</div>
                        <div class="substep-title">Enable MFA for Users</div>
                        <p>Enable MFA for specific users:</p>
                        <ol>
                            <li>Go to the "Users" tab in MFA settings</li>
                            <li>Select all the users you created</li>
                            <li>Click "Enable" in the quick steps panel</li>
                            <li>Confirm the action</li>
                        </ol>
                        
                        <div class="success">
                            <strong>✅ Success Indicator:</strong> Users should now show "Enabled" status in the MFA column.
                        </div>
                    </div>
                </div>
            </div>

            <!-- Validation Section -->
            <div class="validation-section">
                <h4>🔍 Validation & Testing</h4>
                <ol>
                    <li><strong>Verify User Creation:</strong> Go to Azure AD > Users and confirm all 4 users are listed</li>
                    <li><strong>Check Group Membership:</strong> Go to Groups and verify each group has the correct members</li>
                    <li><strong>Test MFA:</strong> Try logging in as one of the created users to verify MFA prompt appears</li>
                    <li><strong>Verify Permissions:</strong> Ensure users can access only their designated resources</li>
                </ol>
            </div>

            <div class="troubleshooting">
                <h4>🛠️ Common Issues & Solutions</h4>
                <ul>
                    <li><strong>Can't create users:</strong> Verify you have Global Administrator role</li>
                    <li><strong>MFA not working:</strong> Check if Conditional Access policies are blocking</li>
                    <li><strong>Groups not showing:</strong> Refresh the page and wait a few minutes for sync</li>
                    <li><strong>Domain issues:</strong> Use the default .onmicrosoft.com domain for testing</li>
                </ul>
            </div>

            <div class="resource-links">
                <h4>📚 Additional Resources</h4>
                <ul>
                    <li><a href="https://docs.microsoft.com/en-us/azure/active-directory/" target="_blank">Azure AD Official Documentation</a></li>
                    <li><a href="https://docs.microsoft.com/en-us/azure/active-directory/authentication/howto-mfa-getstarted" target="_blank">MFA Setup Guide</a></li>
                    <li><a href="https://docs.microsoft.com/en-us/azure/active-directory/fundamentals/active-directory-groups-create-azure-portal" target="_blank">Groups Management</a></li>
                </ul>
            </div>
        </div>

        <!-- Assignment 2: Virtual Network Configuration -->
        <div id="solution2" class="solution-card">
            <div class="solution-header">
                <div class="solution-title">Assignment 2: Azure Virtual Network Configuration</div>
                <div class="solution-meta">
                    <span class="meta-tag">🌐 Networking</span>
                    <span class="meta-tag">⏱️ 90 minutes</span>
                    <span class="meta-tag">📊 Intermediate</span>
                </div>
            </div>

            <div class="time-estimate">
                <strong>⏱️ Estimated Time:</strong> 1.5 hours<br>
                <strong>🎯 Learning Objective:</strong> Create secure network infrastructure with subnets and security groups
            </div>

            <div class="prerequisites">
                <h4>📋 Prerequisites</h4>
                <ul>
                    <li>Completed Assignment 1 (Azure AD setup)</li>
                    <li>Basic networking concepts knowledge (IP addresses, subnets)</li>
                    <li>Understanding of network security principles</li>
                </ul>
            </div>

            <!-- Step 1 -->
            <div class="step-section">
                <div class="step-header">
                    <div class="step-title">Step 1: Creating a Virtual Network</div>
                    <div class="step-description">Set up the foundational network infrastructure</div>
                </div>
                <div class="step-content">
                    <div class="substep">
                        <div class="substep-number">1</div>
                        <div class="substep-title">Navigate to Virtual Networks</div>
                        <p>In the Azure Portal:</p>
                        <ul>
                            <li>Click "Create a resource" (+) in the top-left</li>
                            <li>Search for "Virtual Network"</li>
                            <li>Click "Virtual Network" from Microsoft</li>
                            <li>Click "Create"</li>
                        </ul>
                    </div>

                    <div class="substep">
                        <div class="substep-number">2</div>
                        <div class="substep-title">Configure Basic Settings</div>
                        <p>Fill in the basic configuration:</p>
                        
                        <div class="code-block">
Subscription: (Your Azure subscription)
Resource Group: Create new "NetworkingRG"
Name: Enterprise-VNet
Region: East US (or your preferred region)
                        </div>
                        
                        <div class="tip">
                            <strong>💡 Naming Convention:</strong> Use descriptive names that indicate the purpose and environment (e.g., Enterprise-VNet-Prod).
                        </div>
                    </div>

                    <div class="substep">
                        <div class="substep-number">3</div>
                        <div class="substep-title">Configure IP Address Space</div>
                        <p>Set up the network address range:</p>
                        
                        <div class="code-block">
IPv4 address space: 10.0.0.0/16
                        </div>
                        
                        <p>This provides 65,536 IP addresses for your network.</p>
                        
                        <div class="warning">
                            <strong>⚠️ Important:</strong> Choose an address space that doesn't conflict with your on-premises network if you plan to connect them later.
                        </div>
                    </div>
                </div>
            </div>

            <!-- Step 2 -->
            <div class="step-section">
                <div class="step-header">
                    <div class="step-title">Step 2: Creating Subnets</div>
                    <div class="step-description">Divide the network into logical segments for different tiers</div>
                </div>
                <div class="step-content">
                    <div class="substep">
                        <div class="substep-number">1</div>
                        <div class="substep-title">Create Web Tier Subnet</div>
                        <p>In the VNet creation wizard, go to the "Security" tab, then "IP Addresses" tab:</p>
                        
                        <div class="code-block">
Subnet name: WebTier-Subnet
Subnet address range: 10.0.1.0/24
                        </div>
                        
                        <p>This provides 254 usable IP addresses for web servers.</p>
                    </div>

                    <div class="substep">
                        <div class="substep-number">2</div>
                        <div class="substep-title">Create Application Tier Subnet</div>
                        <p>Add another subnet:</p>
                        <ol>
                            <li>Click "Add subnet"</li>
                            <li>Configure the application tier:</li>
                        </ol>
                        
                        <div class="code-block">
Subnet name: AppTier-Subnet
Subnet address range: 10.0.2.0/24
                        </div>
                    </div>

                    <div class="substep">
                        <div class="substep-number">3</div>
                        <div class="substep-title">Create Database Tier Subnet</div>
                        <p>Add the database subnet:</p>
                        
                        <div class="code-block">
Subnet name: DatabaseTier-Subnet
Subnet address range: 10.0.3.0/24
                        </div>
                        
                        <p>Click "Review + Create" and then "Create" to deploy the VNet.</p>
                    </div>
                </div>
            </div>

            <!-- Step 3 -->
            <div class="step-section">
                <div class="step-header">
                    <div class="step-title">Step 3: Configuring Network Security Groups</div>
                    <div class="step-description">Create firewall rules to control traffic flow between subnets</div>
                </div>
                <div class="step-content">
                    <div class="substep">
                        <div class="substep-number">1</div>
                        <div class="substep-title">Create Web Tier NSG</div>
                        <p>Create the first Network Security Group:</p>
                        <ol>
                            <li>Go to "Create a resource" > Search "Network Security Group"</li>
                            <li>Configure:</li>
                        </ol>
                        
                        <div class="code-block">
Name: WebTier-NSG
Resource Group: NetworkingRG
Region: East US
                        </div>
                        
                        <p>Click "Review + Create" and then "Create".</p>
                    </div>

                    <div class="substep">
                        <div class="substep-number">2</div>
                        <div class="substep-title">Configure Web Tier Security Rules</div>
                        <p>Add security rules for the web tier:</p>
                        <ol>
                            <li>Go to the created NSG > "Inbound security rules"</li>
                            <li>Add these rules:</li>
                        </ol>
                        
                        <div class="code-block">
Rule 1 - Allow HTTP:
  Source: Any
  Source port ranges: *
  Destination: Any
  Destination port ranges: 80
  Protocol: TCP
  Action: Allow
  Priority: 100
  Name: Allow-HTTP

Rule 2 - Allow HTTPS:
  Source: Any
  Source port ranges: *
  Destination: Any
  Destination port ranges: 443
  Protocol: TCP
  Action: Allow
  Priority: 110
  Name: Allow-HTTPS
                        </div>
                    </div>

                    <div class="substep">
                        <div class="substep-number">3</div>
                        <div class="substep-title">Create and Configure App Tier NSG</div>
                        <p>Create NSG for application tier:</p>
                        
                        <div class="code-block">
Name: AppTier-NSG
Resource Group: NetworkingRG

Inbound Rules:
Rule 1 - Allow from Web Tier:
  Source: IP Addresses
  Source IP addresses: 10.0.1.0/24
  Destination port ranges: 8080
  Protocol: TCP
  Action: Allow
  Priority: 100
  Name: Allow-WebTier-to-AppTier
                        </div>
                    </div>

                    <div class="substep">
                        <div class="substep-number">4</div>
                        <div class="substep-title">Create Database Tier NSG</div>
                        <p>Create the most restrictive NSG for the database:</p>
                        
                        <div class="code-block">
Name: DatabaseTier-NSG
Resource Group: NetworkingRG

Inbound Rules:
Rule 1 - Allow from App Tier:
  Source: IP Addresses
  Source IP addresses: 10.0.2.0/24
  Destination port ranges: 1433
  Protocol: TCP
  Action: Allow
  Priority: 100
  Name: Allow-AppTier-to-Database
                        </div>
                    </div>
                </div>
            </div>

            <!-- Step 4 -->
            <div class="step-section">
                <div class="step-header">
                    <div class="step-title">Step 4: Associating NSGs with Subnets</div>
                    <div class="step-description">Link security groups to their respective subnets</div>
                </div>
                <div class="step-content">
                    <div class="substep">
                        <div class="substep-number">1</div>
                        <div class="substep-title">Associate Web Tier NSG</div>
                        <p>Link the Web NSG to its subnet:</p>
                        <ol>
                            <li>Go to WebTier-NSG > "Subnets" in the left menu</li>
                            <li>Click "Associate"</li>
                            <li>Select:
                                <ul>
                                    <li>Virtual network: Enterprise-VNet</li>
                                    <li>Subnet: WebTier-Subnet</li>
                                </ul>
                            </li>
                            <li>Click "OK"</li>
                        </ol>
                    </div>

                    <div class="substep">
                        <div class="substep-number">2</div>
                        <div class="substep-title">Associate Remaining NSGs</div>
                        <p>Repeat the process for the other NSGs:</p>
                        
                        <div class="code-block">
AppTier-NSG → AppTier-Subnet
DatabaseTier-NSG → DatabaseTier-Subnet
                        </div>
                        
                        <div class="success">
                            <strong>✅ Verification:</strong> Each NSG should show 1 associated subnet in its overview page.
                        </div>
                    </div>
                </div>
            </div>

            <!-- Validation Section -->
            <div class="validation-section">
                <h4>🔍 Validation & Testing</h4>
                <ol>
                    <li><strong>VNet Created:</strong> Verify Enterprise-VNet exists with 3 subnets</li>
                    <li><strong>Subnets Configured:</strong> Check that each subnet has the correct IP range</li>
                    <li><strong>NSGs Associated:</strong> Confirm each subnet has its NSG attached</li>
                    <li><strong>Security Rules:</strong> Verify traffic rules are configured correctly</li>
                    <li><strong>Resource Group:</strong> All resources should be in NetworkingRG</li>
                </ol>
            </div>

            <div class="resource-links">
                <h4>📚 Additional Resources</h4>
                <ul>
                    <li><a href="https://docs.microsoft.com/en-us/azure/virtual-network/" target="_blank">Azure Virtual Network Documentation</a></li>
                    <li><a href="https://docs.microsoft.com/en-us/azure/virtual-network/network-security-groups-overview" target="_blank">Network Security Groups Guide</a></li>
                    <li><a href="https://docs.microsoft.com/en-us/azure/virtual-network/virtual-networks-overview" target="_blank">VNet Planning Guide</a></li>
                </ul>
            </div>
        </div>

        <!-- Assignment 3: Storage Solutions -->
        <div id="solution3" class="solution-card">
            <div class="solution-header">
                <div class="solution-title">Assignment 3: Azure Storage Solutions</div>
                <div class="solution-meta">
                    <span class="meta-tag">💾 Storage</span>
                    <span class="meta-tag">⏱️ 75 minutes</span>
                    <span class="meta-tag">📊 Beginner</span>
                </div>
            </div>

            <div class="time-estimate">
                <strong>⏱️ Estimated Time:</strong> 75 minutes<br>
                <strong>🎯 Learning Objective:</strong> Implement comprehensive storage solutions for different business needs
            </div>

            <div class="prerequisites">
                <h4>📋 Prerequisites</h4>
                <ul>
                    <li>Understanding of different storage types (files, blobs, queues, tables)</li>
                    <li>Basic knowledge of data redundancy concepts</li>
                    <li>Familiarity with access permissions</li>
                </ul>
            </div>

            <!-- Step 1 -->
            <div class="step-section">
                <div class="step-header">
                    <div class="step-title">Step 1: Creating a Storage Account</div>
                    <div class="step-description">Set up the foundational storage infrastructure</div>
                </div>
                <div class="step-content">
                    <div class="substep">
                        <div class="substep-number">1</div>
                        <div class="substep-title">Create Storage Account</div>
                        <p>In the Azure Portal:</p>
                        <ol>
                            <li>Click "Create a resource" > Search "Storage Account"</li>
                            <li>Click "Storage account" by Microsoft</li>
                            <li>Click "Create"</li>
                        </ol>
                        
                        <div class="code-block">
Subscription: (Your subscription)
Resource Group: Create new "StorageRG"
Storage account name: enterprisestorage[random] (must be globally unique)
Region: East US
Performance: Standard
Redundancy: Geo-redundant storage (GRS)
                        </div>
                        
                        <div class="tip">
                            <strong>💡 Naming Tip:</strong> Storage account names must be 3-24 characters, lowercase letters and numbers only, and globally unique.
                        </div>
                    </div>

                    <div class="substep">
                        <div class="substep-number">2</div>
                        <div class="substep-title">Configure Advanced Settings</div>
                        <p>In the "Advanced" tab:</p>
                        
                        <div class="code-block">
Security:
✅ Require secure transfer for REST API operations
✅ Enable blob public access
✅ Enable storage account key access
✅ Minimum TLS version: Version 1.2

Blob storage:
✅ Enable hierarchical namespace (for Data Lake)
Access tier: Hot
                        </div>
                    </div>

                    <div class="substep">
                        <div class="substep-number">3</div>
                        <div class="substep-title">Review and Create</div>
                        <p>Click "Review + Create" and then "Create". Wait for deployment to complete (usually 1-2 minutes).</p>
                        
                        <div class="success">
                            <strong>✅ Success:</strong> You should see "Your deployment is complete" message.
                        </div>
                    </div>
                </div>
            </div>

            <!-- Step 2 -->
            <div class="step-section">
                <div class="step-header">
                    <div class="step-title">Step 2: Setting Up Blob Storage Containers</div>
                    <div class="step-description">Create containers for different types of data</div>
                </div>
                <div class="step-content">
                    <div class="substep">
                        <div class="substep-number">1</div>
                        <div class="substep-title">Access Blob Storage</div>
                        <p>Navigate to your storage account:</p>
                        <ol>
                            <li>Go to your storage account</li>
                            <li>In the left menu, under "Data storage", click "Containers"</li>
                            <li>Click "+ Container" to create a new container</li>
                        </ol>
                    </div>

                    <div class="substep">
                        <div class="substep-number">2</div>
                        <div class="substep-title">Create Public Container for Website Assets</div>
                        <p>Create a container for publicly accessible files:</p>
                        
                        <div class="code-block">
Name: website-assets
Public access level: Container (anonymous read access for containers and blobs)
                        </div>
                        
                        <p>Click "Create".</p>
                        
                        <div class="tip">
                            <strong>💡 Use Case:</strong> This container is perfect for storing images, CSS, JavaScript files that need to be publicly accessible.
                        </div>
                    </div>

                    <div class="substep">
                        <div class="substep-number">3</div>
                        <div class="substep-title">Create Private Container for Documents</div>
                        <p>Create a secure container:</p>
                        
                        <div class="code-block">
Name: company-documents
Public access level: Private (no anonymous access)
                        </div>
                        
                        <p>This will store sensitive company documents.</p>
                    </div>

                    <div class="substep">
                        <div class="substep-number">4</div>
                        <div class="substep-title">Create Container for Backups</div>
                        <p>Create a container for backup data:</p>
                        
                        <div class="code-block">
Name: system-backups
Public access level: Private (no anonymous access)
                        </div>
                    </div>
                </div>
            </div>

            <!-- Step 3 -->
            <div class="step-section">
                <div class="step-header">
                    <div class="step-title">Step 3: Configuring File Storage</div>
                    <div class="step-description">Set up Azure Files for shared file access</div>
                </div>
                <div class="step-content">
                    <div class="substep">
                        <div class="substep-number">1</div>
                        <div class="substep-title">Create File Share</div>
                        <p>In your storage account:</p>
                        <ol>
                            <li>Click "File shares" under "Data storage"</li>
                            <li>Click "+ File share"</li>
                            <li>Configure the file share:</li>
                        </ol>
                        
                        <div class="code-block">
Name: company-shared-files
Tier: Transaction optimized
Quota: 100 GiB
                        </div>
                        
                        <p>Click "Create".</p>
                    </div>

                    <div class="substep">
                        <div class="substep-number">2</div>
                        <div class="substep-title">Upload Sample Files</div>
                        <p>Add some sample files to the share:</p>
                        <ol>
                            <li>Click on the "company-shared-files" share</li>
                            <li>Click "Upload"</li>
                            <li>Create a simple text file on your computer named "company-policy.txt"</li>
                            <li>Upload this file to the share</li>
                        </ol>
                        
                        <div class="tip">
                            <strong>💡 Real-world Usage:</strong> File shares are perfect for legacy applications that need SMB/CIFS access or for sharing files across multiple VMs.
                        </div>
                    </div>

                    <div class="substep">
                        <div class="substep-number">3</div>
                        <div class="substep-title">Get Connection Information</div>
                        <p>To connect from a Windows machine:</p>
                        <ol>
                            <li>In the file share, click "Connect"</li>
                            <li>Copy the Windows PowerShell commands shown</li>
                            <li>Note the UNC path: \\\\storageaccount.file.core.windows.net\\company-shared-files</li>
                        </ol>
                    </div>
                </div>
            </div>

            <!-- Step 4 -->
            <div class="step-section">
                <div class="step-header">
                    <div class="step-title">Step 4: Setting Up Access Policies and Security</div>
                    <div class="step-description">Configure secure access to storage resources</div>
                </div>
                <div class="step-content">
                    <div class="substep">
                        <div class="substep-number">1</div>
                        <div class="substep-title">Create Shared Access Signature (SAS)</div>
                        <p>Generate secure access tokens:</p>
                        <ol>
                            <li>Go to storage account > "Shared access signature" (under Security + networking)</li>
                            <li>Configure SAS parameters:</li>
                        </ol>
                        
                        <div class="code-block">
Allowed services: ✅ Blob ✅ File ✅ Queue ✅ Table
Allowed resource types: ✅ Service ✅ Container ✅ Object
Allowed permissions: ✅ Read ✅ Write ✅ Delete ✅ List
Start time: (current date/time)
Expiry time: (1 year from now)
Allowed protocols: HTTPS only
                        </div>
                        
                        <p>Click "Generate SAS and connection string" and save the connection string securely.</p>
                    </div>

                    <div class="substep">
                        <div class="substep-number">2</div>
                        <div class="substep-title">Configure Access Control (IAM)</div>
                        <p>Set up role-based access:</p>
                        <ol>
                            <li>Go to storage account > "Access Control (IAM)"</li>
                            <li>Click "+ Add" > "Add role assignment"</li>
                            <li>Configure access for your Azure AD users:</li>
                        </ol>
                        
                        <div class="code-block">
Role: Storage Blob Data Contributor
Assign access to: User, group, or service principal
Select: john.it@yourtenantname.onmicrosoft.com (from Assignment 1)
                        </div>
                        
                        <p>Click "Save" to assign the role.</p>
                    </div>

                    <div class="substep">
                        <div class="substep-number">3</div>
                        <div class="substep-title">Enable Storage Analytics</div>
                        <p>Set up monitoring and logging:</p>
                        <ol>
                            <li>Go to "Diagnostic settings" under Monitoring</li>
                            <li>Click "+ Add diagnostic setting"</li>
                            <li>Configure:</li>
                        </ol>
                        
                        <div class="code-block">
Diagnostic setting name: storage-analytics
Logs: ✅ StorageRead ✅ StorageWrite ✅ StorageDelete
Metrics: ✅ Transaction
Destination: Send to Log Analytics workspace (create new if needed)
                        </div>
                    </div>
                </div>
            </div>

            <!-- Validation Section -->
            <div class="validation-section">
                <h4>🔍 Validation & Testing</h4>
                <ol>
                    <li><strong>Storage Account:</strong> Verify account is created with GRS redundancy</li>
                    <li><strong>Containers:</strong> Check all 3 containers exist with correct access levels</li>
                    <li><strong>File Share:</strong> Confirm file share is accessible and files can be uploaded</li>
                    <li><strong>Security:</strong> Test SAS token works for accessing blobs</li>
                    <li><strong>Permissions:</strong> Verify IAM roles are correctly assigned</li>
                </ol>
            </div>

            <div class="resource-links">
                <h4>📚 Additional Resources</h4>
                <ul>
                    <li><a href="https://docs.microsoft.com/en-us/azure/storage/" target="_blank">Azure Storage Documentation</a></li>
                    <li><a href="https://docs.microsoft.com/en-us/azure/storage/blobs/storage-blobs-introduction" target="_blank">Blob Storage Guide</a></li>
                    <li><a href="https://docs.microsoft.com/en-us/azure/storage/files/storage-files-introduction" target="_blank">Azure Files Documentation</a></li>
                </ul>
            </div>
        </div>

        <!-- Additional assignments can be added similarly -->
        <div id="solution4" class="solution-card">
            <div class="solution-header">
                <div class="solution-title">Assignment 4: Azure App Services Deployment</div>
                <div class="solution-meta">
                    <span class="meta-tag">🚀 Web Apps</span>
                    <span class="meta-tag">⏱️ 85 minutes</span>
                    <span class="meta-tag">📊 Intermediate</span>
                </div>
            </div>
            
            <div class="time-estimate">
                <strong>⏱️ Estimated Time:</strong> 1 hour 25 minutes<br>
                <strong>🎯 Learning Objective:</strong> Deploy scalable web applications with deployment slots and auto-scaling
            </div>

            <div class="prerequisites">
                <h4>📋 Prerequisites</h4>
                <ul>
                    <li>Completed Assignment 1 (Azure AD setup)</li>
                    <li>Basic understanding of web applications and HTTP/HTTPS</li>
                    <li>Knowledge of deployment concepts (staging vs production)</li>
                    <li>Familiarity with application monitoring principles</li>
                </ul>
            </div>

            <!-- Step 1 -->
            <div class="step-section">
                <div class="step-header">
                    <div class="step-title">Step 1: Creating App Service Plan</div>
                    <div class="step-description">Set up the hosting environment and compute resources for web applications</div>
                </div>
                <div class="step-content">
                    <div class="substep">
                        <div class="substep-number">1</div>
                        <div class="substep-title">Navigate to App Service Plans</div>
                        <p>In the Azure Portal:</p>
                        <ul>
                            <li>Click "Create a resource" (+) in the top-left corner</li>
                            <li>Search for "App Service Plan"</li>
                            <li>Click "App Service Plan" from Microsoft</li>
                            <li>Click "Create"</li>
                        </ul>
                    </div>

                    <div class="substep">
                        <div class="substep-number">2</div>
                        <div class="substep-title">Configure Basic Settings</div>
                        <p>Fill in the basic configuration:</p>
                        
                        <div class="code-block">
Subscription: (Your Azure subscription)
Resource Group: Create new "WebApp-RG"
Name: Production-AppServicePlan
Operating System: Linux
Region: East US (or your preferred region)
                        </div>
                        
                        <div class="tip">
                            <strong>💡 Linux vs Windows:</strong> Linux plans are generally more cost-effective and support popular runtimes like Node.js, Python, and PHP natively.
                        </div>
                    </div>

                    <div class="substep">
                        <div class="substep-number">3</div>
                        <div class="substep-title">Select Pricing Tier</div>
                        <p>Choose the appropriate service tier:</p>
                        
                        <div class="code-block">
Pricing Tier: Standard S1
ACUs: 100
Memory: 1.75 GB
Storage: 50 GB
Maximum instances: 10
                        </div>
                        
                        <p>Click "Review + Create" and then "Create". Wait for deployment completion.</p>
                        
                        <div class="warning">
                            <strong>⚠️ Cost Alert:</strong> Standard S1 tier costs approximately $74/month. For learning purposes, you can use Free F1 tier, but it has limitations on deployment slots.
                        </div>
                    </div>
                </div>
            </div>

            <!-- Step 2 -->
            <div class="step-section">
                <div class="step-header">
                    <div class="step-title">Step 2: Creating the Web Application</div>
                    <div class="step-description">Deploy a Node.js web application with Application Insights monitoring</div>
                </div>
                <div class="step-content">
                    <div class="substep">
                        <div class="substep-number">1</div>
                        <div class="substep-title">Create Web App</div>
                        <p>Create a new web application:</p>
                        <ol>
                            <li>Go to "Create a resource" > Search "Web App"</li>
                            <li>Click "Web App" by Microsoft</li>
                            <li>Click "Create"</li>
                        </ol>
                        
                        <div class="code-block">
Subscription: (Your subscription)
Resource Group: WebApp-RG
Name: enterprise-webapp-[random] (must be globally unique)
Publish: Code
Runtime stack: Node 18 LTS
Operating System: Linux
Region: East US
                        </div>
                    </div>

                    <div class="substep">
                        <div class="substep-number">2</div>
                        <div class="substep-title">Link to App Service Plan</div>
                        <p>Connect to your existing App Service Plan:</p>
                        
                        <div class="code-block">
App Service Plan: 
  Select "Production-AppServicePlan" (created in Step 1)
  
SKU and size: 
  Standard S1 (inherited from the plan)
                        </div>
                        
                        <div class="tip">
                            <strong>💡 Resource Sharing:</strong> Multiple web apps can share the same App Service Plan, allowing efficient resource utilization and cost optimization.
                        </div>
                    </div>

                    <div class="substep">
                        <div class="substep-number">3</div>
                        <div class="substep-title">Enable Application Insights</div>
                        <p>Configure monitoring and telemetry:</p>
                        <ol>
                            <li>Go to the "Monitoring" tab</li>
                            <li>Set "Enable Application Insights" to "Yes"</li>
                            <li>Configure:</li>
                        </ol>
                        
                        <div class="code-block">
Application Insights: 
  Create new "enterprise-webapp-insights"
  Region: East US
  Log Analytics Workspace: Create new
                        </div>
                        
                        <p>Click "Review + Create" and then "Create".</p>
                    </div>
                </div>
            </div>

            <!-- Step 3 -->
            <div class="step-section">
                <div class="step-header">
                    <div class="step-title">Step 3: Configuring Deployment Slots</div>
                    <div class="step-description">Set up staging environment for blue-green deployment strategy</div>
                </div>
                <div class="step-content">
                    <div class="substep">
                        <div class="substep-number">1</div>
                        <div class="substep-title">Access Deployment Slots</div>
                        <p>Navigate to deployment slots configuration:</p>
                        <ol>
                            <li>Go to your created Web App</li>
                            <li>In the left menu, find "Deployment"</li>
                            <li>Click "Deployment slots"</li>
                            <li>Click "Add slot"</li>
                        </ol>
                    </div>

                    <div class="substep">
                        <div class="substep-number">2</div>
                        <div class="substep-title">Create Staging Slot</div>
                        <p>Configure the staging environment:</p>
                        
                        <div class="code-block">
Name: staging
Clone settings from: enterprise-webapp-[random] (production slot)
Clone configuration: Yes
                        </div>
                        
                        <p>Click "Add" to create the staging slot.</p>
                        
                        <div class="success">
                            <strong>✅ Success:</strong> You should now see both "production" and "staging" slots listed.
                        </div>
                    </div>

                    <div class="substep">
                        <div class="substep-number">3</div>
                        <div class="substep-title">Configure Slot-Specific Settings</div>
                        <p>Set environment-specific configurations:</p>
                        <ol>
                            <li>Click on the "staging" slot</li>
                            <li>Go to "Configuration" in the left menu</li>
                            <li>Add application settings:</li>
                        </ol>
                        
                        <div class="code-block">
Application Settings:
  ENVIRONMENT = "staging"
  DEBUG = "true"
  LOG_LEVEL = "debug"
  
Slot setting: ✅ (check this box for all settings)
                        </div>
                        
                        <p>Click "Save" to apply the configuration.</p>
                        
                        <div class="tip">
                            <strong>💡 Slot Settings:</strong> Marking settings as "slot settings" prevents them from being swapped during deployment, ensuring environment-specific configurations remain in place.
                        </div>
                    </div>
                </div>
            </div>

            <!-- Step 4 -->
            <div class="step-section">
                <div class="step-header">
                    <div class="step-title">Step 4: Deploying Sample Application</div>
                    <div class="step-description">Deploy a sample Node.js application to test the setup</div>
                </div>
                <div class="step-content">
                    <div class="substep">
                        <div class="substep-number">1</div>
                        <div class="substep-title">Deploy to Staging Slot</div>
                        <p>Deploy sample code to the staging environment:</p>
                        <ol>
                            <li>Go to the staging slot</li>
                            <li>Click "Deployment Center" in the left menu</li>
                            <li>Choose "Local Git" as source</li>
                            <li>Configure deployment credentials:</li>
                        </ol>
                        
                        <div class="code-block">
Deployment Center Settings:
  Source: Local Git
  
Deployment Credentials:
  Username: enterpriseuser
  Password: ComplexP@ssw0rd123!
  
Scope: This app
                        </div>
                    </div>

                    <div class="substep">
                        <div class="substep-number">2</div>
                        <div class="substep-title">Create Sample Application</div>
                        <p>Use the App Service Editor to create a simple Node.js app:</p>
                        <ol>
                            <li>Go to "Development Tools" > "App Service Editor (Preview)"</li>
                            <li>Click "Go" to open the editor</li>
                            <li>Create these files:</li>
                        </ol>
                        
                        <div class="code-block">
package.json:
{
  "name": "enterprise-webapp",
  "version": "1.0.0",
  "main": "server.js",
  "scripts": {
    "start": "node server.js"
  },
  "dependencies": {
    "express": "^4.18.0"
  }
}

server.js:
const express = require('express');
const app = express();
const port = process.env.PORT || 3000;

app.get('/', (req, res) => {
  res.send({
    message: 'Welcome to Enterprise Web App!',
    environment: process.env.ENVIRONMENT || 'production',
    timestamp: new Date().toISOString()
  });
});

app.listen(port, () => {
  console.log(`Server running on port ${port}`);
});
                        </div>
                    </div>

                    <div class="substep">
                        <div class="substep-number">3</div>
                        <div class="substep-title">Test Staging Deployment</div>
                        <p>Verify the staging application is working:</p>
                        <ol>
                            <li>Go to the staging slot overview</li>
                            <li>Click the URL to open the staging app</li>
                            <li>Verify the response shows environment: "staging"</li>
                        </ol>
                        
                        <div class="success">
                            <strong>✅ Expected Response:</strong>
                            <pre>{
  "message": "Welcome to Enterprise Web App!",
  "environment": "staging",
  "timestamp": "2025-10-07T10:30:00.000Z"
}</pre>
                        </div>
                    </div>
                </div>
            </div>

            <!-- Step 5 -->
            <div class="step-section">
                <div class="step-header">
                    <div class="step-title">Step 5: Implementing Auto-Scaling</div>
                    <div class="step-description">Configure automatic scaling based on performance metrics</div>
                </div>
                <div class="step-content">
                    <div class="substep">
                        <div class="substep-number">1</div>
                        <div class="substep-title">Access Scale-out Settings</div>
                        <p>Configure auto-scaling rules:</p>
                        <ol>
                            <li>Go to your App Service Plan "Production-AppServicePlan"</li>
                            <li>In the left menu, click "Scale out (App Service plan)"</li>
                            <li>Click "Custom autoscale"</li>
                        </ol>
                    </div>

                    <div class="substep">
                        <div class="substep-number">2</div>
                        <div class="substep-title">Configure Scaling Rules</div>
                        <p>Set up CPU-based auto-scaling:</p>
                        
                        <div class="code-block">
Autoscale setting name: Production-AutoScale

Scale condition:
  Scale mode: Scale based on a metric
  
Instance limits:
  Minimum: 1
  Maximum: 5
  Default: 2

Rules:
  Scale out rule:
    Metric source: Current resource (App Service Plan)
    Metric name: CPU Percentage
    Operator: Greater than
    Threshold: 70
    Duration: 10 minutes
    Action: Increase count by 1
    
  Scale in rule:
    Metric source: Current resource (App Service Plan)
    Metric name: CPU Percentage
    Operator: Less than
    Threshold: 30
    Duration: 10 minutes
    Action: Decrease count by 1
                        </div>
                        
                        <p>Click "Save" to apply the auto-scaling configuration.</p>
                    </div>

                    <div class="substep">
                        <div class="substep-number">3</div>
                        <div class="substep-title">Test Slot Swapping</div>
                        <p>Perform a blue-green deployment:</p>
                        <ol>
                            <li>Go back to "Deployment slots" in your Web App</li>
                            <li>Click "Swap" at the top</li>
                            <li>Configure the swap:</li>
                        </ol>
                        
                        <div class="code-block">
Swap type: Swap
Source: staging
Target: production

Swap with preview: No (for this test)
                        </div>
                        
                        <p>Click "Swap" to complete the operation.</p>
                        
                        <div class="warning">
                            <strong>⚠️ Production Impact:</strong> In real scenarios, always test with "Swap with preview" first to validate changes before completing the swap.
                        </div>
                    </div>
                </div>
            </div>

            <!-- Validation Section -->
            <div class="validation-section">
                <h4>🔍 Validation & Testing</h4>
                <ol>
                    <li><strong>App Service Plan:</strong> Verify "Production-AppServicePlan" is running with Standard S1 tier</li>
                    <li><strong>Web App Deployed:</strong> Confirm the web app is accessible via HTTPS</li>
                    <li><strong>Deployment Slots:</strong> Check both production and staging slots exist and are functional</li>
                    <li><strong>Application Insights:</strong> Verify telemetry data is being collected</li>
                    <li><strong>Auto-scaling:</strong> Confirm scaling rules are configured and active</li>
                    <li><strong>Slot Configuration:</strong> Ensure environment-specific settings are properly configured</li>
                    <li><strong>SSL Certificate:</strong> Verify HTTPS is working with the default Azure certificate</li>
                </ol>
            </div>

            <div class="troubleshooting">
                <h4>🛠️ Common Issues & Solutions</h4>
                <ul>
                    <li><strong>App won't start:</strong> Check package.json and ensure dependencies are correctly specified</li>
                    <li><strong>Deployment fails:</strong> Verify deployment credentials and Git repository configuration</li>
                    <li><strong>Slot swap fails:</strong> Check for slot-specific settings and ensure both slots are healthy</li>
                    <li><strong>Auto-scaling not working:</strong> Verify metrics are being generated and thresholds are realistic</li>
                    <li><strong>SSL issues:</strong> Check if custom domain is configured correctly</li>
                </ul>
            </div>

            <div class="resource-links">
                <h4>📚 Additional Resources</h4>
                <ul>
                    <li><a href="https://docs.microsoft.com/en-us/azure/app-service/" target="_blank">Azure App Service Documentation</a></li>
                    <li><a href="https://docs.microsoft.com/en-us/azure/app-service/deploy-staging-slots" target="_blank">Deployment Slots Guide</a></li>
                    <li><a href="https://docs.microsoft.com/en-us/azure/azure-monitor/app/app-insights-overview" target="_blank">Application Insights Overview</a></li>
                    <li><a href="https://docs.microsoft.com/en-us/azure/app-service/manage-automatic-scaling" target="_blank">Auto-scaling Configuration</a></li>
                </ul>
            </div>
        </div>

        <div id="solution5" class="solution-card">
            <div class="solution-header">
                <div class="solution-title">Assignment 5: Azure SQL Database Setup</div>
                <div class="solution-meta">
                    <span class="meta-tag">🗄️ Database</span>
                    <span class="meta-tag">⏱️ 95 minutes</span>
                    <span class="meta-tag">📊 Advanced</span>
                </div>
            </div>
            
            <div class="time-estimate">
                <strong>⏱️ Estimated Time:</strong> 1 hour 35 minutes<br>
                <strong>🎯 Learning Objective:</strong> Create enterprise-grade database with advanced security, backup, and monitoring
            </div>

            <div class="prerequisites">
                <h4>📋 Prerequisites</h4>
                <ul>
                    <li>Completed Assignment 2 (Virtual Network setup)</li>
                    <li>Basic understanding of relational databases and SQL</li>
                    <li>Knowledge of database security concepts (authentication, encryption)</li>
                    <li>Familiarity with backup and recovery principles</li>
                </ul>
            </div>

            <!-- Step 1 -->
            <div class="step-section">
                <div class="step-header">
                    <div class="step-title">Step 1: Creating Azure SQL Server</div>
                    <div class="step-description">Set up the logical server that will host your databases</div>
                </div>
                <div class="step-content">
                    <div class="substep">
                        <div class="substep-number">1</div>
                        <div class="substep-title">Navigate to SQL Servers</div>
                        <p>In the Azure Portal:</p>
                        <ul>
                            <li>Click "Create a resource" (+) in the top-left corner</li>
                            <li>Search for "SQL Database"</li>
                            <li>Click "SQL Database" from Microsoft</li>
                            <li>Click "Create"</li>
                        </ul>
                        
                        <div class="tip">
                            <strong>💡 Note:</strong> We'll create the server as part of the database creation process, which is the most common approach.
                        </div>
                    </div>

                    <div class="substep">
                        <div class="substep-number">2</div>
                        <div class="substep-title">Configure Database Basics</div>
                        <p>Fill in the basic database information:</p>
                        
                        <div class="code-block">
Subscription: (Your Azure subscription)
Resource Group: Create new "Database-RG"
Database name: EnterpriseDB
Server: Create new
                        </div>
                    </div>

                    <div class="substep">
                        <div class="substep-number">3</div>
                        <div class="substep-title">Create SQL Server</div>
                        <p>Click "Create new" next to Server and configure:</p>
                        
                        <div class="code-block">
Server name: enterprise-sql-[random] (must be globally unique)
Location: East US
Authentication method: Use SQL authentication

Server admin login: sqladmin
Password: ComplexP@ssw0rd123!
Confirm password: ComplexP@ssw0rd123!
                        </div>
                        
                        <div class="warning">
                            <strong>⚠️ Security Note:</strong> In production, use Azure AD authentication and store passwords in Key Vault. This SQL authentication is for learning purposes only.
                        </div>
                        
                        <p>Click "OK" to create the server configuration.</p>
                    </div>
                </div>
            </div>

            <!-- Step 2 -->
            <div class="step-section">
                <div class="step-header">
                    <div class="step-title">Step 2: Configuring Database Performance and Storage</div>
                    <div class="step-description">Set up appropriate compute and storage resources for enterprise workloads</div>
                </div>
                <div class="step-content">
                    <div class="substep">
                        <div class="substep-number">1</div>
                        <div class="substep-title">Configure Compute + Storage</div>
                        <p>Click "Configure database" to set performance tier:</p>
                        
                        <div class="code-block">
Service tier: General Purpose
Compute tier: Provisioned
Generation: Gen5
vCores: 2
Memory: 10.4 GB
Max storage size: 32 GB
                        </div>
                        
                        <div class="tip">
                            <strong>💡 Performance Tip:</strong> General Purpose tier provides balanced compute and I/O performance. For higher I/O requirements, consider Business Critical tier.
                        </div>
                    </div>

                    <div class="substep">
                        <div class="substep-number">2</div>
                        <div class="substep-title">Configure Backup Storage</div>
                        <p>Set up backup redundancy and retention:</p>
                        
                        <div class="code-block">
Backup storage redundancy: Geo-redundant backup storage

Advanced settings:
  Collation: SQL_Latin1_General_CP1_CI_AS
  Use existing data: None
                        </div>
                        
                        <p>Click "Apply" to confirm the configuration.</p>
                    </div>
                </div>
            </div>

            <!-- Step 3 -->
            <div class="step-section">
                <div class="step-header">
                    <div class="step-title">Step 3: Implementing Network Security</div>
                    <div class="step-description">Configure firewall rules and private endpoints for secure access</div>
                </div>
                <div class="step-content">
                    <div class="substep">
                        <div class="substep-number">1</div>
                        <div class="substep-title">Configure Networking</div>
                        <p>Go to the "Networking" tab in the database creation wizard:</p>
                        
                        <div class="code-block">
Connectivity method: Public endpoint

Firewall rules:
✓ Allow Azure services and resources to access this server
✓ Add current client IP address: (Your IP will be auto-detected)

Connection policy: Default
                        </div>
                    </div>

                    <div class="substep">
                        <div class="substep-number">2</div>
                        <div class="substep-title">Review and Create</div>
                        <p>Verify all settings and create the database:</p>
                        <ol>
                            <li>Click "Review + Create"</li>
                            <li>Review the configuration summary</li>
                            <li>Click "Create" to deploy</li>
                            <li>Wait for deployment completion (5-10 minutes)</li>
                        </ol>
                        
                        <div class="success">
                            <strong>✅ Deployment Status:</strong> Monitor the deployment progress in the notifications panel.
                        </div>
                    </div>
                </div>
            </div>

            <!-- Step 4 -->
            <div class="step-section">
                <div class="step-header">
                    <div class="step-title">Step 4: Configuring Advanced Security</div>
                    <div class="step-description">Enable threat detection, auditing, and advanced data security features</div>
                </div>
                <div class="step-content">
                    <div class="substep">
                        <div class="substep-number">1</div>
                        <div class="substep-title">Enable Microsoft Defender for SQL</div>
                        <p>Set up advanced threat protection:</p>
                        <ol>
                            <li>Go to your SQL Server (not the database)</li>
                            <li>Click "Microsoft Defender for Cloud" in the left menu</li>
                            <li>Click "Enable Microsoft Defender for SQL"</li>
                        </ol>
                        
                        <div class="code-block">
Microsoft Defender for SQL:
  Status: Enabled
  
Vulnerability Assessment:
  Storage account: Create new "sqlvaassessment[random]"
  Periodic recurring scans: Enabled
  Send scan reports to: (your email)
  
Advanced Threat Protection:
  Send alerts to: (your email)
  
Data Discovery & Classification:
  Auto-classification: Enabled
                        </div>
                        
                        <div class="warning">
                            <strong>⚠️ Cost Alert:</strong> Microsoft Defender for SQL costs approximately $15/server/month. For learning, you can disable it after testing.
                        </div>
                    </div>

                    <div class="substep">
                        <div class="substep-number">2</div>
                        <div class="substep-title">Configure Auditing</div>
                        <p>Enable database auditing for compliance and monitoring:</p>
                        <ol>
                            <li>Go to your EnterpriseDB database</li>
                            <li>Click "Auditing" in the left menu under "Security"</li>
                            <li>Configure auditing settings:</li>
                        </ol>
                        
                        <div class="code-block">
Auditing:
  Enable Azure SQL Auditing: ON
  
Audit log destination:
  ✓ Storage
  Storage account: Create new "sqlauditlogs[random]"
  
Advanced properties:
  Retention (days): 90
  
Actions and groups to audit:
  - SUCCESSFUL_DATABASE_AUTHENTICATION_GROUP
  - DATABASE_LOGOUT_GROUP
  - USER_CHANGE_PASSWORD_GROUP
                        </div>
                        
                        <p>Click "Save" to enable auditing.</p>
                    </div>

                    <div class="substep">
                        <div class="substep-number">3</div>
                        <div class="substep-title">Configure Transparent Data Encryption</div>
                        <p>Enable encryption at rest for data protection:</p>
                        <ol>
                            <li>Go to "Transparent data encryption" in the database menu</li>
                            <li>Verify TDE is enabled (it's on by default)</li>
                            <li>Optionally configure customer-managed keys:</li>
                        </ol>
                        
                        <div class="code-block">
Transparent Data Encryption:
  Data encryption: ON (Service-managed key)
  
Optional - Customer-managed key:
  Key Vault: (Link to Azure Key Vault if needed)
  Key: (Select encryption key)
  
Encryption status: Encrypted
                        </div>
                        
                        <div class="success">
                            <strong>✅ Security Status:</strong> TDE is enabled by default for all new Azure SQL databases, providing encryption at rest.
                        </div>
                    </div>
                </div>
            </div>

            <!-- Step 5 -->
            <div class="step-section">
                <div class="step-header">
                    <div class="step-title">Step 5: Setting Up Backup and Recovery</div>
                    <div class="step-description">Configure automated backups and test point-in-time recovery</div>
                </div>
                <div class="step-content">
                    <div class="substep">
                        <div class="substep-number">1</div>
                        <div class="substep-title">Review Automated Backup Settings</div>
                        <p>Check the default backup configuration:</p>
                        <ol>
                            <li>Go to your EnterpriseDB database</li>
                            <li>Click "Backup/Restore" in the left menu</li>
                            <li>Review automatic backup settings:</li>
                        </ol>
                        
                        <div class="code-block">
Automated Backups:
  Full backups: Weekly
  Differential backups: Every 12-24 hours
  Transaction log backups: Every 5-10 minutes
  
Retention period:
  Point-in-time restore: 7 days (default)
  Long-term retention: Not configured
  
Backup storage redundancy: Geo-redundant
                        </div>
                    </div>

                    <div class="substep">
                        <div class="substep-number">2</div>
                        <div class="substep-title">Configure Long-term Retention</div>
                        <p>Set up long-term backup retention policy:</p>
                        <ol>
                            <li>Click "Configure policies" in the backup section</li>
                            <li>Configure LTR policy:</li>
                        </ol>
                        
                        <div class="code-block">
Long-term Retention Policy:
  Weekly backups: Keep for 12 weeks
  Monthly backups: Keep for 12 months
  Yearly backups: Keep for 5 years
  
Backup storage redundancy: Geo-redundant storage
                        </div>
                        
                        <p>Click "Apply" to save the policy.</p>
                    </div>

                    <div class="substep">
                        <div class="substep-number">3</div>
                        <div class="substep-title">Test Database Connection</div>
                        <p>Verify database connectivity using Query Editor:</p>
                        <ol>
                            <li>Go to your EnterpriseDB database</li>
                            <li>Click "Query editor (preview)" in the left menu</li>
                            <li>Login with SQL authentication:</li>
                        </ol>
                        
                        <div class="code-block">
Login credentials:
  Login: sqladmin
  Password: ComplexP@ssw0rd123!
                        </div>
                        
                        <p>Once connected, run this test query:</p>
                        
                        <div class="code-block">
-- Test query to verify database functionality
SELECT 
    GETDATE() AS CurrentDateTime,
    @@VERSION AS SQLVersion,
    DB_NAME() AS DatabaseName;

-- Create sample table
CREATE TABLE Employees (
    ID int IDENTITY(1,1) PRIMARY KEY,
    FirstName nvarchar(50),
    LastName nvarchar(50),
    Email nvarchar(100),
    Department nvarchar(50),
    CreatedDate datetime2 DEFAULT GETDATE()
);

-- Insert sample data
INSERT INTO Employees (FirstName, LastName, Email, Department)
VALUES 
    ('John', 'Smith', 'john.smith@company.com', 'IT'),
    ('Sarah', 'Johnson', 'sarah.johnson@company.com', 'Marketing'),
    ('Mike', 'Davis', 'mike.davis@company.com', 'Sales');

-- Query sample data
SELECT * FROM Employees;
                        </div>
                        
                        <div class="success">
                            <strong>✅ Expected Result:</strong> You should see the system information and 3 employee records inserted successfully.
                        </div>
                    </div>
                </div>
            </div>

            <!-- Validation Section -->
            <div class="validation-section">
                <h4>🔍 Validation & Testing</h4>
                <ol>
                    <li><strong>SQL Server Created:</strong> Verify server "enterprise-sql-[random]" exists and is online</li>
                    <li><strong>Database Operational:</strong> Confirm "EnterpriseDB" is accessible and running</li>
                    <li><strong>Security Configured:</strong> Check that auditing and threat detection are enabled</li>
                    <li><strong>Backup Settings:</strong> Verify automated backup and LTR policies are configured</li>
                    <li><strong>Firewall Rules:</strong> Confirm appropriate access rules are in place</li>
                    <li><strong>Encryption:</strong> Verify Transparent Data Encryption is enabled</li>
                    <li><strong>Monitoring:</strong> Check that Microsoft Defender for SQL is operational</li>
                    <li><strong>Connectivity:</strong> Test database connection and query execution</li>
                </ol>
            </div>

            <div class="troubleshooting">
                <h4>🛠️ Common Issues & Solutions</h4>
                <ul>
                    <li><strong>Connection timeout:</strong> Check firewall rules and ensure your IP is whitelisted</li>
                    <li><strong>Authentication failed:</strong> Verify username/password and ensure SQL authentication is enabled</li>
                    <li><strong>Cannot access Query Editor:</strong> Check browser compatibility and disable popup blockers</li>
                    <li><strong>Backup configuration failed:</strong> Verify storage account permissions and region compatibility</li>
                    <li><strong>High costs:</strong> Monitor DTU/vCore usage and consider scaling down for development</li>
                    <li><strong>Defender alerts:</strong> Review security recommendations and implement suggested fixes</li>
                </ul>
            </div>

            <div class="resource-links">
                <h4>📚 Additional Resources</h4>
                <ul>
                    <li><a href="https://docs.microsoft.com/en-us/azure/azure-sql/" target="_blank">Azure SQL Database Documentation</a></li>
                    <li><a href="https://docs.microsoft.com/en-us/azure/azure-sql/database/security-overview" target="_blank">SQL Database Security Overview</a></li>
                    <li><a href="https://docs.microsoft.com/en-us/azure/azure-sql/database/automated-backups-overview" target="_blank">Automated Backups Guide</a></li>
                    <li><a href="https://docs.microsoft.com/en-us/azure/defender-for-cloud/defender-for-sql-introduction" target="_blank">Microsoft Defender for SQL</a></li>
                </ul>
            </div>
        </div>

        <div id="solution6" class="solution-card">
            <div class="solution-header">
                <div class="solution-title">Assignment 6: Azure Key Vault Configuration</div>
                <div class="solution-meta">
                    <span class="meta-tag">🔐 Security</span>
                    <span class="meta-tag">⏱️ 70 minutes</span>
                    <span class="meta-tag">📊 Intermediate</span>
                </div>
            </div>
            
            <div class="time-estimate">
                <strong>⏱️ Estimated Time:</strong> 1 hour 10 minutes<br>
                <strong>🎯 Learning Objective:</strong> Implement centralized secrets management with access policies and integration
            </div>

            <div class="prerequisites">
                <h4>📋 Prerequisites</h4>
                <ul>
                    <li>Completed Assignment 4 (App Services deployment)</li>
                    <li>Understanding of cryptographic concepts (keys, secrets, certificates)</li>
                    <li>Basic knowledge of application authentication methods</li>
                    <li>Familiarity with Azure role-based access control (RBAC)</li>
                </ul>
            </div>

            <!-- Step 1 -->
            <div class="step-section">
                <div class="step-header">
                    <div class="step-title">Step 1: Creating Azure Key Vault</div>
                    <div class="step-description">Set up secure vault for storing secrets, keys, and certificates</div>
                </div>
                <div class="step-content">
                    <div class="substep">
                        <div class="substep-number">1</div>
                        <div class="substep-title">Navigate to Key Vault Service</div>
                        <p>In the Azure Portal:</p>
                        <ul>
                            <li>Click "Create a resource" (+) in the top-left corner</li>
                            <li>Search for "Key Vault"</li>
                            <li>Click "Key Vault" from Microsoft</li>
                            <li>Click "Create"</li>
                        </ul>
                    </div>

                    <div class="substep">
                        <div class="substep-number">2</div>
                        <div class="substep-title">Configure Basic Settings</div>
                        <p>Fill in the basic vault configuration:</p>
                        
                        <div class="code-block">
Subscription: (Your Azure subscription)
Resource Group: Create new "Security-RG"
Key vault name: enterprise-keyvault-[random] (must be globally unique)
Region: East US
Pricing tier: Standard
                        </div>
                        
                        <div class="tip">
                            <strong>💡 Naming Convention:</strong> Key Vault names must be 3-24 characters, alphanumeric and hyphens only, and globally unique across Azure.
                        </div>
                    </div>

                    <div class="substep">
                        <div class="substep-number">3</div>
                        <div class="substep-title">Configure Access Policy</div>
                        <p>Set up the access control method:</p>
                        
                        <div class="code-block">
Access configuration:
  Permission model: Azure role-based access control (RBAC)
  
Resource access:
  ✓ Azure Virtual Machines for deployment
  ✓ Azure Resource Manager for template deployment
  ✓ Azure Disk Encryption for volume encryption
                        </div>
                        
                        <div class="warning">
                            <strong>⚠️ Security Note:</strong> RBAC provides more granular and manageable access control compared to access policies. This is the recommended approach for new deployments.
                        </div>
                    </div>

                    <div class="substep">
                        <div class="substep-number">4</div>
                        <div class="substep-title">Review and Create</div>
                        <p>Complete the Key Vault creation:</p>
                        <ol>
                            <li>Click "Review + Create"</li>
                            <li>Verify all configuration settings</li>
                            <li>Click "Create" to deploy the Key Vault</li>
                            <li>Wait for deployment completion (1-2 minutes)</li>
                        </ol>
                        
                        <div class="success">
                            <strong>✅ Deployment Complete:</strong> Key Vault should be ready and accessible.
                        </div>
                    </div>
                </div>
            </div>

            <!-- Step 2 -->
            <div class="step-section">
                <div class="step-header">
                    <div class="step-title">Step 2: Configuring Access Permissions</div>
                    <div class="step-description">Set up role assignments for secure access to vault contents</div>
                </div>
                <div class="step-content">
                    <div class="substep">
                        <div class="substep-number">1</div>
                        <div class="substep-title">Assign Key Vault Administrator Role</div>
                        <p>Grant yourself administrative access:</p>
                        <ol>
                            <li>Go to your created Key Vault</li>
                            <li>Click "Access control (IAM)" in the left menu</li>
                            <li>Click "Add" > "Add role assignment"</li>
                        </ol>
                        
                        <div class="code-block">
Role assignment:
  Role: Key Vault Administrator
  Assign access to: User, group, or service principal
  Members: (Select your user account)
  
Description: Full access to Key Vault and its objects
                        </div>
                        
                        <p>Click "Review + assign" and then "Assign".</p>
                    </div>

                    <div class="substep">
                        <div class="substep-number">2</div>
                        <div class="substep-title">Create Service Principal for Applications</div>
                        <p>Set up an identity for your web application:</p>
                        <ol>
                            <li>Go to Azure Active Directory</li>
                            <li>Click "App registrations"</li>
                            <li>Click "New registration"</li>
                        </ol>
                        
                        <div class="code-block">
App registration:
  Name: Enterprise-WebApp-KeyVault
  Supported account types: Accounts in this organizational directory only
  Redirect URI: (Leave blank for now)
                        </div>
                        
                        <p>Click "Register" to create the service principal.</p>
                    </div>

                    <div class="substep">
                        <div class="substep-number">3</div>
                        <div class="substep-title">Assign Application Permissions</div>
                        <p>Grant the web app access to Key Vault secrets:</p>
                        <ol>
                            <li>Go back to your Key Vault</li>
                            <li>Click "Access control (IAM)"</li>
                            <li>Add another role assignment:</li>
                        </ol>
                        
                        <div class="code-block">
Role assignment:
  Role: Key Vault Secrets User
  Assign access to: User, group, or service principal
  Members: Enterprise-WebApp-KeyVault (select the app registration)
  
Permissions: Read secret contents only
                        </div>
                        
                        <div class="tip">
                            <strong>💡 Principle of Least Privilege:</strong> Only grant the minimum permissions needed. The Secrets User role allows reading secrets but not managing them.
                        </div>
                    </div>
                </div>
            </div>

            <!-- Step 3 -->
            <div class="step-section">
                <div class="step-header">
                    <div class="step-title">Step 3: Storing Secrets and Keys</div>
                    <div class="step-description">Add various types of sensitive information to the vault</div>
                </div>
                <div class="step-content">
                    <div class="substep">
                        <div class="substep-number">1</div>
                        <div class="substep-title">Create Database Connection Secret</div>
                        <p>Store the SQL database connection string:</p>
                        <ol>
                            <li>Go to your Key Vault</li>
                            <li>Click "Secrets" in the left menu</li>
                            <li>Click "Generate/Import"</li>
                        </ol>
                        
                        <div class="code-block">
Secret creation:
  Upload options: Manual
  Name: DatabaseConnectionString
  Secret value: Server=tcp:enterprise-sql-[random].database.windows.net,1433;Initial Catalog=EnterpriseDB;Persist Security Info=False;User ID=sqladmin;Password=ComplexP@ssw0rd123!;MultipleActiveResultSets=False;Encrypt=True;TrustServerCertificate=False;Connection Timeout=30;
  
Optional settings:
  Content type: Connection String
  Set activation date: (Leave blank)
  Set expiration date: (Set to 1 year from now)
  Enabled: Yes
                        </div>
                        
                        <p>Click "Create" to store the secret.</p>
                    </div>

                    <div class="substep">
                        <div class="substep-number">2</div>
                        <div class="substep-title">Create API Key Secret</div>
                        <p>Store a sample API key for external services:</p>
                        
                        <div class="code-block">
Secret creation:
  Upload options: Manual
  Name: ThirdPartyAPIKey
  Secret value: sk-1234567890abcdef-sample-api-key-do-not-use-in-production
  
Tags:
  Environment: Production
  Service: External API
  Owner: IT Department
                        </div>
                        
                        <p>Click "Create" to store the API key.</p>
                    </div>

                    <div class="substep">
                        <div class="substep-number">3</div>
                        <div class="substep-title">Generate Encryption Key</div>
                        <p>Create a cryptographic key for data encryption:</p>
                        <ol>
                            <li>Click "Keys" in the left menu</li>
                            <li>Click "Generate/Import"</li>
                        </ol>
                        
                        <div class="code-block">
Key creation:
  Options: Generate
  Name: DataEncryptionKey
  Key type: RSA
  RSA key size: 2048
  
Advanced options:
  Set activation date: (Current date)
  Set expiration date: (2 years from now)
  Enabled: Yes
  
Operations:
  ✓ Encrypt
  ✓ Decrypt
  ✓ Wrap Key
  ✓ Unwrap Key
                        </div>
                        
                        <p>Click "Create" to generate the encryption key.</p>
                    </div>

                    <div class="substep">
                        <div class="substep-number">4</div>
                        <div class="substep-title">Upload SSL Certificate</div>
                        <p>Store a certificate for HTTPS encryption:</p>
                        <ol>
                            <li>Click "Certificates" in the left menu</li>
                            <li>Click "Generate/Import"</li>
                        </ol>
                        
                        <div class="code-block">
Certificate creation:
  Method of Certificate Creation: Generate
  Certificate Name: WebAppSSLCert
  Type of Certificate Authority (CA): Self-signed certificate
  Subject: CN=enterprise-webapp.azurewebsites.net
  
DNS Names:
  enterprise-webapp.azurewebsites.net
  www.enterprise-webapp.azurewebsites.net
  
Validity Period (in months): 12
  Content Type: PKCS #12
  
Advanced Policy Configuration:
  Exportable Private Key: Yes
  Key Type: RSA
  Key Size: 2048
  Key Usage: Digital Signature, Key Encipherment
                        </div>
                        
                        <p>Click "Create" to generate the certificate.</p>
                        
                        <div class="warning">
                            <strong>⚠️ Production Note:</strong> For production, use certificates from trusted CAs like DigiCert or Let's Encrypt instead of self-signed certificates.
                        </div>
                    </div>
                </div>
            </div>

            <!-- Step 4 -->
            <div class="step-section">
                <div class="step-header">
                    <div class="step-title">Step 4: Integrating with Web Application</div>
                    <div class="step-description">Configure your web app to securely access Key Vault secrets</div>
                </div>
                <div class="step-content">
                    <div class="substep">
                        <div class="substep-number">1</div>
                        <div class="substep-title">Enable Managed Identity</div>
                        <p>Configure your web app to use managed identity:</p>
                        <ol>
                            <li>Go to your Web App (from Assignment 4)</li>
                            <li>Click "Identity" in the left menu</li>
                            <li>Configure system-assigned identity:</li>
                        </ol>
                        
                        <div class="code-block">
System assigned:
  Status: On
  
Permissions: This will create an identity for the web app
                        </div>
                        
                        <p>Click "Save" and then "Yes" to confirm.</p>
                        
                        <div class="tip">
                            <strong>💡 Managed Identity:</strong> This eliminates the need to store credentials in your application code, as Azure handles the authentication automatically.
                        </div>
                    </div>

                    <div class="substep">
                        <div class="substep-number">2</div>
                        <div class="substep-title">Grant Web App Access to Key Vault</div>
                        <p>Assign Key Vault permissions to the web app's managed identity:</p>
                        <ol>
                            <li>Copy the Object (principal) ID from the web app's identity page</li>
                            <li>Go back to your Key Vault</li>
                            <li>Click "Access control (IAM)"</li>
                            <li>Add role assignment:</li>
                        </ol>
                        
                        <div class="code-block">
Role assignment:
  Role: Key Vault Secrets User
  Assign access to: User, group, or service principal
  Members: (Paste the Object ID from web app)
  
Note: The web app identity might show as "Unknown" until Azure propagates the identity
                        </div>
                    </div>

                    <div class="substep">
                        <div class="substep-number">3</div>
                        <div class="substep-title">Configure Application Settings</div>
                        <p>Add Key Vault references to your web app configuration:</p>
                        <ol>
                            <li>Go to your Web App</li>
                            <li>Click "Configuration" in the left menu</li>
                            <li>Add new application settings:</li>
                        </ol>
                        
                        <div class="code-block">
Application Settings:

Setting 1:
  Name: ConnectionStrings__DefaultConnection
  Value: @Microsoft.KeyVault(VaultName=enterprise-keyvault-[random];SecretName=DatabaseConnectionString)
  
Setting 2:
  Name: ExternalAPI__ApiKey
  Value: @Microsoft.KeyVault(VaultName=enterprise-keyvault-[random];SecretName=ThirdPartyAPIKey)
  
Setting 3:
  Name: KeyVault__VaultUri
  Value: https://enterprise-keyvault-[random].vault.azure.net/
                        </div>
                        
                        <p>Click "Save" to apply the configuration.</p>
                    </div>

                    <div class="substep">
                        <div class="substep-number">4</div>
                        <div class="substep-title">Test Secret Retrieval</div>
                        <p>Create a simple test endpoint to verify Key Vault integration:</p>
                        <ol>
                            <li>Go to your web app's App Service Editor</li>
                            <li>Update the server.js file to include secret testing:</li>
                        </ol>
                        
                        <div class="code-block">
// Updated server.js with Key Vault integration test
const express = require('express');
const app = express();
const port = process.env.PORT || 3000;

app.get('/', (req, res) => {
  res.send({
    message: 'Welcome to Enterprise Web App with Key Vault!',
    environment: process.env.ENVIRONMENT || 'production',
    timestamp: new Date().toISOString(),
    keyVaultConfigured: !!process.env.KeyVault__VaultUri
  });
});

app.get('/config-test', (req, res) => {
  res.send({
    hasConnectionString: !!process.env.ConnectionStrings__DefaultConnection,
    hasApiKey: !!process.env.ExternalAPI__ApiKey,
    vaultUri: process.env.KeyVault__VaultUri || 'Not configured',
    // Note: Never return actual secret values in production
    connectionStringSource: process.env.ConnectionStrings__DefaultConnection ? 'Key Vault Reference' : 'Direct Value'
  });
});

app.listen(port, () => {
  console.log(`Server running on port ${port}`);
});
                        </div>
                        
                        <p>Save the file and restart the web app to test the integration.</p>
                    </div>
                </div>
            </div>

            <!-- Validation Section -->
            <div class="validation-section">
                <h4>🔍 Validation & Testing</h4>
                <ol>
                    <li><strong>Key Vault Created:</strong> Verify vault is accessible and operational</li>
                    <li><strong>Access Permissions:</strong> Confirm RBAC roles are properly assigned</li>
                    <li><strong>Secrets Stored:</strong> Check that all secrets (connection string, API key) are created</li>
                    <li><strong>Keys Generated:</strong> Verify encryption key is available</li>
                    <li><strong>Certificate Created:</strong> Confirm SSL certificate is generated</li>
                    <li><strong>Managed Identity:</strong> Verify web app has system-assigned identity enabled</li>
                    <li><strong>Web App Integration:</strong> Test that web app can access Key Vault references</li>
                    <li><strong>Secret Retrieval:</strong> Visit /config-test endpoint to verify configuration</li>
                </ol>
            </div>

            <div class="troubleshooting">
                <h4>🛠️ Common Issues & Solutions</h4>
                <ul>
                    <li><strong>Access denied errors:</strong> Check RBAC role assignments and managed identity configuration</li>
                    <li><strong>Key Vault references not resolving:</strong> Verify syntax and secret names in app settings</li>
                    <li><strong>Certificate generation failed:</strong> Check subject name format and DNS validity</li>
                    <li><strong>Managed identity not working:</strong> Wait a few minutes for Azure AD propagation</li>
                    <li><strong>Secrets not appearing:</strong> Verify you have Key Vault Administrator role</li>
                    <li><strong>Web app can't access vault:</strong> Check if soft delete is blocking access to previous vault</li>
                </ul>
            </div>

            <div class="resource-links">
                <h4>📚 Additional Resources</h4>
                <ul>
                    <li><a href="https://docs.microsoft.com/en-us/azure/key-vault/" target="_blank">Azure Key Vault Documentation</a></li>
                    <li><a href="https://docs.microsoft.com/en-us/azure/key-vault/general/rbac-guide" target="_blank">Key Vault RBAC Guide</a></li>
                    <li><a href="https://docs.microsoft.com/en-us/azure/app-service/app-service-key-vault-references" target="_blank">Key Vault References in App Service</a></li>
                    <li><a href="https://docs.microsoft.com/en-us/azure/active-directory/managed-identities-azure-resources/" target="_blank">Managed Identities Overview</a></li>
                </ul>
            </div>
        </div>

        <div id="solution7" class="solution-card">
            <div class="solution-header">
                <div class="solution-title">Assignment 7: Azure Monitor and Alerting</div>
                <div class="solution-meta">
                    <span class="meta-tag">📊 Monitoring</span>
                    <span class="meta-tag">⏱️ 80 minutes</span>
                    <span class="meta-tag">📊 Intermediate</span>
                </div>
            </div>
            
            <div class="time-estimate">
                <strong>⏱️ Estimated Time:</strong> 80 minutes<br>
                <strong>🎯 Learning Objective:</strong> Comprehensive monitoring and alerting setup
            </div>

            <div class="step-section">
                <div class="step-header">
                    <div class="step-title">Step 1: Configure Log Analytics Workspace</div>
                    <div class="step-description">Centralized logging and analytics</div>
                </div>
                <div class="step-content">
                    <div class="substep">
                        <div class="substep-number">1</div>
                        <div class="substep-title">Create Workspace</div>
                        <div class="code-block">
Resource Group: Monitoring-RG
Name: enterprise-logs-workspace
Region: East US
Pricing tier: Pay-as-you-go
                        </div>
                    </div>
                    <div class="substep">
                        <div class="substep-number">2</div>
                        <div class="substep-title">Set Up Data Collection</div>
                        <div class="code-block">
Connect Azure resources
Configure diagnostic settings
Install monitoring agents
Set up custom logs collection
                        </div>
                    </div>
                    <div class="substep">
                        <div class="substep-number">3</div>
                        <div class="substep-title">Create Alerts</div>
                        <div class="code-block">
CPU usage alerts
Memory utilization warnings
Database performance alerts
Application error notifications
                        </div>
                    </div>
                </div>
            </div>

            <div class="validation-section">
                <h4>� Validation</h4>
                <p>✅ Logs flowing to workspace, alerts configured, dashboards created, notifications working</p>
            </div>
        </div>

        <div id="solution8" class="solution-card">
            <div class="solution-header">
                <div class="solution-title">Assignment 8: Azure Backup Solutions</div>
                <div class="solution-meta">
                    <span class="meta-tag">💾 Backup</span>
                    <span class="meta-tag">⏱️ 75 minutes</span>
                    <span class="meta-tag">📊 Intermediate</span>
                </div>
            </div>
            
            <div class="time-estimate">
                <strong>⏱️ Estimated Time:</strong> 1 hour 15 minutes<br>
                <strong>🎯 Learning Objective:</strong> Implement comprehensive backup and disaster recovery strategy
            </div>

            <div class="prerequisites">
                <h4>📋 Prerequisites</h4>
                <ul>
                    <li>Understanding of backup and recovery concepts (RPO, RTO)</li>
                    <li>Knowledge of storage redundancy options</li>
                    <li>Basic familiarity with disaster recovery planning</li>
                    <li>Access to create virtual machines (for testing)</li>
                </ul>
            </div>

            <!-- Step 1 -->
            <div class="step-section">
                <div class="step-header">
                    <div class="step-title">Step 1: Creating Recovery Services Vault</div>
                    <div class="step-description">Set up centralized backup and disaster recovery infrastructure</div>
                </div>
                <div class="step-content">
                    <div class="substep">
                        <div class="substep-number">1</div>
                        <div class="substep-title">Create Recovery Services Vault</div>
                        <p>In the Azure Portal:</p>
                        <ul>
                            <li>Click "Create a resource" (+)</li>
                            <li>Search for "Recovery Services vault"</li>
                            <li>Click "Recovery Services vault"</li>
                            <li>Click "Create"</li>
                        </ul>
                        
                        <div class="code-block">
Subscription: (Your Azure subscription)
Resource Group: Create new "Backup-RG"
Vault name: enterprise-recovery-vault
Region: East US
                        </div>
                        
                        <p>Click "Review + Create" and then "Create".</p>
                    </div>

                    <div class="substep">
                        <div class="substep-number">2</div>
                        <div class="substep-title">Configure Storage Redundancy</div>
                        <p>Set up appropriate backup storage redundancy:</p>
                        <ol>
                            <li>Go to your created Recovery Services vault</li>
                            <li>Click "Properties" under "Settings"</li>
                            <li>Click "Update" under "Backup Configuration"</li>
                        </ol>
                        
                        <div class="code-block">
Storage replication type: Geo-redundant storage (GRS)

Cross Region Restore: Enable

Description: 
- GRS replicates data to secondary region
- Cross Region Restore allows restore from secondary region
- Provides protection against regional disasters
                        </div>
                        
                        <p>Click "Save" to apply the configuration.</p>
                        
                        <div class="warning">
                            <strong>⚠️ Important:</strong> Storage redundancy cannot be changed after backup items are configured. Choose carefully based on your disaster recovery requirements.
                        </div>
                    </div>
                </div>
            </div>

            <!-- Step 2 -->
            <div class="step-section">
                <div class="step-header">
                    <div class="step-title">Step 2: Creating Test Virtual Machine</div>
                    <div class="step-description">Deploy a VM to demonstrate backup and restore capabilities</div>
                </div>
                <div class="step-content">
                    <div class="substep">
                        <div class="substep-number">1</div>
                        <div class="substep-title">Create Virtual Machine</div>
                        <p>Create a simple VM for backup testing:</p>
                        <ol>
                            <li>Go to "Create a resource" > "Virtual machine"</li>
                            <li>Configure basic settings:</li>
                        </ol>
                        
                        <div class="code-block">
Subscription: (Your subscription)
Resource group: Backup-RG
Virtual machine name: backup-test-vm
Region: East US
Image: Ubuntu Server 20.04 LTS - Gen2
Size: Standard_B1s (1 vcpu, 1 GiB memory)

Administrator account:
Authentication type: Password
Username: azureuser
Password: BackupTest123!

Inbound port rules:
Public inbound ports: Allow selected ports
Select inbound ports: SSH (22)
                        </div>
                        
                        <p>Click "Review + Create" and then "Create".</p>
                    </div>

                    <div class="substep">
                        <div class="substep-number">2</div>
                        <div class="substep-title">Add Sample Data</div>
                        <p>Create some test data on the VM:</p>
                        <ol>
                            <li>Connect to the VM via SSH or Azure Cloud Shell</li>
                            <li>Create sample files:</li>
                        </ol>
                        
                        <div class="code-block">
# SSH to VM (replace with your VM's public IP)
ssh azureuser@<VM-PUBLIC-IP>

# Create sample directories and files
sudo mkdir -p /opt/testdata
sudo mkdir -p /home/azureuser/documents

# Create sample files with different content
echo "Important business data - $(date)" | sudo tee /opt/testdata/business-data.txt
echo "User documents - $(date)" > /home/azureuser/documents/user-file.txt
echo "Configuration file - $(date)" | sudo tee /etc/sample-config.conf

# Create a larger file for testing
dd if=/dev/zero of=/home/azureuser/large-file.bin bs=1M count=100

# List created files
ls -la /opt/testdata/
ls -la /home/azureuser/documents/
sudo ls -la /etc/sample-config.conf
                        </div>
                    </div>
                </div>
            </div>

            <!-- Step 3 -->
            <div class="step-section">
                <div class="step-header">
                    <div class="step-title">Step 3: Configuring VM Backup</div>
                    <div class="step-description">Set up automated backup policies for virtual machines</div>
                </div>
                <div class="step-content">
                    <div class="substep">
                        <div class="substep-number">1</div>
                        <div class="substep-title">Enable VM Backup</div>
                        <p>Configure backup for the test VM:</p>
                        <ol>
                            <li>Go to your Recovery Services vault</li>
                            <li>Click "Backup" under "Getting started"</li>
                            <li>Configure backup settings:</li>
                        </ol>
                        
                        <div class="code-block">
Where is your workload running? Azure
What do you want to back up? Virtual machine

Click "Backup" to continue

Backup policy: Create new policy
Policy name: Enterprise-VM-Policy

Backup schedule:
  Frequency: Daily
  Time: 2:00 AM
  Timezone: (UTC-05:00) Eastern Time

Instant Restore:
  Retain instant recovery snapshot for: 5 Days

Retention range:
  Daily backup retention: 30 Days
  Weekly backup retention: 12 Weeks (every Sunday)
  Monthly backup retention: 12 Months (First Sunday)
  Yearly backup retention: 5 Years (First Sunday of January)
                        </div>
                        
                        <p>Click "OK" to create the policy.</p>
                    </div>

                    <div class="substep">
                        <div class="substep-number">2</div>
                        <div class="substep-title">Add VM to Backup Policy</div>
                        <p>Select and configure VM backup:</p>
                        
                        <div class="code-block">
Select virtual machines:
✓ backup-test-vm

Backup policy: Enterprise-VM-Policy

Resource group for instant restore snapshots:
Create new: "backup-snapshots-rg"

Enable backup: Click to confirm
                        </div>
                        
                        <p>Click "Enable Backup" to start the configuration.</p>
                        
                        <div class="tip">
                            <strong>💡 Best Practice:</strong> Place snapshots in a separate resource group for better organization and cost tracking.
                        </div>
                    </div>

                    <div class="substep">
                        <div class="substep-number">3</div>
                        <div class="substep-title">Trigger Initial Backup</div>
                        <p>Start the first backup immediately:</p>
                        <ol>
                            <li>Go to "Backup items" in your Recovery Services vault</li>
                            <li>Click "Azure Virtual Machine"</li>
                            <li>Find your VM and click on it</li>
                            <li>Click "Backup now"</li>
                        </ol>
                        
                        <div class="code-block">
Backup now settings:
  Retain backup till: (30 days from now)
  
Click "OK" to start backup

Expected duration: 15-30 minutes for initial backup
Status: Monitor in "Backup Jobs" section
                        </div>
                    </div>
                </div>
            </div>

            <!-- Step 4 -->
            <div class="step-section">
                <div class="step-header">
                    <div class="step-title">Step 4: File and Folder Backup</div>
                    <div class="step-description">Configure Azure Backup Agent for file-level backup</div>
                </div>
                <div class="step-content">
                    <div class="substep">
                        <div class="substep-number">1</div>
                        <div class="substep-title">Create Storage Account for File Backup</div>
                        <p>Set up storage for Azure File Shares backup:</p>
                        <ol>
                            <li>Create a new storage account:</li>
                        </ol>
                        
                        <div class="code-block">
Resource group: Backup-RG
Storage account name: backupfiles[random]
Region: East US
Performance: Standard
Redundancy: Geo-redundant storage (GRS)

Advanced settings:
  Require secure transfer: Enabled
  Allow blob public access: Disabled
  Minimum TLS version: Version 1.2
                        </div>
                    </div>

                    <div class="substep">
                        <div class="substep-number">2</div>
                        <div class="substep-title">Create File Share</div>
                        <p>Set up Azure File Share for backup:</p>
                        <ol>
                            <li>Go to your storage account</li>
                            <li>Click "File shares" under "Data storage"</li>
                            <li>Click "+ File share"</li>
                        </ol>
                        
                        <div class="code-block">
File share settings:
  Name: enterprise-fileshare
  Tier: Transaction optimized
  Quota: 100 GB
  
Access tier: Hot
Protocol: SMB
                        </div>
                        
                        <p>Create and upload some sample files to the share.</p>
                    </div>

                    <div class="substep">
                        <div class="substep-number">3</div>
                        <div class="substep-title">Configure File Share Backup</div>
                        <p>Enable backup for Azure File Shares:</p>
                        <ol>
                            <li>Go to your Recovery Services vault</li>
                            <li>Click "Backup" under "Getting started"</li>
                            <li>Configure:</li>
                        </ol>
                        
                        <div class="code-block">
Where is your workload running? Azure
What do you want to back up? Azure File share

Storage Account: backupfiles[random]
File Shares: enterprise-fileshare

Backup Policy: Create new
Policy name: FileShare-Daily-Policy

Schedule:
  Frequency: Daily
  Time: 1:00 AM
  Timezone: (UTC-05:00) Eastern Time

Retention:
  Daily backup retention: 30 Days
  Weekly backup retention: 12 Weeks
  Monthly backup retention: 12 Months
                        </div>
                    </div>
                </div>
            </div>

            <!-- Step 5 -->
            <div class="step-section">
                <div class="step-header">
                    <div class="step-title">Step 5: Testing Backup and Restore</div>
                    <div class="step-description">Validate backup functionality and practice restore procedures</div>
                </div>
                <div class="step-content">
                    <div class="substep">
                        <div class="substep-number">1</div>
                        <div class="substep-title">Monitor Backup Jobs</div>
                        <p>Check backup job status and completion:</p>
                        <ol>
                            <li>Go to your Recovery Services vault</li>
                            <li>Click "Backup jobs" under "Monitoring"</li>
                            <li>Monitor job progress:</li>
                        </ol>
                        
                        <div class="code-block">
Backup Jobs Status:
✓ VM Backup: In Progress/Completed
✓ File Share Backup: In Progress/Completed

Job Details:
- Start time
- Duration
- Status
- Data transferred
- Any warnings or errors
                        </div>
                    </div>

                    <div class="substep">
                        <div class="substep-number">2</div>
                        <div class="substep-title">Test File-Level Restore</div>
                        <p>Practice restoring individual files from VM backup:</p>
                        <ol>
                            <li>Go to "Backup items" > "Azure Virtual Machine"</li>
                            <li>Click on your VM</li>
                            <li>Click "File Recovery"</li>
                        </ol>
                        
                        <div class="code-block">
File Recovery Steps:
1. Select recovery point (most recent)
2. Download executable file
3. Run script on source/target machine
4. Browse mounted backup
5. Copy needed files
6. Unmount after recovery

Recovery point: Latest backup
Script valid for: 12 hours
Mounted as: Drive letter (Windows) or mount point (Linux)
                        </div>
                        
                        <div class="warning">
                            <strong>⚠️ Security Note:</strong> The recovery script contains sensitive information. Download and execute only on trusted machines.
                        </div>
                    </div>

                    <div class="substep">
                        <div class="substep-number">3</div>
                        <div class="substep-title">Test Complete VM Restore</div>
                        <p>Practice full VM restore to validate disaster recovery:</p>
                        <ol>
                            <li>In your VM backup item, click "Restore VM"</li>
                            <li>Configure restore settings:</li>
                        </ol>
                        
                        <div class="code-block">
Restore Configuration:
  Restore point: Latest backup
  Restore Type: Create new virtual machine
  
Virtual machine details:
  Virtual machine name: backup-test-vm-restored
  Resource group: Backup-RG
  Virtual network: (Select existing or create new)
  Subnet: (Select appropriate subnet)
  Storage account: (For staging location)
  
Note: This creates a new VM alongside the original
Original VM remains unchanged
                        </div>
                        
                        <p>Click "Restore" to start the process (this may take 30-60 minutes).</p>
                        
                        <div class="tip">
                            <strong>💡 Production Tip:</strong> Always restore to a different name/location first to avoid overwriting production systems.
                        </div>
                    </div>
                </div>
            </div>

            <!-- Validation Section -->
            <div class="validation-section">
                <h4>🔍 Validation & Testing</h4>
                <ol>
                    <li><strong>Recovery Services Vault:</strong> Verify vault is created with GRS storage</li>
                    <li><strong>VM Backup Enabled:</strong> Confirm backup policy is active and initial backup completed</li>
                    <li><strong>File Share Backup:</strong> Verify file share backup is configured and running</li>
                    <li><strong>Backup Jobs:</strong> Check all backup jobs completed successfully</li>
                    <li><strong>File Recovery:</strong> Test file-level restore functionality</li>
                    <li><strong>VM Restore:</strong> Verify complete VM restore creates functional copy</li>
                    <li><strong>Monitoring:</strong> Confirm backup alerts and monitoring are configured</li>
                    <li><strong>Documentation:</strong> Ensure recovery procedures are documented</li>
                </ol>
            </div>

            <div class="troubleshooting">
                <h4>🛠️ Common Issues & Solutions</h4>
                <ul>
                    <li><strong>Backup jobs failing:</strong> Check VM agent status and network connectivity</li>
                    <li><strong>Insufficient permissions:</strong> Verify backup service has required access to resources</li>
                    <li><strong>Snapshot failures:</strong> Ensure adequate disk space and check disk health</li>
                    <li><strong>Restore taking too long:</strong> Consider instant restore for faster recovery</li>
                    <li><strong>Cross-region restore issues:</strong> Verify GRS is enabled and secondary region is available</li>
                    <li><strong>High backup costs:</strong> Review retention policies and optimize backup frequency</li>
                </ul>
            </div>

            <div class="resource-links">
                <h4>📚 Additional Resources</h4>
                <ul>
                    <li><a href="https://docs.microsoft.com/en-us/azure/backup/" target="_blank">Azure Backup Documentation</a></li>
                    <li><a href="https://docs.microsoft.com/en-us/azure/backup/backup-azure-vms-introduction" target="_blank">VM Backup Overview</a></li>
                    <li><a href="https://docs.microsoft.com/en-us/azure/backup/backup-afs" target="_blank">Azure File Share Backup</a></li>
                    <li><a href="https://docs.microsoft.com/en-us/azure/backup/backup-azure-recovery-services-vault-overview" target="_blank">Recovery Services Vault</a></li>
                </ul>
            </div>
        </div>

        <div id="solution9" class="solution-card">
            <div class="solution-header">
                <div class="solution-title">Assignment 9: Azure Load Balancer Setup</div>
                <div class="solution-meta">
                    <span class="meta-tag">⚖️ Load Balancing</span>
                    <span class="meta-tag">⏱️ 85 minutes</span>
                    <span class="meta-tag">📊 Advanced</span>
                </div>
            </div>
            
            <div class="time-estimate">
                <strong>⏱️ Estimated Time:</strong> 1 hour 25 minutes<br>
                <strong>🎯 Learning Objective:</strong> Implement high availability load balancing with SSL termination and traffic distribution
            </div>

            <div class="prerequisites">
                <h4>📋 Prerequisites</h4>
                <ul>
                    <li>Completed Assignment 2 (Virtual Networks) and Assignment 4 (Web Apps)</li>
                    <li>Understanding of load balancing concepts and high availability</li>
                    <li>Basic knowledge of SSL/TLS certificates and HTTPS</li>
                    <li>Familiarity with DNS and domain management</li>
                </ul>
            </div>

            <!-- Step 1 -->
            <div class="step-section">
                <div class="step-header">
                    <div class="step-title">Step 1: Creating Multiple Backend Web Apps</div>
                    <div class="step-description">Deploy multiple web app instances to demonstrate load balancing</div>
                </div>
                <div class="step-content">
                    <div class="substep">
                        <div class="substep-number">1</div>
                        <div class="substep-title">Create Additional Web Apps</div>
                        <p>Create two more web apps for load balancing:</p>
                        <ol>
                            <li>Go to "Create a resource" > "Web App"</li>
                            <li>Create first additional web app:</li>
                        </ol>
                        
                        <div class="code-block">
Web App 1:
  Name: enterprise-webapp-east-[random]
  Resource Group: WebApp-RG
  Runtime: Node.js 18 LTS
  App Service Plan: Production-AppServicePlan
  Region: East US

Web App 2:
  Name: enterprise-webapp-central-[random]
  Resource Group: WebApp-RG
  Runtime: Node.js 18 LTS
  App Service Plan: Create new "Central-AppServicePlan"
  Region: Central US
                        </div>
                        
                        <div class="tip">
                            <strong>💡 Multi-Region Strategy:</strong> Deploying apps in different regions provides geographic redundancy and improved performance for users.
                        </div>
                    </div>

                    <div class="substep">
                        <div class="substep-number">2</div>
                        <div class="substep-title">Deploy Identification Code</div>
                        <p>Add server identification to each web app:</p>
                        
                        <div class="code-block">
// Enhanced server.js for load balancing testing
const express = require('express');
const os = require('os');
const app = express();
const port = process.env.PORT || 3000;

// Server identification
const SERVER_ID = process.env.WEBSITE_SITE_NAME || 'local-server';
const REGION = process.env.WEBSITE_RESOURCE_GROUP || 'unknown-region';

app.get('/', (req, res) => {
  res.json({
    message: 'Welcome to Enterprise Web App!',
    serverId: SERVER_ID,
    region: REGION,
    hostname: os.hostname(),
    timestamp: new Date().toISOString(),
    requestId: Math.random().toString(36).substr(2, 9)
  });
});

app.get('/health', (req, res) => {
  res.json({
    status: 'healthy',
    serverId: SERVER_ID,
    uptime: process.uptime(),
    timestamp: new Date().toISOString()
  });
});

app.get('/load-test', (req, res) => {
  // Simulate some processing load
  const start = Date.now();
  let sum = 0;
  for (let i = 0; i < 1000000; i++) {
    sum += Math.random();
  }
  const processingTime = Date.now() - start;
  
  res.json({
    serverId: SERVER_ID,
    processingTime: `${processingTime}ms`,
    result: sum,
    timestamp: new Date().toISOString()
  });
});

app.listen(port, () => {
  console.log(`Server ${SERVER_ID} running on port ${port}`);
});
                        </div>
                        
                        <p>Deploy this code to all three web apps using App Service Editor.</p>
                    </div>
                </div>
            </div>

            <!-- Step 2 -->
            <div class="step-section">
                <div class="step-header">
                    <div class="step-title">Step 2: Creating Application Gateway</div>
                    <div class="step-description">Set up Layer 7 load balancer with advanced traffic management</div>
                </div>
                <div class="step-content">
                    <div class="substep">
                        <div class="substep-number">1</div>
                        <div class="substep-title">Create Application Gateway</div>
                        <p>Create the Application Gateway for load balancing:</p>
                        <ol>
                            <li>Go to "Create a resource" > Search "Application Gateway"</li>
                            <li>Click "Application Gateway" and "Create"</li>
                        </ol>
                        
                        <div class="code-block">
Basics:
  Subscription: (Your subscription)
  Resource group: Create new "LoadBalancer-RG"
  Application gateway name: enterprise-app-gateway
  Region: East US
  Tier: Standard_v2
  Enable autoscaling: Yes
  Instance count: Min 2, Max 10

Availability zone: None (for cost savings in demo)
HTTP2: Enabled
                        </div>
                    </div>

                    <div class="substep">
                        <div class="substep-number">2</div>
                        <div class="substep-title">Configure Virtual Network</div>
                        <p>Set up network configuration for the Application Gateway:</p>
                        
                        <div class="code-block">
Virtual network: Create new "AppGateway-VNet"
Address space: 10.1.0.0/16

Subnet configuration:
  Subnet name: AppGateway-Subnet
  Subnet address range: 10.1.0.0/24
  
Note: Application Gateway requires its own dedicated subnet
                        </div>
                        
                        <div class="warning">
                            <strong>⚠️ Network Requirement:</strong> Application Gateway must be deployed in its own subnet and cannot share it with other resources.
                        </div>
                    </div>

                    <div class="substep">
                        <div class="substep-number">3</div>
                        <div class="substep-title">Configure Frontend IP</div>
                        <p>Set up public IP for external access:</p>
                        
                        <div class="code-block">
Frontend IP configurations:
  IP address type: Public
  Public IP address: Create new
  
New public IP:
  Name: AppGateway-PublicIP
  SKU: Standard
  Assignment: Static
  DNS name label: enterprise-gateway-[random]
                        </div>
                    </div>
                </div>
            </div>

            <!-- Step 3 -->
            <div class="step-section">
                <div class="step-header">
                    <div class="step-title">Step 3: Configuring Backend Pools and Health Probes</div>
                    <div class="step-description">Set up backend targets and health monitoring</div>
                </div>
                <div class="step-content">
                    <div class="substep">
                        <div class="substep-number">1</div>
                        <div class="substep-title">Create Backend Pool</div>
                        <p>Configure backend pool with web app targets:</p>
                        
                        <div class="code-block">
Backend pools:
  Pool name: WebApp-BackendPool
  
Target type: App Service
Targets:
  1. enterprise-webapp-[original].azurewebsites.net
  2. enterprise-webapp-east-[random].azurewebsites.net
  3. enterprise-webapp-central-[random].azurewebsites.net
                        </div>
                    </div>

                    <div class="substep">
                        <div class="substep-number">2</div>
                        <div class="substep-title">Configure Health Probe</div>
                        <p>Set up health monitoring for backend services:</p>
                        
                        <div class="code-block">
Health probe configuration:
  Name: WebApp-HealthProbe
  Protocol: HTTPS
  Host: Pick host name from backend target
  Path: /health
  Interval: 30 seconds
  Timeout: 30 seconds
  Unhealthy threshold: 3
  
HTTP settings:
  Use for App Service: Yes
  Override with new host name: Yes
  Host name override: Pick host name from backend target
                        </div>
                        
                        <div class="tip">
                            <strong>💡 Health Monitoring:</strong> The /health endpoint we created earlier will be used to monitor application health and remove unhealthy instances from load balancing.
                        </div>
                    </div>

                    <div class="substep">
                        <div class="substep-number">3</div>
                        <div class="substep-title">Configure HTTP Settings</div>
                        <p>Set up backend HTTP settings:</p>
                        
                        <div class="code-block">
HTTP settings:
  Name: WebApp-HTTPSettings
  Backend protocol: HTTPS
  Backend port: 443
  Cookie-based affinity: Disabled
  Request timeout: 30 seconds
  
Custom probe: WebApp-HealthProbe
Host name: Override with specific domain name
Host name: Pick host name from backend target
                        </div>
                    </div>
                </div>
            </div>

            <!-- Step 4 -->
            <div class="step-section">
                <div class="step-header">
                    <div class="step-title">Step 4: Configuring Listeners and Routing Rules</div>
                    <div class="step-description">Set up traffic routing and SSL termination</div>
                </div>
                <div class="step-content">
                    <div class="substep">
                        <div class="substep-number">1</div>
                        <div class="substep-title">Create HTTP Listener</div>
                        <p>Configure basic HTTP listener:</p>
                        
                        <div class="code-block">
Listener configuration:
  Listener name: HTTP-Listener
  Frontend IP: Public
  Protocol: HTTP
  Port: 80
  Additional settings: Basic
                        </div>
                    </div>

                    <div class="substep">
                        <div class="substep-number">2</div>
                        <div class="substep-title">Create Routing Rule</div>
                        <p>Set up traffic routing from listener to backend:</p>
                        
                        <div class="code-block">
Routing rule:
  Rule name: WebApp-RoutingRule
  Priority: 100
  
Listener: HTTP-Listener
Backend targets:
  Target type: Backend pool
  Backend target: WebApp-BackendPool
  Backend settings: WebApp-HTTPSettings
                        </div>
                        
                        <p>Click "Add" to create the routing rule.</p>
                    </div>

                    <div class="substep">
                        <div class="substep-number">3</div>
                        <div class="substep-title">Review and Create</div>
                        <p>Complete the Application Gateway deployment:</p>
                        <ol>
                            <li>Go to "Review + Create" tab</li>
                            <li>Review all configurations</li>
                            <li>Click "Create"</li>
                            <li>Wait for deployment (10-15 minutes)</li>
                        </ol>
                        
                        <div class="warning">
                            <strong>⚠️ Deployment Time:</strong> Application Gateway deployment takes significantly longer than other resources. Be patient during the creation process.
                        </div>
                    </div>
                </div>
            </div>

            <!-- Step 5 -->
            <div class="step-section">
                <div class="step-header">
                    <div class="step-title">Step 5: Testing Load Balancing and Monitoring</div>
                    <div class="step-description">Validate traffic distribution and performance monitoring</div>
                </div>
                <div class="step-content">
                    <div class="substep">
                        <div class="substep-number">1</div>
                        <div class="substep-title">Test Load Distribution</div>
                        <p>Verify that traffic is being distributed across backend servers:</p>
                        
                        <div class="code-block">
# Get Application Gateway public IP
# Visit: http://your-app-gateway-ip/

# Refresh multiple times and observe:
# - Different serverId values
# - Different region values  
# - Different hostname values

# Test load endpoint:
# Visit: http://your-app-gateway-ip/load-test
# Refresh multiple times to see different servers responding

# Expected rotation between:
# - enterprise-webapp-[original]
# - enterprise-webapp-east-[random]
# - enterprise-webapp-central-[random]
                        </div>
                        
                        <div class="success">
                            <strong>✅ Success Indicator:</strong> You should see requests being distributed among different servers, indicated by varying serverId values.
                        </div>
                    </div>

                    <div class="substep">
                        <div class="substep-number">2</div>
                        <div class="substep-title">Monitor Backend Health</div>
                        <p>Check backend pool health status:</p>
                        <ol>
                            <li>Go to your Application Gateway in Azure portal</li>
                            <li>Click "Backend health" under "Monitoring"</li>
                            <li>Verify all backends show as "Healthy"</li>
                        </ol>
                        
                        <div class="code-block">
Backend health status:
✓ WebApp-BackendPool
  ✓ enterprise-webapp-[original].azurewebsites.net - Healthy
  ✓ enterprise-webapp-east-[random].azurewebsites.net - Healthy
  ✓ enterprise-webapp-central-[random].azurewebsites.net - Healthy

Health probe details:
  Protocol: HTTPS
  Path: /health
  Response: 200 OK
  Response time: < 30 seconds
                        </div>
                    </div>

                    <div class="substep">
                        <div class="substep-number">3</div>
                        <div class="substep-title">Test Failover Scenario</div>
                        <p>Simulate server failure to test high availability:</p>
                        <ol>
                            <li>Stop one of your web apps</li>
                            <li>Monitor backend health</li>
                            <li>Test continued service availability</li>
                        </ol>
                        
                        <div class="code-block">
Failover test:
1. Go to one web app and click "Stop"
2. Wait 2-3 minutes for health probe to detect failure
3. Check backend health - should show one as "Unhealthy"
4. Continue testing load balancer URL
5. Traffic should only go to healthy backends
6. Restart the stopped web app
7. Verify it returns to "Healthy" status
                        </div>
                        
                        <div class="tip">
                            <strong>💡 Production Benefit:</strong> This demonstrates how Application Gateway automatically removes unhealthy backends from rotation, ensuring high availability.
                        </div>
                    </div>

                    <div class="substep">
                        <div class="substep-number">4</div>
                        <div class="substep-title">Configure Custom Domain (Optional)</div>
                        <p>Set up a custom domain for production-like setup:</p>
                        
                        <div class="code-block">
Custom domain setup:
1. Create CNAME record pointing to Application Gateway public IP
2. Add multi-site listener in Application Gateway
3. Create new routing rule for custom domain
4. Update SSL certificate for HTTPS

Example DNS configuration:
Type: CNAME
Name: www
Value: enterprise-gateway-[random].eastus.cloudapp.azure.com
TTL: 300
                        </div>
                    </div>
                </div>
            </div>

            <!-- Validation Section -->
            <div class="validation-section">
                <h4>🔍 Validation & Testing</h4>
                <ol>
                    <li><strong>Application Gateway Deployed:</strong> Verify gateway is running and accessible</li>
                    <li><strong>Backend Pool Configured:</strong> Confirm all three web apps are in backend pool</li>
                    <li><strong>Health Probes Working:</strong> Check all backends show as "Healthy"</li>
                    <li><strong>Load Distribution:</strong> Verify traffic rotates between different servers</li>
                    <li><strong>Failover Testing:</strong> Confirm unhealthy backends are removed from rotation</li>
                    <li><strong>Performance:</strong> Test response times and throughput</li>
                    <li><strong>Monitoring:</strong> Verify metrics and logs are being collected</li>
                    <li><strong>Auto-scaling:</strong> Check that gateway scales based on traffic</li>
                </ol>
            </div>

            <div class="troubleshooting">
                <h4>🛠️ Common Issues & Solutions</h4>
                <ul>
                    <li><strong>502 Bad Gateway errors:</strong> Check backend health and HTTP settings configuration</li>
                    <li><strong>Health probes failing:</strong> Verify /health endpoint is accessible and returns 200 OK</li>
                    <li><strong>Uneven load distribution:</strong> Disable cookie-based affinity and check routing rules</li>
                    <li><strong>Slow response times:</strong> Check backend pool health and App Service performance</li>
                    <li><strong>SSL/TLS issues:</strong> Verify certificate configuration and backend HTTPS settings</li>
                    <li><strong>High costs:</strong> Monitor auto-scaling settings and instance counts</li>
                </ul>
            </div>

            <div class="resource-links">
                <h4>📚 Additional Resources</h4>
                <ul>
                    <li><a href="https://docs.microsoft.com/en-us/azure/application-gateway/" target="_blank">Azure Application Gateway Documentation</a></li>
                    <li><a href="https://docs.microsoft.com/en-us/azure/application-gateway/application-gateway-probe-overview" target="_blank">Health Probes Overview</a></li>
                    <li><a href="https://docs.microsoft.com/en-us/azure/application-gateway/ssl-overview" target="_blank">SSL Termination Guide</a></li>
                    <li><a href="https://docs.microsoft.com/en-us/azure/application-gateway/autoscaling-zone-redundant" target="_blank">Auto-scaling Configuration</a></li>
                </ul>
            </div>
        </div>

        <div id="solution10" class="solution-card">
            <div class="solution-header">
                <div class="solution-title">Assignment 10: Azure CDN Configuration</div>
                <div class="solution-meta">
                    <span class="meta-tag">🌐 CDN</span>
                    <span class="meta-tag">⏱️ 60 minutes</span>
                    <span class="meta-tag">📊 Beginner</span>
                </div>
            </div>
            
            <div class="time-estimate">
                <strong>⏱️ Estimated Time:</strong> 1 hour<br>
                <strong>🎯 Learning Objective:</strong> Implement global content delivery with caching and performance optimization
            </div>

            <div class="prerequisites">
                <h4>📋 Prerequisites</h4>
                <ul>
                    <li>Completed Assignment 4 (Web App) or Assignment 9 (Load Balancer)</li>
                    <li>Understanding of content delivery networks and caching</li>
                    <li>Basic knowledge of HTTP headers and cache control</li>
                    <li>Familiarity with static content (images, CSS, JavaScript)</li>
                </ul>
            </div>

            <!-- Step 1 -->
            <div class="step-section">
                <div class="step-header">
                    <div class="step-title">Step 1: Creating Static Content for CDN</div>
                    <div class="step-description">Set up static assets and storage for CDN acceleration</div>
                </div>
                <div class="step-content">
                    <div class="substep">
                        <div class="substep-number">1</div>
                        <div class="substep-title">Create Storage Account for Static Content</div>
                        <p>Set up storage for static assets:</p>
                        <ol>
                            <li>Go to "Create a resource" > "Storage Account"</li>
                            <li>Configure storage account:</li>
                        </ol>
                        
                        <div class="code-block">
Subscription: (Your subscription)
Resource Group: Create new "CDN-RG"
Storage account name: cdnstatic[random]
Region: East US
Performance: Standard
Redundancy: Geo-redundant storage (GRS)

Advanced settings:
  Allow blob public access: Enabled
  Minimum TLS version: Version 1.2
  Enable hierarchical namespace: Disabled
                        </div>
                        
                        <p>Click "Review + Create" and then "Create".</p>
                    </div>

                    <div class="substep">
                        <div class="substep-number">2</div>
                        <div class="substep-title">Create Public Blob Container</div>
                        <p>Set up container for public static content:</p>
                        <ol>
                            <li>Go to your storage account</li>
                            <li>Click "Containers" under "Data storage"</li>
                            <li>Click "+ Container"</li>
                        </ol>
                        
                        <div class="code-block">
Container configuration:
  Name: static-assets
  Public access level: Container (anonymous read access)
  
Description: Container for publicly accessible static content
                        </div>
                        
                        <div class="warning">
                            <strong>⚠️ Security Note:</strong> Only set public access for content that should be publicly accessible. Never expose sensitive data through public containers.
                        </div>
                    </div>

                    <div class="substep">
                        <div class="substep-number">3</div>
                        <div class="substep-title">Upload Sample Static Content</div>
                        <p>Add various types of static content for testing:</p>
                        
                        <div class="code-block">
# Create sample files to upload:

1. styles.css:
body {
    font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
    background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
    color: white;
    margin: 0;
    padding: 20px;
}

.container {
    max-width: 1200px;
    margin: 0 auto;
    text-align: center;
}

.hero {
    background: rgba(255,255,255,0.1);
    padding: 40px;
    border-radius: 10px;
    backdrop-filter: blur(10px);
}

2. app.js:
console.log('CDN JavaScript loaded successfully!');

document.addEventListener('DOMContentLoaded', function() {
    const timestamp = new Date().toISOString();
    const statusDiv = document.createElement('div');
    statusDiv.innerHTML = `<p>CDN Content loaded at: ${timestamp}</p>`;
    statusDiv.style.cssText = 'background: rgba(255,255,255,0.2); padding: 10px; margin: 20px 0; border-radius: 5px;';
    document.body.appendChild(statusDiv);
});

3. sample-image.svg:
<svg width="200" height="200" xmlns="http://www.w3.org/2000/svg">
  <rect width="200" height="200" fill="#4CAF50"/>
  <text x="100" y="100" text-anchor="middle" dy=".3em" 
        fill="white" font-size="16" font-family="Arial">
    CDN Test Image
  </text>
  <circle cx="100" cy="150" r="20" fill="white"/>
</svg>
                        </div>
                        
                        <p>Upload these files to the static-assets container.</p>
                    </div>
                </div>
            </div>

            <!-- Step 2 -->
            <div class="step-section">
                <div class="step-header">
                    <div class="step-title">Step 2: Creating Azure CDN Profile</div>
                    <div class="step-description">Set up CDN infrastructure and configuration</div>
                </div>
                <div class="step-content">
                    <div class="substep">
                        <div class="substep-number">1</div>
                        <div class="substep-title">Create CDN Profile</div>
                        <p>Set up the CDN profile:</p>
                        <ol>
                            <li>Go to "Create a resource" > Search "CDN"</li>
                            <li>Click "Front Door and CDN profiles"</li>
                            <li>Click "Create"</li>
                        </ol>
                        
                        <div class="code-block">
CDN Profile Configuration:
  Subscription: (Your subscription)
  Resource group: CDN-RG
  Name: enterprise-cdn-profile
  Pricing tier: Standard Microsoft
  
Create new CDN endpoint: Yes
  CDN endpoint name: enterprise-cdn-[random]
  Origin type: Storage static website
  Origin hostname: cdnstatic[random].blob.core.windows.net
  Origin path: /static-assets
                        </div>
                        
                        <div class="tip">
                            <strong>💡 Pricing Tiers:</strong> Standard Microsoft tier provides good performance and features. Premium Verizon offers advanced analytics but costs more.
                        </div>
                    </div>

                    <div class="substep">
                        <div class="substep-number">2</div>
                        <div class="substep-title">Configure Endpoint Settings</div>
                        <p>Optimize CDN endpoint for performance:</p>
                        <ol>
                            <li>Go to your CDN endpoint after creation</li>
                            <li>Click "Endpoint properties"</li>
                            <li>Configure optimization settings:</li>
                        </ol>
                        
                        <div class="code-block">
Optimization settings:
  Optimization type: General web delivery
  
Compression:
  Enable compression: Yes
  Compression file types: 
    - text/css
    - text/javascript
    - application/javascript
    - text/html
    - application/json
    - text/xml
    - application/xml

Query string caching:
  Query string caching behavior: Ignore query strings
  
Geo-filtering:
  Filter type: None (allow all countries)
                        </div>
                    </div>

                    <div class="substep">
                        <div class="substep-number">3</div>
                        <div class="substep-title">Configure Custom Caching Rules</div>
                        <p>Set up cache duration for different content types:</p>
                        
                        <div class="code-block">
Caching rules:

Rule 1 - CSS Files:
  Name: CSS-Cache-Rule
  Match conditions:
    Path: *.css
  Actions:
    Cache expiration: Override
    Cache duration: 7 days
    
Rule 2 - JavaScript Files:
  Name: JS-Cache-Rule  
  Match conditions:
    Path: *.js
  Actions:
    Cache expiration: Override
    Cache duration: 7 days
    
Rule 3 - Images:
  Name: Image-Cache-Rule
  Match conditions:
    Path: *.svg, *.png, *.jpg, *.gif
  Actions:
    Cache expiration: Override
    Cache duration: 30 days
                        </div>
                    </div>
                </div>
            </div>

            <!-- Step 3 -->
            <div class="step-section">
                <div class="step-header">
                    <div class="step-title">Step 3: Testing CDN Performance</div>
                    <div class="step-description">Validate content delivery and measure performance improvements</div>
                </div>
                <div class="step-content">
                    <div class="substep">
                        <div class="substep-number">1</div>
                        <div class="substep-title">Test Direct Access vs CDN</div>
                        <p>Compare performance between direct storage access and CDN:</p>
                        
                        <div class="code-block">
# Direct storage URLs:
https://cdnstatic[random].blob.core.windows.net/static-assets/styles.css
https://cdnstatic[random].blob.core.windows.net/static-assets/app.js
https://cdnstatic[random].blob.core.windows.net/static-assets/sample-image.svg

# CDN URLs (replace with your endpoint):
https://enterprise-cdn-[random].azureedge.net/styles.css
https://enterprise-cdn-[random].azureedge.net/app.js
https://enterprise-cdn-[random].azureedge.net/sample-image.svg

# Testing steps:
1. Access files directly from storage (note response time)
2. Access files through CDN (note response time)
3. Access CDN files multiple times (observe caching)
4. Check response headers for cache status
                        </div>
                    </div>

                    <div class="substep">
                        <div class="substep-number">2</div>
                        <div class="substep-title">Create Test Web Page</div>
                        <p>Build a test page that uses CDN resources:</p>
                        
                        <div class="code-block">
# Update your web app (from Assignment 4) with CDN integration
# Add this to your web app's public folder or update server.js:

<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>CDN Performance Test</title>
    <link rel="stylesheet" href="https://enterprise-cdn-[random].azureedge.net/styles.css">
</head>
<body>
    <div class="container">
        <div class="hero">
            <h1>CDN Performance Test</h1>
            <img src="https://enterprise-cdn-[random].azureedge.net/sample-image.svg" 
                 alt="CDN Test Image" 
                 style="max-width: 200px; margin: 20px;">
            <p>This page loads static assets from Azure CDN</p>
            <div id="performance-info"></div>
        </div>
    </div>
    
    <script src="https://enterprise-cdn-[random].azureedge.net/app.js"></script>
    <script>
        // Performance measurement
        window.addEventListener('load', function() {
            const loadTime = window.performance.timing.loadEventEnd - 
                           window.performance.timing.navigationStart;
            document.getElementById('performance-info').innerHTML = 
                `<p>Page load time: ${loadTime}ms</p>`;
        });
    </script>
</body>
</html>
                        </div>
                    </div>

                    <div class="substep">
                        <div class="substep-number">3</div>
                        <div class="substep-title">Analyze CDN Performance</div>
                        <p>Monitor CDN metrics and performance:</p>
                        
                        <div class="code-block">
Performance analysis:

1. Browser Developer Tools:
   - Open Network tab
   - Reload page multiple times
   - Check response headers:
     * Cache-Control: public, max-age=604800
     * X-Cache: HIT (cached) vs MISS (not cached)
     * Response time improvements

2. CDN Analytics (in Azure portal):
   - Go to CDN endpoint > Analytics
   - Monitor:
     * Hit ratio (should increase over time)
     * Bandwidth usage
     * Request count
     * Origin traffic vs edge traffic

3. Performance improvements to observe:
   - Faster load times (especially repeat visits)
   - Reduced origin server load
   - Better performance from different global locations
                        </div>
                        
                        <div class="success">
                            <strong>✅ Performance Indicators:</strong> CDN hits should show faster response times and cache headers indicating successful content delivery from edge locations.
                        </div>
                    </div>
                </div>
            </div>

            <!-- Step 4 -->
            <div class="step-section">
                <div class="step-header">
                    <div class="step-title">Step 4: Advanced CDN Configuration</div>
                    <div class="step-description">Configure advanced features like custom domains and SSL</div>
                </div>
                <div class="step-content">
                    <div class="substep">
                        <div class="substep-number">1</div>
                        <div class="substep-title">Configure Custom Domain (Optional)</div>
                        <p>Set up a custom domain for branded CDN URLs:</p>
                        
                        <div class="code-block">
Custom domain setup:

1. DNS Configuration:
   Create CNAME record:
   Name: cdn
   Value: enterprise-cdn-[random].azureedge.net
   TTL: 300

2. Add custom domain in Azure:
   - Go to CDN endpoint
   - Click "Custom domains"
   - Add domain: cdn.yourdomain.com
   - Enable HTTPS: Yes
   - Certificate management: CDN managed

3. Update references:
   Replace azureedge.net URLs with custom domain
   https://cdn.yourdomain.com/styles.css
                        </div>
                    </div>

                    <div class="substep">
                        <div class="substep-number">2</div>
                        <div class="substep-title">Configure Geographic Filtering</div>
                        <p>Set up geo-blocking if needed for compliance:</p>
                        
                        <div class="code-block">
Geo-filtering configuration:

Filter type: Allow list (for specific countries)
OR
Filter type: Block list (to block specific countries)

Example - Allow only specific regions:
  Allowed countries: 
    - United States
    - Canada  
    - United Kingdom
    - European Union countries
    
Path: / (applies to all content)
                        </div>
                    </div>

                    <div class="substep">
                        <div class="substep-number">3</div>
                        <div class="substep-title">Monitor and Optimize</div>
                        <p>Set up monitoring and optimization practices:</p>
                        
                        <div class="code-block">
Monitoring setup:

1. Enable diagnostic logs:
   - Core analytics
   - HTTP access logs
   - Send to Log Analytics workspace

2. Set up alerts:
   - High bandwidth usage
   - Low hit ratio
   - Origin health issues
   - Error rate thresholds

3. Regular optimization:
   - Review cache hit ratios
   - Analyze popular content
   - Adjust cache durations
   - Update compression settings
   - Monitor origin server performance
                        </div>
                    </div>
                </div>
            </div>

            <!-- Validation Section -->
            <div class="validation-section">
                <h4>🔍 Validation & Testing</h4>
                <ol>
                    <li><strong>CDN Profile Created:</strong> Verify CDN profile and endpoint are operational</li>
                    <li><strong>Content Accessible:</strong> Confirm static assets load through CDN URLs</li>
                    <li><strong>Caching Working:</strong> Verify cache headers and hit/miss status</li>
                    <li><strong>Performance Improved:</strong> Measure faster load times compared to origin</li>
                    <li><strong>Compression Enabled:</strong> Check that text assets are compressed</li>
                    <li><strong>Global Distribution:</strong> Test access from different geographic locations</li>
                    <li><strong>Analytics Functional:</strong> Verify CDN metrics are being collected</li>
                    <li><strong>Custom Rules Applied:</strong> Confirm caching rules work as configured</li>
                </ol>
            </div>

            <div class="troubleshooting">
                <h4>🛠️ Common Issues & Solutions</h4>
                <ul>
                    <li><strong>404 errors from CDN:</strong> Check origin path configuration and file accessibility</li>
                    <li><strong>Content not updating:</strong> Purge CDN cache or check cache duration settings</li>
                    <li><strong>Poor cache hit ratio:</strong> Review caching rules and query string handling</li>
                    <li><strong>Slow initial load:</strong> Expected for first request; subsequent requests should be faster</li>
                    <li><strong>CORS errors:</strong> Configure CORS settings on origin storage account</li>
                    <li><strong>SSL certificate issues:</strong> Wait for certificate provisioning or check custom domain setup</li>
                </ul>
            </div>

            <div class="resource-links">
                <h4>📚 Additional Resources</h4>
                <ul>
                    <li><a href="https://docs.microsoft.com/en-us/azure/cdn/" target="_blank">Azure CDN Documentation</a></li>
                    <li><a href="https://docs.microsoft.com/en-us/azure/cdn/cdn-optimization-overview" target="_blank">CDN Optimization Guide</a></li>
                    <li><a href="https://docs.microsoft.com/en-us/azure/cdn/cdn-custom-ssl" target="_blank">Custom SSL Configuration</a></li>
                    <li><a href="https://docs.microsoft.com/en-us/azure/cdn/cdn-analyze-usage-patterns" target="_blank">CDN Analytics and Monitoring</a></li>
                </ul>
            </div>
        </div>

        <div id="solution11" class="solution-card">
            <div class="solution-header">
                <div class="solution-title">Assignment 11: Azure DevOps Pipeline Setup</div>
                <div class="solution-meta">
                    <span class="meta-tag">�️ DevOps</span>
                    <span class="meta-tag">⏱️ 105 minutes</span>
                    <span class="meta-tag">📊 Advanced</span>
                </div>
            </div>
            
            <div class="time-estimate">
                <strong>⏱️ Estimated Time:</strong> 1 hour 45 minutes<br>
                <strong>🎯 Learning Objective:</strong> Implement CI/CD pipelines with automated testing and deployment to Azure
            </div>

            <div class="prerequisites">
                <h4>📋 Prerequisites</h4>
                <ul>
                    <li>Completed Assignment 4 (Web App) or Assignment 9 (Load Balancer)</li>
                    <li>Azure DevOps organization (free tier available)</li>
                    <li>GitHub account or Azure Repos for source control</li>
                    <li>Basic understanding of CI/CD concepts</li>
                    <li>Familiarity with YAML syntax and version control</li>
                </ul>
            </div>

            <!-- Step 1 -->
            <div class="step-section">
                <div class="step-header">
                    <div class="step-title">Step 1: Setting Up Azure DevOps Project</div>
                    <div class="step-description">Create project and configure source control repository</div>
                </div>
                <div class="step-content">
                    <div class="substep">
                        <div class="substep-number">1</div>
                        <div class="substep-title">Create Azure DevOps Organization</div>
                        <p>Set up DevOps organization and project:</p>
                        <ol>
                            <li>Go to <a href="https://dev.azure.com" target="_blank">https://dev.azure.com</a></li>
                            <li>Sign in with your Azure account</li>
                            <li>Create new organization (if needed)</li>
                        </ol>
                        
                        <div class="code-block">
Organization setup:
  Organization name: [yourname]-enterprise-devops
  Region: Select closest to your location
  
Project setup:
  Project name: EnterpriseWebApp
  Visibility: Private
  Version control: Git
  Work item process: Agile
                        </div>
                        
                        <div class="tip">
                            <strong>💡 Free Tier:</strong> Azure DevOps provides free CI/CD for open source projects and up to 5 users for private projects.
                        </div>
                    </div>

                    <div class="substep">
                        <div class="substep-number">2</div>
                        <div class="substep-title">Initialize Repository</div>
                        <p>Set up source control with sample application:</p>
                        <ol>
                            <li>Go to "Repos" in your DevOps project</li>
                            <li>Initialize repository with README</li>
                            <li>Clone repository locally or use web editor</li>
                        </ol>
                        
                        <div class="code-block">
# Clone repository
git clone https://[organization]@dev.azure.com/[organization]/EnterpriseWebApp/_git/EnterpriseWebApp
cd EnterpriseWebApp

# Or use Azure DevOps web interface to create files
                        </div>
                    </div>

                    <div class="substep">
                        <div class="substep-number">3</div>
                        <div class="substep-title">Create Sample Application</div>
                        <p>Add a Node.js application with tests:</p>
                        
                        <div class="code-block">
# package.json
{
  "name": "enterprise-webapp",
  "version": "1.0.0",
  "description": "Enterprise web application with CI/CD",
  "main": "server.js",
  "scripts": {
    "start": "node server.js",
    "test": "jest",
    "test:coverage": "jest --coverage",
    "lint": "eslint ."
  },
  "dependencies": {
    "express": "^4.18.2",
    "helmet": "^7.0.0",
    "cors": "^2.8.5"
  },
  "devDependencies": {
    "jest": "^29.0.0",
    "supertest": "^6.3.0",
    "eslint": "^8.45.0"
  },
  "engines": {
    "node": ">=18.0.0"
  }
}

# server.js
const express = require('express');
const helmet = require('helmet');
const cors = require('cors');
const app = express();
const port = process.env.PORT || 3000;

// Security middleware
app.use(helmet());
app.use(cors());
app.use(express.json());

// Health check endpoint
app.get('/health', (req, res) => {
  res.status(200).json({
    status: 'healthy',
    timestamp: new Date().toISOString(),
    version: process.env.BUILD_NUMBER || '1.0.0'
  });
});

// Main endpoint
app.get('/', (req, res) => {
  res.json({
    message: 'Enterprise Web App - CI/CD Pipeline Demo',
    version: process.env.BUILD_NUMBER || '1.0.0',
    environment: process.env.NODE_ENV || 'development',
    timestamp: new Date().toISOString()
  });
});

if (require.main === module) {
  app.listen(port, () => {
    console.log(`Server running on port ${port}`);
  });
}

module.exports = app;
                        </div>
                        
                        <p>Commit and push these files to your repository.</p>
                    </div>
                </div>
            </div>

            <!-- Step 2 -->
            <div class="step-section">
                <div class="step-header">
                    <div class="step-title">Step 2: Creating Build Pipeline (CI)</div>
                    <div class="step-description">Set up continuous integration with automated testing</div>
                </div>
                <div class="step-content">
                    <div class="substep">
                        <div class="substep-number">1</div>
                        <div class="substep-title">Create Build Pipeline YAML</div>
                        <p>Create CI pipeline configuration:</p>
                        
                        <div class="code-block">
# azure-pipelines.yml
trigger:
  branches:
    include:
      - main
      - develop
  paths:
    exclude:
      - README.md
      - docs/*

variables:
  nodeVersion: '18.x'
  buildConfiguration: 'Release'

pool:
  vmImage: 'ubuntu-latest'

stages:
- stage: Build
  displayName: 'Build and Test'
  jobs:
  - job: BuildJob
    displayName: 'Build Application'
    steps:
    - task: NodeTool@0
      displayName: 'Install Node.js'
      inputs:
        versionSpec: $(nodeVersion)
    
    - task: Cache@2
      displayName: 'Cache npm packages'
      inputs:
        key: 'npm | "$(Agent.OS)" | package-lock.json'
        restoreKeys: |
          npm | "$(Agent.OS)"
        path: ~/.npm
    
    - script: |
        npm ci
      displayName: 'Install dependencies'
    
    - script: |
        npm run lint
      displayName: 'Run ESLint'
      continueOnError: false
    
    - script: |
        npm run test:coverage
      displayName: 'Run tests with coverage'
    
    - task: PublishTestResults@2
      displayName: 'Publish test results'
      condition: succeededOrFailed()
      inputs:
        testResultsFormat: 'JUnit'
        testResultsFiles: '**/test-results.xml'
        mergeTestResults: true
    
    - task: ArchiveFiles@2
      displayName: 'Archive application'
      inputs:
        rootFolderOrFile: '$(System.DefaultWorkingDirectory)'
        includeRootFolder: false
        archiveType: 'zip'
        archiveFile: '$(Build.ArtifactStagingDirectory)/$(Build.BuildId).zip'
        replaceExistingArchive: true
    
    - task: PublishBuildArtifacts@1
      displayName: 'Publish build artifacts'
      inputs:
        pathToPublish: '$(Build.ArtifactStagingDirectory)'
        artifactName: 'drop'
                        </div>
                    </div>

                    <div class="substep">
                        <div class="substep-number">2</div>
                        <div class="substep-title">Set Up Pipeline in Azure DevOps</div>
                        <p>Create the build pipeline:</p>
                        <ol>
                            <li>Go to "Pipelines" > "Create Pipeline"</li>
                            <li>Select "Azure Repos Git"</li>
                            <li>Choose your repository</li>
                            <li>Select "Existing Azure Pipelines YAML file"</li>
                            <li>Choose "/azure-pipelines.yml"</li>
                            <li>Review and run the pipeline</li>
                        </ol>
                        
                        <div class="success">
                            <strong>✅ Success Indicator:</strong> Pipeline should complete successfully with green checkmarks for all stages.
                        </div>
                    </div>
                </div>
            </div>

            <!-- Step 3 -->
            <div class="step-section">
                <div class="step-header">
                    <div class="step-title">Step 3: Creating Release Pipeline (CD)</div>
                    <div class="step-description">Set up continuous deployment to Azure App Service</div>
                </div>
                <div class="step-content">
                    <div class="substep">
                        <div class="substep-number">1</div>
                        <div class="substep-title">Create Service Connection</div>
                        <p>Connect Azure DevOps to your Azure subscription:</p>
                        <ol>
                            <li>Go to "Project Settings" > "Service connections"</li>
                            <li>Click "New service connection"</li>
                            <li>Select "Azure Resource Manager"</li>
                            <li>Choose "Service principal (automatic)"</li>
                        </ol>
                        
                        <div class="code-block">
Service connection configuration:
  Connection name: Azure-Production
  Scope level: Subscription
  Subscription: (Your Azure subscription)
  Resource group: (Leave empty for subscription scope)
  
Permissions:
  Grant access to all pipelines: Yes
                        </div>
                    </div>

                    <div class="substep">
                        <div class="substep-number">2</div>
                        <div class="substep-title">Add Deployment Stages</div>
                        <p>Add deployment stages to your YAML pipeline:</p>
                        
                        <div class="code-block">
# Add to azure-pipelines.yml after the Build stage:

- stage: DeployDev
  displayName: 'Deploy to Development'
  dependsOn: Build
  condition: succeeded()
  variables:
    environmentName: 'development'
    webAppName: 'enterprise-webapp-dev-$(Build.BuildId)'
  jobs:
  - deployment: DeployToDev
    displayName: 'Deploy to Dev Environment'
    environment: 'development'
    pool:
      vmImage: 'ubuntu-latest'
    strategy:
      runOnce:
        deploy:
          steps:
          - task: AzureWebApp@1
            displayName: 'Deploy to Azure Web App'
            inputs:
              azureSubscription: 'Azure-Production'
              appType: 'webAppLinux'
              appName: '$(webAppName)'
              package: '$(Pipeline.Workspace)/drop/$(Build.BuildId).zip'
              runtimeStack: 'NODE|18-lts'
              startUpCommand: 'npm start'
                        </div>
                    </div>

                    <div class="substep">
                        <div class="substep-number">3</div>
                        <div class="substep-title">Configure Environments</div>
                        <p>Set up environments with approval gates:</p>
                        <ol>
                            <li>Go to "Pipelines" > "Environments"</li>
                            <li>Create "development" environment</li>
                            <li>Create "production" environment</li>
                            <li>Add approval checks to production</li>
                        </ol>
                        
                        <div class="code-block">
Environment setup:

Development environment:
  Name: development
  Description: Development deployment target
  Security: No approvals required
  
Production environment:
  Name: production
  Description: Production deployment target
  Security: Required approvals
                        </div>
                    </div>
                </div>
            </div>

            <!-- Validation Section -->
            <div class="validation-section">
                <h4>🔍 Validation & Testing</h4>
                <ol>
                    <li><strong>Build Pipeline Working:</strong> Verify CI pipeline runs successfully on code commits</li>
                    <li><strong>Tests Executing:</strong> Confirm automated tests run and results are published</li>
                    <li><strong>Artifacts Generated:</strong> Check build artifacts are created and published</li>
                    <li><strong>Deployment Pipeline:</strong> Verify CD pipeline deploys to development automatically</li>
                    <li><strong>Approval Process:</strong> Test production deployment approval workflow</li>
                    <li><strong>Environment Configuration:</strong> Confirm different settings for dev/prod</li>
                </ol>
            </div>

            <div class="troubleshooting">
                <h4>🛠️ Common Issues & Solutions</h4>
                <ul>
                    <li><strong>Pipeline fails on first run:</strong> Check service connection permissions and Azure resource access</li>
                    <li><strong>Test results not publishing:</strong> Verify test output format is correct</li>
                    <li><strong>Deployment failures:</strong> Check App Service exists and pipeline has deployment permissions</li>
                    <li><strong>Missing artifacts:</strong> Verify artifact paths and ensure build stage completes successfully</li>
                </ul>
            </div>

            <div class="resource-links">
                <h4>📚 Additional Resources</h4>
                <ul>
                    <li><a href="https://docs.microsoft.com/en-us/azure/devops/pipelines/" target="_blank">Azure Pipelines Documentation</a></li>
                    <li><a href="https://docs.microsoft.com/en-us/azure/devops/pipelines/yaml-schema" target="_blank">YAML Pipeline Schema Reference</a></li>
                    <li><a href="https://docs.microsoft.com/en-us/azure/devops/pipelines/ecosystems/javascript" target="_blank">JavaScript Pipeline Guide</a></li>
                </ul>
            </div>
        </div>

        <div id="solution12" class="solution-card">
            <div class="solution-header">
                <div class="solution-title">Assignment 12: Azure Container Instances</div>
                <div class="solution-meta">
                    <span class="meta-tag">📦 Containers</span>
                    <span class="meta-tag">⏱️ 75 minutes</span>
                    <span class="meta-tag">📊 Intermediate</span>
                </div>
            </div>
            
            <div class="time-estimate">
                <strong>⏱️ Estimated Time:</strong> 1 hour 15 minutes<br>
                <strong>🎯 Learning Objective:</strong> Deploy and manage containerized applications with serverless containers
            </div>

            <div class="prerequisites">
                <h4>📋 Prerequisites</h4>
                <ul>
                    <li>Basic understanding of Docker and containerization concepts</li>
                    <li>Familiarity with container images and registries</li>
                    <li>Completed Assignment 2 (Virtual Networks) for networking concepts</li>
                    <li>Docker Desktop installed locally (optional, for building custom images)</li>
                </ul>
            </div>

            <!-- Step 1 -->
            <div class="step-section">
                <div class="step-header">
                    <div class="step-title">Step 1: Creating Azure Container Registry</div>
                    <div class="step-description">Set up private container registry for storing images</div>
                </div>
                <div class="step-content">
                    <div class="substep">
                        <div class="substep-number">1</div>
                        <div class="substep-title">Create Container Registry</div>
                        <p>Set up Azure Container Registry (ACR):</p>
                        <ol>
                            <li>Go to "Create a resource" > "Container Registry"</li>
                            <li>Configure the registry:</li>
                        </ol>
                        
                        <div class="code-block">
Subscription: (Your subscription)
Resource Group: Create new "Containers-RG"
Registry name: enterpriseacr[random]
Location: East US
SKU: Basic (for cost efficiency in demo)

Advanced settings:
  Admin user: Enable
  Public network access: All networks
  Data endpoint: Disabled (Basic tier)
  Encryption: Microsoft-managed keys
                        </div>
                        
                        <div class="tip">
                            <strong>💡 SKU Selection:</strong> Basic tier is suitable for learning. Standard/Premium offer additional features like geo-replication and advanced security.
                        </div>
                    </div>

                    <div class="substep">
                        <div class="substep-number">2</div>
                        <div class="substep-title">Configure Registry Access</div>
                        <p>Set up authentication and access credentials:</p>
                        
                        <div class="code-block">
Registry information:
  Login server: enterpriseacr[random].azurecr.io
  Username: enterpriseacr[random]
  Password: [Generated password]

# These credentials will be used for:
# - Pushing images to the registry
# - Pulling images from Container Instances
# - Integration with CI/CD pipelines
                        </div>
                    </div>

                    <div class="substep">
                        <div class="substep-number">3</div>
                        <div class="substep-title">Push Sample Image to Registry</div>
                        <p><strong>Option 1: Import from Docker Hub (easier for demo):</strong></p>
                        
                        <div class="code-block">
az acr import \
    --name enterpriseacr[random] \
    --source docker.io/library/nginx:alpine \
    --image nginx:v1
</div>

                        <p><strong>Option 2: Build and push custom image:</strong></p>
                        <p>Create Dockerfile:</p>
                        
                        <div class="code-block">
FROM node:18-alpine
WORKDIR /app
COPY package*.json ./
RUN npm ci --only=production
COPY . .
EXPOSE 3000
USER node
CMD ["npm", "start"]
</div>

                        <p>Build and push the image:</p>
                        
                        <div class="code-block">
docker build -t enterpriseacr[random].azurecr.io/webapp:v1 .
</div>

                        <div class="code-block">
docker login enterpriseacr[random].azurecr.io
</div>

                        <div class="code-block">
docker push enterpriseacr[random].azurecr.io/webapp:v1
</div>
                    </div>
                </div>
            </div>

            <!-- Step 2 -->
            <div class="step-section">
                <div class="step-header">
                    <div class="step-title">Step 2: Deploying Single Container Instance</div>
                    <div class="step-description">Create and configure basic container instance</div>
                </div>
                <div class="step-content">
                    <div class="substep">
                        <div class="substep-number">1</div>
                        <div class="substep-title">Create Container Instance</div>
                        <p>Deploy first container instance:</p>
                        
                        <div class="code-block">
Basics:
  Subscription: (Your subscription)
  Resource group: Containers-RG
  Container name: webapp-container
  Region: East US
  Availability zones: None
  SKU: Standard
  
Image source: Azure Container Registry
  Registry: enterpriseacr[random].azurecr.io
  Image: nginx (or webapp if you built custom)
  Image tag: v1
  
Authentication:
  Username: enterpriseacr[random]
  Password: [Registry password]
                        </div>
                    </div>

                    <div class="substep">
                        <div class="substep-number">2</div>
                        <div class="substep-title">Configure Networking</div>
                        <p>Set up network configuration:</p>
                        
                        <div class="code-block">
Networking:
  Networking type: Public
  DNS name label: webapp-aci-[random]
  Ports:
    - Protocol: TCP
    - Port: 80 (for nginx) or 3000 (for custom app)
    - Type: Public
  
OS type: Linux
Size:
  CPU cores: 1
  Memory (GB): 1.5
  GPU: None
                        </div>
                        
                        <div class="tip">
                            <strong>💡 Resource Sizing:</strong> Start with minimal resources. Container Instances can be easily recreated with different specifications.
                        </div>
                    </div>

                    <div class="substep">
                        <div class="substep-number">3</div>
                        <div class="substep-title">Test Container Access</div>
                        <p>Verify container is running and accessible:</p>
                        
                        <p><strong>Access via browser:</strong></p>
                        <div class="code-block">
http://webapp-aci-[random].eastus.azurecontainer.io
</div>

                        <p><strong>Check container status via Azure CLI:</strong></p>
                        <div class="code-block">
az container show \
    --resource-group Containers-RG \
    --name webapp-container \
    --query "ipAddress.fqdn" \
    --out tsv
</div>

                        <p><strong>Check container logs:</strong></p>
                        <div class="code-block">
az container logs \
    --resource-group Containers-RG \
    --name webapp-container
</div>
                        
                        <div class="success">
                            <strong>✅ Success Indicator:</strong> Container should be accessible via the FQDN and show the nginx welcome page or your custom application.
                        </div>
                    </div>
                </div>
            </div>

            <!-- Step 3 -->
            <div class="step-section">
                <div class="step-header">
                    <div class="step-title">Step 3: Creating Multi-Container Group</div>
                    <div class="step-description">Deploy container group with shared resources</div>
                </div>
                <div class="step-content">
                    <div class="substep">
                        <div class="substep-number">1</div>
                        <div class="substep-title">Create Container Group</div>
                        <p>Create multi-container group using Azure CLI:</p>
                        
                        <div class="code-block">
az container create \
    --resource-group Containers-RG \
    --name multi-container-group \
    --image enterpriseacr[random].azurecr.io/webapp:v1 \
    --registry-login-server enterpriseacr[random].azurecr.io \
    --registry-username enterpriseacr[random] \
    --registry-password "[your-registry-password]" \
    --dns-name-label multi-webapp-[random] \
    --ports 3000 \
    --cpu 1 \
    --memory 1.5 \
    --environment-variables 'NODE_ENV'='production' 'PORT'='3000'
</div>
                    </div>

                    <div class="substep">
                        <div class="substep-number">2</div>
                        <div class="substep-title">Add Persistent Storage</div>
                        <p>Create storage account for file shares:</p>
                        
                        <div class="code-block">
az storage account create \
    --name containerstorage[random] \
    --resource-group Containers-RG \
    --location eastus \
    --sku Standard_LRS
</div>

                        <p>Create file share:</p>
                        
                        <div class="code-block">
az storage share create \
    --name container-data \
    --account-name containerstorage[random]
</div>

                        <p>Get storage key:</p>
                        
                        <div class="code-block">
STORAGE_KEY=$(az storage account keys list \
    --resource-group Containers-RG \
    --account-name containerstorage[random] \
    --query "[0].value" \
    --output tsv)
</div>

                        <p>Create container with mounted storage:</p>
                        
                        <div class="code-block">
az container create \
    --resource-group Containers-RG \
    --name webapp-with-storage \
    --image enterpriseacr[random].azurecr.io/webapp:v1 \
    --azure-file-volume-account-name containerstorage[random] \
    --azure-file-volume-account-key $STORAGE_KEY \
    --azure-file-volume-share-name container-data \
    --azure-file-volume-mount-path /app/data \
    --dns-name-label webapp-storage-[random] \
    --ports 3000
</div>
                    </div>

                    <div class="substep">
                        <div class="substep-number">3</div>
                        <div class="substep-title">Monitor Container Performance</div>
                        <p>View container metrics:</p>
                        
                        <div class="code-block">
az container show \
    --resource-group Containers-RG \
    --name webapp-container \
    --query "containers[0].instanceView.currentState" \
    --out table
</div>

                        <p>Stream container logs:</p>
                        
                        <div class="code-block">
az container logs \
    --resource-group Containers-RG \
    --name webapp-container \
    --follow
</div>

                        <p>Execute commands in running container:</p>
                        
                        <div class="code-block">
az container exec \
    --resource-group Containers-RG \
    --name webapp-container \
    --exec-command "/bin/sh"
</div>

                        <p>Restart container if needed:</p>
                        
                        <div class="code-block">
az container restart \
    --resource-group Containers-RG \
    --name webapp-container
</div>
                        
                        <div class="warning">
                            <strong>⚠️ Container Lifecycle:</strong> Container Instances are ephemeral. Use persistent storage for data that needs to survive container restarts.
                        </div>
                    </div>
                </div>
            </div>

            <!-- Validation Section -->
            <div class="validation-section">
                <h4>🔍 Validation & Testing</h4>
                <ol>
                    <li><strong>Container Registry Created:</strong> Verify ACR is operational and images can be pushed/pulled</li>
                    <li><strong>Single Container Running:</strong> Confirm container instance is accessible via public IP/FQDN</li>
                    <li><strong>Multi-Container Group:</strong> Verify container group with shared resources is working</li>
                    <li><strong>Persistent Storage:</strong> Verify file shares are mounted and data persists</li>
                    <li><strong>Logging Functional:</strong> Check that container logs are accessible and streaming</li>
                    <li><strong>Resource Management:</strong> Confirm CPU and memory limits are enforced</li>
                </ol>
            </div>

            <div class="troubleshooting">
                <h4>🛠️ Common Issues & Solutions</h4>
                <ul>
                    <li><strong>Container fails to start:</strong> Check image availability, registry credentials, and resource limits</li>
                    <li><strong>Cannot pull from ACR:</strong> Verify admin user is enabled and credentials are correct</li>
                    <li><strong>Network connectivity issues:</strong> Check security groups, DNS configuration, and port settings</li>
                    <li><strong>High memory usage:</strong> Monitor container performance and adjust memory limits</li>
                    <li><strong>Logs not appearing:</strong> Verify container output and logging configuration</li>
                </ul>
            </div>

            <div class="resource-links">
                <h4>📚 Additional Resources</h4>
                <ul>
                    <li><a href="https://docs.microsoft.com/en-us/azure/container-instances/" target="_blank">Azure Container Instances Documentation</a></li>
                    <li><a href="https://docs.microsoft.com/en-us/azure/container-registry/" target="_blank">Azure Container Registry Guide</a></li>
                    <li><a href="https://docs.microsoft.com/en-us/azure/container-instances/container-instances-multi-container-group" target="_blank">Multi-Container Groups</a></li>
                    <li><a href="https://docs.microsoft.com/en-us/azure/container-instances/container-instances-monitor" target="_blank">Container Monitoring Guide</a></li>
                </ul>
            </div>
        </div>

        <div id="solution13" class="solution-card">
            <div class="solution-header">
                <div class="solution-title">Assignment 13: Azure Functions Serverless</div>
                <div class="solution-meta">
                    <span class="meta-tag">⚡ Serverless</span>
                    <span class="meta-tag">⏱️ 85 minutes</span>
                    <span class="meta-tag">📊 Intermediate</span>
                </div>
            </div>
            
            <div class="time-estimate">
                <strong>⏱️ Estimated Time:</strong> 1 hour 25 minutes<br>
                <strong>🎯 Learning Objective:</strong> Build event-driven serverless applications with multiple trigger types and integrations
            </div>

            <div class="prerequisites">
                <h4>📋 Prerequisites</h4>
                <ul>
                    <li>Basic understanding of serverless computing concepts</li>
                    <li>Familiarity with HTTP APIs and event-driven architecture</li>
                    <li>Completed Assignment 3 (Storage Account) for blob storage integration</li>
                    <li>Node.js or Python development knowledge</li>
                    <li>Azure CLI or Azure Functions Core Tools installed locally (optional)</li>
                </ul>
            </div>

            <!-- Step 1 -->
            <div class="step-section">
                <div class="step-header">
                    <div class="step-title">Step 1: Creating Function App and HTTP Triggered Function</div>
                    <div class="step-description">Set up serverless function hosting and create first HTTP endpoint</div>
                </div>
                <div class="step-content">
                    <div class="substep">
                        <div class="substep-number">1</div>
                        <div class="substep-title">Create Function App</div>
                        <p>Set up Azure Function App with consumption plan:</p>
                        <ol>
                            <li>Go to "Create a resource" > "Function App"</li>
                            <li>Configure the function app:</li>
                        </ol>
                        
                        <div class="code-block">
Subscription: (Your subscription)
Resource Group: Create new "Serverless-RG"
Function App name: enterprise-functions-[random]
Publish: Code
Runtime stack: Node.js
Version: 18 LTS
Region: East US

Hosting:
  Operating System: Linux
  Plan type: Consumption (Serverless)
  
Storage:
  Storage account: Create new
  Account name: funcstorageacct[random]
  Performance: Standard
  Replication: LRS
  
Monitoring:
  Enable Application Insights: Yes
  Application Insights: Create new
  Name: enterprise-functions-insights
                        </div>
                        
                        <p>Click "Review + Create" and then "Create".</p>
                        
                        <div class="tip">
                            <strong>💡 Consumption Plan:</strong> Pay only for execution time and memory used. Perfect for event-driven workloads with variable traffic.
                        </div>
                    </div>

                    <div class="substep">
                        <div class="substep-number">2</div>
                        <div class="substep-title">Create HTTP Triggered Function</div>
                        <p>Build your first serverless API endpoint:</p>
                        <ol>
                            <li>Go to your Function App</li>
                            <li>Click "Functions" in the left menu</li>
                            <li>Click "+ Create"</li>
                            <li>Select "HTTP trigger" template</li>
                        </ol>
                        
                        <div class="code-block">
Function configuration:
  Template: HTTP trigger
  New Function: UserDataAPI
  Authorization level: Function
  
Template details:
  Name: UserDataAPI
  Authorization level: Function (requires function key)
                        </div>
                    </div>

                    <div class="substep">
                        <div class="substep-number">3</div>
                        <div class="substep-title">Implement Business Logic</div>
                        <p>Add comprehensive business logic to the HTTP function:</p>
                        
                        <div class="code-block">
// index.js - Replace the default function code
const { app } = require('@azure/functions');

// In-memory user database (for demo purposes)
let users = [
    { id: 1, name: 'John Doe', email: 'john@example.com', role: 'admin', created: '2024-01-15' },
    { id: 2, name: 'Jane Smith', email: 'jane@example.com', role: 'user', created: '2024-02-20' },
    { id: 3, name: 'Bob Wilson', email: 'bob@example.com', role: 'user', created: '2024-03-10' }
];

app.http('UserDataAPI', {
    methods: ['GET', 'POST', 'PUT', 'DELETE'],
    authLevel: 'function',
    handler: async (request, context) => {
        context.log(`HTTP function processed request for url "${request.url}"`);
        
        const method = request.method;
        const url = new URL(request.url);
        const userId = url.searchParams.get('id');
        
        try {
            switch (method) {
                case 'GET':
                    if (userId) {
                        // Get specific user
                        const user = users.find(u => u.id == userId);
                        if (!user) {
                            return {
                                status: 404,
                                jsonBody: { error: 'User not found', userId: userId }
                            };
                        }
                        return {
                            status: 200,
                            jsonBody: { user: user, timestamp: new Date().toISOString() }
                        };
                    } else {
                        // Get all users with pagination
                        const page = parseInt(url.searchParams.get('page')) || 1;
                        const limit = parseInt(url.searchParams.get('limit')) || 10;
                        const startIndex = (page - 1) * limit;
                        const endIndex = startIndex + limit;
                        
                        return {
                            status: 200,
                            jsonBody: {
                                users: users.slice(startIndex, endIndex),
                                pagination: {
                                    page: page,
                                    limit: limit,
                                    total: users.length,
                                    totalPages: Math.ceil(users.length / limit)
                                },
                                timestamp: new Date().toISOString()
                            }
                        };
                    }
                    
                case 'POST':
                    // Create new user
                    const newUserData = await request.json();
                    
                    if (!newUserData.name || !newUserData.email) {
                        return {
                            status: 400,
                            jsonBody: { error: 'Name and email are required' }
                        };
                    }
                    
                    const newUser = {
                        id: Math.max(...users.map(u => u.id)) + 1,
                        name: newUserData.name,
                        email: newUserData.email,
                        role: newUserData.role || 'user',
                        created: new Date().toISOString().split('T')[0]
                    };
                    
                    users.push(newUser);
                    
                    return {
                        status: 201,
                        jsonBody: { 
                            message: 'User created successfully', 
                            user: newUser,
                            timestamp: new Date().toISOString()
                        }
                    };
                    
                case 'PUT':
                    // Update user
                    if (!userId) {
                        return {
                            status: 400,
                            jsonBody: { error: 'User ID is required for updates' }
                        };
                    }
                    
                    const updateData = await request.json();
                    const userIndex = users.findIndex(u => u.id == userId);
                    
                    if (userIndex === -1) {
                        return {
                            status: 404,
                            jsonBody: { error: 'User not found', userId: userId }
                        };
                    }
                    
                    users[userIndex] = { ...users[userIndex], ...updateData };
                    
                    return {
                        status: 200,
                        jsonBody: { 
                            message: 'User updated successfully', 
                            user: users[userIndex],
                            timestamp: new Date().toISOString()
                        }
                    };
                    
                case 'DELETE':
                    // Delete user
                    if (!userId) {
                        return {
                            status: 400,
                            jsonBody: { error: 'User ID is required for deletion' }
                        };
                    }
                    
                    const deleteIndex = users.findIndex(u => u.id == userId);
                    
                    if (deleteIndex === -1) {
                        return {
                            status: 404,
                            jsonBody: { error: 'User not found', userId: userId }
                        };
                    }
                    
                    const deletedUser = users.splice(deleteIndex, 1)[0];
                    
                    return {
                        status: 200,
                        jsonBody: { 
                            message: 'User deleted successfully', 
                            deletedUser: deletedUser,
                            timestamp: new Date().toISOString()
                        }
                    };
                    
                default:
                    return {
                        status: 405,
                        jsonBody: { error: 'Method not allowed' }
                    };
            }
        } catch (error) {
            context.log.error('Error processing request:', error);
            return {
                status: 500,
                jsonBody: { 
                    error: 'Internal server error',
                    message: error.message,
                    timestamp: new Date().toISOString()
                }
            };
        }
    }
});
                        </div>
                        
                        <p>Save and test the function using the "Test/Run" feature.</p>
                    </div>
                </div>
            </div>

            <!-- Step 2 -->
            <div class="step-section">
                <div class="step-header">
                    <div class="step-title">Step 2: Creating Timer and Blob Triggered Functions</div>
                    <div class="step-description">Implement scheduled tasks and file processing automation</div>
                </div>
                <div class="step-content">
                    <div class="substep">
                        <div class="substep-number">1</div>
                        <div class="substep-title">Create Timer Triggered Function</div>
                        <p>Set up a scheduled function for maintenance tasks:</p>
                        <ol>
                            <li>In your Function App, click "+ Create"</li>
                            <li>Select "Timer trigger" template</li>
                            <li>Configure the timer function:</li>
                        </ol>
                        
                        <div class="code-block">
Timer function configuration:
  Template: Timer trigger
  New Function: DailyMaintenanceJob
  Schedule: 0 0 2 * * * (Daily at 2:00 AM UTC)
  
Cron expression explanation:
  0 0 2 * * * 
  | | | | | |
  | | | | | +-- Day of week (0-6, Sunday=0)
  | | | | +---- Month (1-12)
  | | | +------ Day of month (1-31)
  | | +-------- Hour (0-23)
  | +---------- Minute (0-59)
  +------------ Second (0-59)
                        </div>
                    </div>

                    <div class="substep">
                        <div class="substep-number">2</div>
                        <div class="substep-title">Implement Timer Function Logic</div>
                        <p>Add maintenance and cleanup logic:</p>
                        
                        <div class="code-block">
// DailyMaintenanceJob/index.js
const { app } = require('@azure/functions');

app.timer('DailyMaintenanceJob', {
    schedule: '0 0 2 * * *', // Daily at 2:00 AM UTC
    handler: async (myTimer, context) => {
        const timestamp = new Date().toISOString();
        context.log('Daily maintenance job triggered at:', timestamp);
        
        try {
            // Simulate maintenance tasks
            const maintenanceTasks = [
                'Cleaning temporary files',
                'Updating user statistics',
                'Archiving old logs',
                'Refreshing cached data',
                'Sending daily reports'
            ];
            
            const results = [];
            
            for (const task of maintenanceTasks) {
                context.log(`Executing: ${task}`);
                
                // Simulate task execution time
                await new Promise(resolve => setTimeout(resolve, Math.random() * 2000));
                
                const success = Math.random() > 0.1; // 90% success rate
                
                results.push({
                    task: task,
                    status: success ? 'completed' : 'failed',
                    timestamp: new Date().toISOString(),
                    duration: Math.floor(Math.random() * 5000) + 500 // 500-5500ms
                });
                
                context.log(`${task}: ${success ? 'COMPLETED' : 'FAILED'}`);
            }
            
            // Log summary
            const completedTasks = results.filter(r => r.status === 'completed').length;
            const failedTasks = results.filter(r => r.status === 'failed').length;
            
            context.log('Maintenance Summary:', {
                totalTasks: results.length,
                completed: completedTasks,
                failed: failedTasks,
                successRate: `${Math.round((completedTasks / results.length) * 100)}%`,
                executionTime: timestamp
            });
            
            // Simulate sending notification (in real scenario, integrate with Logic Apps or Service Bus)
            if (failedTasks > 0) {
                context.log('WARNING: Some maintenance tasks failed. Admin notification required.');
            }
            
        } catch (error) {
            context.log.error('Maintenance job failed:', error);
            throw error;
        }
    }
});
                        </div>
                    </div>

                    <div class="substep">
                        <div class="substep-number">3</div>
                        <div class="substep-title">Create Blob Triggered Function</div>
                        <p>Set up automatic file processing when files are uploaded:</p>
                        <ol>
                            <li>Create "+ Create" > "Blob trigger" template</li>
                            <li>Configure blob trigger settings:</li>
                        </ol>
                        
                        <div class="code-block">
Blob function configuration:
  Template: Blob trigger
  New Function: ProcessUploadedFiles
  Path: uploads/{name}
  Storage account connection: Use existing storage account
  
Storage connection:
  - Use the storage account created with Function App
  - Connection string will be automatically configured
  - Container 'uploads' will be created if it doesn't exist
                        </div>
                        
                        <div class="code-block">
// ProcessUploadedFiles/index.js
const { app } = require('@azure/functions');

app.storageBlob('ProcessUploadedFiles', {
    path: 'uploads/{name}',
    connection: 'AzureWebJobsStorage',
    handler: async (blob, context) => {
        const blobName = context.bindingData.name;
        const blobSize = blob.length;
        
        context.log(`Processing uploaded file: ${blobName} (${blobSize} bytes)`);
        
        try {
            // File processing logic
            const fileExtension = blobName.split('.').pop().toLowerCase();
            const fileInfo = {
                fileName: blobName,
                fileSize: blobSize,
                fileType: fileExtension,
                uploadTime: new Date().toISOString(),
                processedTime: new Date().toISOString()
            };
            
            // Process different file types
            switch (fileExtension) {
                case 'txt':
                case 'log':
                    // Text file processing
                    const textContent = blob.toString('utf8');
                    const wordCount = textContent.split(/\s+/).length;
                    const lineCount = textContent.split('\n').length;
                    
                    fileInfo.analysis = {
                        type: 'text',
                        wordCount: wordCount,
                        lineCount: lineCount,
                        characterCount: textContent.length
                    };
                    
                    context.log(`Text analysis: ${wordCount} words, ${lineCount} lines`);
                    break;
                    
                case 'json':
                    // JSON file processing
                    try {
                        const jsonContent = JSON.parse(blob.toString('utf8'));
                        fileInfo.analysis = {
                            type: 'json',
                            isValid: true,
                            objectCount: Array.isArray(jsonContent) ? jsonContent.length : 1,
                            keys: typeof jsonContent === 'object' ? Object.keys(jsonContent) : []
                        };
                        context.log('Valid JSON file processed');
                    } catch (jsonError) {
                        fileInfo.analysis = {
                            type: 'json',
                            isValid: false,
                            error: jsonError.message
                        };
                        context.log('Invalid JSON file detected');
                    }
                    break;
                    
                case 'csv':
                    // CSV file processing
                    const csvContent = blob.toString('utf8');
                    const csvLines = csvContent.split('\n').filter(line => line.trim());
                    const headers = csvLines.length > 0 ? csvLines[0].split(',') : [];
                    
                    fileInfo.analysis = {
                        type: 'csv',
                        rowCount: csvLines.length - 1, // Excluding header
                        columnCount: headers.length,
                        headers: headers
                    };
                    
                    context.log(`CSV processed: ${csvLines.length - 1} rows, ${headers.length} columns`);
                    break;
                    
                case 'jpg':
                case 'jpeg':
                case 'png':
                case 'gif':
                    // Image file processing
                    fileInfo.analysis = {
                        type: 'image',
                        format: fileExtension,
                        sizeKB: Math.round(blobSize / 1024)
                    };
                    
                    context.log(`Image file processed: ${fileExtension.toUpperCase()} format`);
                    break;
                    
                default:
                    // Generic file processing
                    fileInfo.analysis = {
                        type: 'unknown',
                        extension: fileExtension
                    };
                    
                    context.log(`Unknown file type processed: ${fileExtension}`);
                    break;
            }
            
            // Security scan simulation
            const isSafe = Math.random() > 0.05; // 95% safe files
            fileInfo.securityScan = {
                scanned: true,
                isSafe: isSafe,
                scanTime: new Date().toISOString(),
                threats: isSafe ? [] : ['Suspicious pattern detected']
            };
            
            if (!isSafe) {
                context.log('WARNING: Security threat detected in file!');
                // In real scenario: quarantine file, send alert, etc.
            }
            
            // Log processing results
            context.log('File processing completed:', JSON.stringify(fileInfo, null, 2));
            
            // In real scenario: 
            // - Store file info in database
            // - Send notifications
            // - Move processed files to different containers
            // - Generate thumbnails for images
            // - Extract metadata
            
        } catch (error) {
            context.log.error(`Error processing file ${blobName}:`, error);
            throw error;
        }
    }
});
                        </div>
                    </div>
                </div>
            </div>

            <!-- Step 3 -->
            <div class="step-section">
                <div class="step-header">
                    <div class="step-title">Step 3: Function Configuration and Monitoring</div>
                    <div class="step-description">Configure environment variables, security, and monitoring</div>
                </div>
                <div class="step-content">
                    <div class="substep">
                        <div class="substep-number">1</div>
                        <div class="substep-title">Configure Application Settings</div>
                        <p>Set up environment variables and connection strings:</p>
                        <ol>
                            <li>Go to Function App > "Configuration"</li>
                            <li>Add application settings:</li>
                        </ol>
                        
                        <div class="code-block">
Application settings to add:

API_VERSION=1.0.0
MAX_USERS_PER_PAGE=50
MAINTENANCE_EMAIL=admin@company.com
FILE_SIZE_LIMIT_MB=10
ALLOWED_FILE_TYPES=txt,json,csv,jpg,jpeg,png,gif
SECURITY_SCAN_ENABLED=true
LOG_LEVEL=INFO
TIMEZONE=UTC

Connection strings (if needed):
DATABASE_CONNECTION=(SQL connection string)
SERVICE_BUS_CONNECTION=(Service Bus connection string)
COSMOS_DB_CONNECTION=(Cosmos DB connection string)
                        </div>
                        
                        <p>Click "Save" to apply settings.</p>
                    </div>

                    <div class="substep">
                        <div class="substep-number">2</div>
                        <div class="substep-title">Configure Function Security</div>
                        <p>Set up authentication and access keys:</p>
                        
                        <div class="code-block">
Function-level security:

1. Get function URLs and keys:
   - Go to Functions > UserDataAPI > "Function Keys"
   - Copy the "default" function key
   - Function URL format: 
     https://enterprise-functions-[random].azurewebsites.net/api/UserDataAPI?code=[function-key]

2. Host-level keys (for admin access):
   - Go to "App keys" in Function App
   - Note the master key (admin access)
   - Create custom host keys for different clients

3. Test function security:
   # Without key (should fail):
   curl https://enterprise-functions-[random].azurewebsites.net/api/UserDataAPI
   
   # With function key (should work):
   curl "https://enterprise-functions-[random].azurewebsites.net/api/UserDataAPI?code=[function-key]"
   
   # Test CRUD operations:
   
   # GET all users:
   curl "https://enterprise-functions-[random].azurewebsites.net/api/UserDataAPI?code=[function-key]"
   
   # GET specific user:
   curl "https://enterprise-functions-[random].azurewebsites.net/api/UserDataAPI?code=[function-key]&id=1"
   
   # POST new user:
   curl -X POST "https://enterprise-functions-[random].azurewebsites.net/api/UserDataAPI?code=[function-key]" \
        -H "Content-Type: application/json" \
        -d '{"name": "Alice Johnson", "email": "alice@example.com", "role": "manager"}'
   
   # PUT update user:
   curl -X PUT "https://enterprise-functions-[random].azurewebsites.net/api/UserDataAPI?code=[function-key]&id=1" \
        -H "Content-Type: application/json" \
        -d '{"role": "senior-admin"}'
   
   # DELETE user:
   curl -X DELETE "https://enterprise-functions-[random].azurewebsites.net/api/UserDataAPI?code=[function-key]&id=3"
                        </div>
                    </div>

                    <div class="substep">
                        <div class="substep-number">3</div>
                        <div class="substep-title">Monitor Function Performance</div>
                        <p>Set up monitoring and troubleshooting:</p>
                        
                        <p><strong>1. Application Insights Integration:</strong></p>
                        <ul>
                            <li>Go to Function App → "Application Insights"</li>
                            <li>Verify integration is enabled</li>
                            <li>Access detailed metrics and logs</li>
                        </ul>
                        
                        <p><strong>2. Key Metrics to Monitor:</strong></p>
                        <ul>
                            <li>Execution count and duration</li>
                            <li>Success/failure rates</li>
                            <li>Memory and CPU usage</li>
                            <li>Cold start frequency</li>
                            <li>Error rates and exceptions</li>
                        </ul>
                        
                        <p><strong>3. View Function Logs:</strong></p>
                        <ul>
                            <li>Go to Functions → [Function Name] → "Monitor"</li>
                            <li>Review execution history</li>
                            <li>Check success/failure status</li>
                            <li>Analyze performance trends</li>
                        </ul>
                        
                        <p><strong>4. Live Log Streaming:</strong></p>
                        <ul>
                            <li>Go to Function App → "Log stream"</li>
                            <li>View real-time logs during function execution</li>
                            <li>Monitor for errors and performance issues</li>
                        </ul>
                        
                        <p><strong>5. Test Blob Trigger Function:</strong></p>
                        <p>Upload test files to trigger the blob function:</p>
                        
                        <div class="code-block">
# Create test container
az storage container create \
    --name uploads \
    --account-name funcstorageacct[random]
</div>
                        
                        <div class="code-block">
# Create and upload text file
echo "Hello World from Azure Functions!" > test.txt

az storage blob upload \
    --file test.txt \
    --container uploads \
    --name test.txt \
    --account-name funcstorageacct[random]
</div>
                        
                        <div class="code-block">
# Create and upload JSON file
echo '{"message": "Test JSON file", "timestamp": "2024-01-01T00:00:00Z"}' > test.json

az storage blob upload \
    --file test.json \
    --container uploads \
    --name test.json \
    --account-name funcstorageacct[random]
</div>
                        
                        <div class="success">
                            <strong>✅ Monitoring Success:</strong> Functions should execute automatically and logs should show detailed processing information.
                        </div>
                    </div>
                </div>
            </div>

            <!-- Validation Section -->
            <div class="validation-section">
                <h4>🔍 Validation & Testing</h4>
                <ol>
                    <li><strong>Function App Created:</strong> Verify Function App is running with consumption plan</li>
                    <li><strong>HTTP Function Working:</strong> Test all CRUD operations via HTTP requests</li>
                    <li><strong>Timer Function Scheduled:</strong> Verify timer function is scheduled and executes correctly</li>
                    <li><strong>Blob Trigger Active:</strong> Confirm blob function processes uploaded files automatically</li>
                    <li><strong>Security Configured:</strong> Verify function keys are required for access</li>
                    <li><strong>Application Settings:</strong> Confirm environment variables are configured</li>
                    <li><strong>Monitoring Active:</strong> Check Application Insights shows execution data</li>
                    <li><strong>Error Handling:</strong> Test error scenarios and exception handling</li>
                </ol>
            </div>

            <div class="troubleshooting">
                <h4>🛠️ Common Issues & Solutions</h4>
                <ul>
                    <li><strong>Function not triggering:</strong> Check trigger configuration and connection strings</li>
                    <li><strong>401 Unauthorized errors:</strong> Verify function keys are included in requests</li>
                    <li><strong>Cold start delays:</strong> Expected behavior in consumption plan; consider premium plan for production</li>
                    <li><strong>Blob trigger not firing:</strong> Check storage account connection and container existence</li>
                    <li><strong>High execution costs:</strong> Monitor function execution frequency and optimize trigger conditions</li>
                    <li><strong>Timeout errors:</strong> Increase function timeout settings or optimize code performance</li>
                </ul>
            </div>

            <div class="resource-links">
                <h4>📚 Additional Resources</h4>
                <ul>
                    <li><a href="https://docs.microsoft.com/en-us/azure/azure-functions/" target="_blank">Azure Functions Documentation</a></li>
                    <li><a href="https://docs.microsoft.com/en-us/azure/azure-functions/functions-triggers-bindings" target="_blank">Triggers and Bindings Reference</a></li>
                    <li><a href="https://docs.microsoft.com/en-us/azure/azure-functions/functions-monitoring" target="_blank">Monitoring Azure Functions</a></li>
                    <li><a href="https://docs.microsoft.com/en-us/azure/azure-functions/functions-best-practices" target="_blank">Best Practices Guide</a></li>
                </ul>
            </div>
        </div>

        <div id="solution14" class="solution-card">
            <div class="solution-header">
                <div class="solution-title">Assignment 14: Azure Logic Apps Workflow</div>
                <div class="solution-meta">
                    <span class="meta-tag">🔄 Workflow</span>
                    <span class="meta-tag">⏱️ 90 minutes</span>
                    <span class="meta-tag">📊 Intermediate</span>
                </div>
            </div>
            
            <div class="time-estimate">
                <strong>⏱️ Estimated Time:</strong> 1 hour 30 minutes<br>
                <strong>🎯 Learning Objective:</strong> Build automated business workflows with visual designer and integrate multiple services
            </div>

            <div class="prerequisites">
                <h4>📋 Prerequisites</h4>
                <ul>
                    <li>Understanding of business process automation concepts</li>
                    <li>Familiarity with REST APIs and HTTP operations</li>
                    <li>Completed Assignment 3 (Storage Account) for file operations</li>
                    <li>Email account for testing notifications (Outlook, Gmail, etc.)</li>
                    <li>Basic knowledge of JSON and conditional logic</li>
                </ul>
            </div>

            <!-- Step 1 -->
            <div class="step-section">
                <div class="step-header">
                    <div class="step-title">Step 1: Creating Logic App and Email Notification Workflow</div>
                    <div class="step-description">Build automated email workflow triggered by HTTP requests</div>
                </div>
                <div class="step-content">
                    <div class="substep">
                        <div class="substep-number">1</div>
                        <div class="substep-title">Create Logic App</div>
                        <p>Set up Logic App with consumption plan:</p>
                        
                        <div class="code-block">
Subscription: (Your subscription)
Resource Group: Create new "Workflows-RG"
Logic App name: enterprise-workflow-[random]
Publish: Workflow
Region: East US

Plan type: Consumption
  Pay-per-execution model
  Built-in connectors and triggers
  Automatic scaling
  
Zone redundancy: Disabled (for cost savings)
Enable log analytics: Yes
  Workspace: Create new "workflow-logs"
                        </div>
                        
                        <div class="tip">
                            <strong>💡 Consumption Plan:</strong> Perfect for intermittent workflows. Pay only for trigger fires and action executions.
                        </div>
                    </div>

                    <div class="substep">
                        <div class="substep-number">2</div>
                        <div class="substep-title">Design Email Notification Workflow</div>
                        <p>Create workflow using the visual designer:</p>
                        
                        <div class="code-block">
Workflow design:

1. Trigger: "When a HTTP request is received"
   - Method: POST
   - JSON Schema: (define request structure)
   
2. Action: "Parse JSON" 
   - Content: Request body
   - Schema: Extract notification details
   
3. Condition: "Check notification priority"
   - High priority: Immediate email
   - Normal priority: Daily digest
   
4. Action: "Send an email (V2)"
   - Email connector: Office 365 Outlook or Gmail
   - Dynamic content from parsed JSON
                        </div>
                    </div>

                    <div class="substep">
                        <div class="substep-number">3</div>
                        <div class="substep-title">Configure HTTP Trigger</div>
                        <p>Set up the HTTP trigger with JSON schema:</p>
                        
                        <div class="code-block">
HTTP Trigger JSON Schema:

{
  "type": "object",
  "properties": {
    "notificationType": {
      "type": "string"
    },
    "priority": {
      "type": "string"
    },
    "recipient": {
      "type": "object",
      "properties": {
        "email": {
          "type": "string"
        },
        "name": {
          "type": "string"
        }
      }
    },
    "subject": {
      "type": "string"
    },
    "message": {
      "type": "string"
    },
    "data": {
      "type": "object",
      "properties": {
        "orderId": {
          "type": "string"
        },
        "amount": {
          "type": "number"
        },
        "timestamp": {
          "type": "string"
        }
      }
    }
  }
}
                        </div>
                    </div>
                </div>
            </div>

            <!-- Step 2 -->
            <div class="step-section">
                <div class="step-header">
                    <div class="step-title">Step 2: Building Complex Workflow Logic</div>
                    <div class="step-description">Add conditional logic, loops, and multiple integrations</div>
                </div>
                <div class="step-content">
                    <div class="substep">
                        <div class="substep-number">1</div>
                        <div class="substep-title">Add Conditional Logic</div>
                        <p>Create conditional branches based on priority:</p>
                        
                        <div class="code-block">
Condition Configuration:

1. Add "Condition" action
2. Left operand: priority (from Parse JSON)
3. Operator: "is equal to"
4. Right operand: "high"

True branch (High Priority):
  - Send immediate email notification
  - Log to storage table
  - Create support ticket

False branch (Normal Priority):
  - Add to daily digest queue
  - Log to file storage
  - Update dashboard metrics
                        </div>
                    </div>

                    <div class="substep">
                        <div class="substep-number">2</div>
                        <div class="substep-title">Configure Email Actions</div>
                        <p>Set up email sending with dynamic content:</p>
                        
                        <div class="code-block">
Email Configuration:

1. Add "Send an email (V2)" action
2. Choose connector: Office 365 Outlook
3. Configure email parameters:
   To: recipient.email (dynamic content)
   Subject: "[URGENT] " + subject (dynamic content)
   
   Body (HTML):
   <h2>High Priority Notification</h2>
   <p>Dear @{body('Parse_JSON')?['recipient']?['name']},</p>
   <p>@{body('Parse_JSON')?['message']}</p>
   
   <div>
     <h3>Details:</h3>
     <ul>
       <li>Type: @{body('Parse_JSON')?['notificationType']}</li>
       <li>Priority: @{body('Parse_JSON')?['priority']}</li>
       <li>Timestamp: @{body('Parse_JSON')?['data']?['timestamp']}</li>
     </ul>
   </div>
                        </div>
                    </div>

                    <div class="substep">
                        <div class="substep-number">3</div>
                        <div class="substep-title">Add Storage Integration</div>
                        <p>Store notification logs in Azure Storage:</p>
                        
                        <div class="code-block">
Storage Integration:

1. Add "Insert or Replace Entity" action (Azure Table Storage)
2. Connection: Create new connection to storage account
3. Table name: NotificationLogs

Entity configuration:
  PartitionKey: @{formatDateTime(utcnow(), 'yyyy-MM-dd')}
  RowKey: @{guid()}
  NotificationType: @{body('Parse_JSON')?['notificationType']}
  Priority: @{body('Parse_JSON')?['priority']}
  Recipient: @{body('Parse_JSON')?['recipient']?['email']}
  Subject: @{body('Parse_JSON')?['subject']}
  Timestamp: @{utcnow()}
  Status: "Processed"
                        </div>
                    </div>
                </div>
            </div>

            <!-- Step 3 -->
            <div class="step-section">
                <div class="step-header">
                    <div class="step-title">Step 3: Testing and Error Handling</div>
                    <div class="step-description">Test workflow and implement error handling</div>
                </div>
                <div class="step-content">
                    <div class="substep">
                        <div class="substep-number">1</div>
                        <div class="substep-title">Test the Workflow</div>
                        <p>Test with sample payloads:</p>
                        
                        <div class="code-block">
Test Payload (High Priority):
{
  "notificationType": "order_confirmation",
  "priority": "high",
  "recipient": {
    "email": "customer@example.com",
    "name": "John Doe"
  },
  "subject": "Your Order Has Been Confirmed",
  "message": "Thank you for your order. Items will ship within 24 hours.",
  "data": {
    "orderId": "ORD-2024-001234",
    "amount": 299.99,
    "timestamp": "2024-01-15T10:30:00Z"
  }
}

# Test using curl:
curl -X POST "[Your Logic App URL]" \
  -H "Content-Type: application/json" \
  -d @test-payload.json
                        </div>
                    </div>

                    <div class="substep">
                        <div class="substep-number">2</div>
                        <div class="substep-title">Add Error Handling</div>
                        <p>Configure retry policies and error notifications:</p>
                        
                        <div class="code-block">
Error Handling:

1. For each action, configure retry policy:
   - Settings > Retry Policy
   - Type: Fixed Interval
   - Count: 3
   - Interval: PT30S (30 seconds)
   
2. Add error notification branch:
   - "Send an email" for admin notification
   - To: admin@company.com
   - Subject: "Workflow Error: @{workflow().name}"
   - Body: Include error details and original request
                        </div>
                    </div>

                    <div class="substep">
                        <div class="substep-number">3</div>
                        <div class="substep-title">Monitor Execution</div>
                        <p>Monitor workflow performance:</p>
                        
                        <div class="code-block">
Monitoring:

1. Go to Logic App > Overview > Runs history
2. Click on run to see detailed execution
3. Verify each step completed successfully
4. Check sent emails and storage logs

Key metrics to monitor:
- Execution count and duration
- Success/failure rates
- Trigger frequency
- Action performance
- Error patterns
                        </div>
                        
                        <div class="success">
                            <strong>✅ Success Indicators:</strong> Workflow completes successfully, emails are sent, and data is stored in Azure Storage.
                        </div>
                    </div>
                </div>
            </div>

            <!-- Validation Section -->
            <div class="validation-section">
                <h4>🔍 Validation & Testing</h4>
                <ol>
                    <li><strong>Logic App Created:</strong> Verify Logic App is deployed and accessible</li>
                    <li><strong>HTTP Trigger Working:</strong> Confirm HTTP endpoint accepts POST requests</li>
                    <li><strong>JSON Parsing:</strong> Verify Parse JSON action extracts data correctly</li>
                    <li><strong>Conditional Logic:</strong> Test both high and normal priority branches</li>
                    <li><strong>Email Notifications:</strong> Confirm emails are sent with proper formatting</li>
                    <li><strong>Storage Integration:</strong> Verify data is stored in Azure Storage</li>
                    <li><strong>Error Handling:</strong> Test error scenarios and retry mechanisms</li>
                    <li><strong>Run History:</strong> Check execution logs and performance metrics</li>
                </ol>
            </div>

            <div class="troubleshooting">
                <h4>🛠️ Common Issues & Solutions</h4>
                <ul>
                    <li><strong>Email connector authentication:</strong> Re-authenticate with email provider if connection fails</li>
                    <li><strong>Dynamic content errors:</strong> Check JSON schema matches actual request payload</li>
                    <li><strong>Storage connection issues:</strong> Verify storage account access keys and connection strings</li>
                    <li><strong>Workflow timeouts:</strong> Optimize actions and add proper error handling</li>
                    <li><strong>Parse JSON failures:</strong> Validate incoming JSON structure matches defined schema</li>
                </ul>
            </div>

            <div class="resource-links">
                <h4>📚 Additional Resources</h4>
                <ul>
                    <li><a href="https://docs.microsoft.com/en-us/azure/logic-apps/" target="_blank">Azure Logic Apps Documentation</a></li>
                    <li><a href="https://docs.microsoft.com/en-us/azure/logic-apps/logic-apps-workflow-definition-language" target="_blank">Workflow Definition Language</a></li>
                    <li><a href="https://docs.microsoft.com/en-us/azure/logic-apps/logic-apps-exception-handling" target="_blank">Error Handling Guide</a></li>
                </ul>
            </div>
        </div>

        <div id="solution15" class="solution-card">
            <div class="solution-header">
                <div class="solution-title">Assignment 15: Azure Event Hub Streaming</div>
                <div class="solution-meta">
                    <span class="meta-tag">📡 Messaging</span>
                    <span class="meta-tag">⏱️ 80 minutes</span>
                    <span class="meta-tag">📊 Advanced</span>
                </div>
            </div>
            
            <div class="time-estimate">
                <strong>⏱️ Estimated Time:</strong> 1 hour 20 minutes<br>
                <strong>🎯 Learning Objective:</strong> Build real-time data streaming platform with event ingestion and processing
            </div>

            <div class="prerequisites">
                <h4>📋 Prerequisites</h4>
                <ul>
                    <li>Understanding of event-driven architecture and streaming concepts</li>
                    <li>Familiarity with real-time data processing patterns</li>
                    <li>Completed Assignment 3 (Storage Account) for data capture</li>
                    <li>Basic knowledge of JSON and message queuing</li>
                    <li>Node.js or Python development knowledge for producers/consumers</li>
                </ul>
            </div>

            <!-- Step 1 -->
            <div class="step-section">
                <div class="step-header">
                    <div class="step-title">Step 1: Creating Event Hub Namespace and Hub</div>
                    <div class="step-description">Set up event streaming infrastructure with proper configuration</div>
                </div>
                <div class="step-content">
                    <div class="substep">
                        <div class="substep-number">1</div>
                        <div class="substep-title">Create Event Hub Namespace</div>
                        <p>Set up the Event Hub namespace for organizing event hubs:</p>
                        <ol>
                            <li>Go to "Create a resource" > "Event Hubs"</li>
                            <li>Configure the namespace:</li>
                        </ol>
                        
                        <div class="code-block">
Subscription: (Your subscription)
Resource Group: Create new "Streaming-RG"
Namespace name: enterprise-events-[random]
Location: East US
Pricing tier: Standard

Throughput Units:
  Min: 1
  Max: 2
  Auto-inflate: Enabled
  
Advanced settings:
  Kafka: Enabled (for Kafka protocol support)
  Zone redundant: Disabled (for cost savings)
  Minimum TLS version: 1.2
                        </div>
                        
                        <p>Click "Review + Create" and then "Create".</p>
                        
                        <div class="tip">
                            <strong>💡 Throughput Units:</strong> Each TU provides 1 MB/s ingress and 2 MB/s egress. Start small and scale based on actual usage.
                        </div>
                    </div>

                    <div class="substep">
                        <div class="substep-number">2</div>
                        <div class="substep-title">Create Event Hub</div>
                        <p>Create the actual event hub for receiving events:</p>
                        <ol>
                            <li>Go to your Event Hub namespace</li>
                            <li>Click "+ Event Hub"</li>
                            <li>Configure the event hub:</li>
                        </ol>
                        
                        <div class="code-block">
Event Hub configuration:
  Name: telemetry-events
  Partition count: 4
  Message retention: 7 days
  
Capture settings:
  Enable capture: Yes
  Time window: 5 minutes
  Size window: 314 MB
  
Capture destination:
  Storage account: Create new or use existing
  Container: eventhub-capture
  Name format: {Namespace}/{EventHub}/{PartitionId}/{Year}/{Month}/{Day}/{Hour}/{Minute}/{Second}
  
Advanced settings:
  Cleanup policy: Delete (removes old messages)
  Segment size: 1073741824 bytes (1 GB)
                        </div>
                        
                        <div class="warning">
                            <strong>⚠️ Partition Count:</strong> Partition count cannot be changed after creation. Plan for peak load and parallel processing needs.
                        </div>
                    </div>

                    <div class="substep">
                        <div class="substep-number">3</div>
                        <div class="substep-title">Configure Access Policies</div>
                        <p>Set up security and access control:</p>
                        
                        <div class="code-block">
Shared Access Policies:

1. Create "EventProducer" policy:
   - Claims: Send
   - Description: For applications sending events
   
2. Create "EventConsumer" policy:
   - Claims: Listen
   - Description: For applications consuming events
   
3. Create "EventAdmin" policy:
   - Claims: Manage, Send, Listen
   - Description: For administrative operations

Connection strings format:
Endpoint=sb://enterprise-events-[random].servicebus.windows.net/;
SharedAccessKeyName=[PolicyName];
SharedAccessKey=[Key];
EntityPath=telemetry-events
                        </div>
                    </div>
                </div>
            </div>

            <!-- Step 2 -->
            <div class="step-section">
                <div class="step-header">
                    <div class="step-title">Step 2: Building Event Producers</div>
                    <div class="step-description">Create applications that send events to Event Hub</div>
                </div>
                <div class="step-content">
                    <div class="substep">
                        <div class="substep-number">1</div>
                        <div class="substep-title">Create IoT Device Simulator</div>
                        <p>Build a producer that simulates IoT device telemetry:</p>
                        
                        <div class="code-block">
# iot-device-simulator.js
const { EventHubProducerClient } = require('@azure/event-hubs');
const crypto = require('crypto');

// Connection string for Event Producer policy
const connectionString = "Endpoint=sb://enterprise-events-[random].servicebus.windows.net/;SharedAccessKeyName=EventProducer;SharedAccessKey=[key];EntityPath=telemetry-events";

class IoTDeviceSimulator {
    constructor() {
        this.producer = new EventHubProducerClient(connectionString);
        this.deviceId = crypto.randomUUID();
        this.isRunning = false;
    }

    generateTelemetryData() {
        const baseTemp = 20;
        const tempVariation = (Math.random() - 0.5) * 10;
        const baseHumidity = 50;
        const humidityVariation = (Math.random() - 0.5) * 20;
        
        return {
            deviceId: this.deviceId,
            deviceType: 'temperature-sensor',
            location: {
                building: 'HQ-Building-A',
                floor: Math.floor(Math.random() * 10) + 1,
                room: Math.floor(Math.random() * 50) + 1
            },
            telemetry: {
                temperature: +(baseTemp + tempVariation).toFixed(2),
                humidity: +(baseHumidity + humidityVariation).toFixed(2),
                pressure: +(1013.25 + (Math.random() - 0.5) * 50).toFixed(2),
                batteryLevel: +(Math.random() * 100).toFixed(2)
            },
            timestamp: new Date().toISOString(),
            metadata: {
                firmware: '1.2.3',
                manufacturer: 'AcmeSensors',
                lastMaintenance: '2024-01-01T00:00:00Z'
            }
        };
    }

    async sendBatch() {
        try {
            // Create a batch of events
            const batch = await this.producer.createBatch();
            const batchSize = Math.floor(Math.random() * 10) + 1; // 1-10 events
            
            console.log(`Creating batch of ${batchSize} events...`);
            
            for (let i = 0; i < batchSize; i++) {
                const telemetryData = this.generateTelemetryData();
                
                // Add event to batch with partition key for consistent routing
                const eventData = {
                    body: telemetryData,
                    properties: {
                        messageType: 'telemetry',
                        deviceType: telemetryData.deviceType,
                        priority: telemetryData.telemetry.temperature > 35 ? 'high' : 'normal'
                    },
                    contentType: 'application/json'
                };
                
                // Use device ID as partition key for ordering
                const addEventResult = batch.tryAdd(eventData, { partitionKey: this.deviceId });
                
                if (!addEventResult) {
                    console.log('Batch is full, sending current batch...');
                    break;
                }
            }
            
            if (batch.count > 0) {
                await this.producer.sendBatch(batch);
                console.log(`Sent batch of ${batch.count} events from device ${this.deviceId}`);
            }
            
        } catch (error) {
            console.error('Error sending batch:', error);
        }
    }

    async startSimulation(intervalSeconds = 10) {
        console.log(`Starting IoT device simulation for device ${this.deviceId}`);
        console.log(`Sending telemetry every ${intervalSeconds} seconds...`);
        
        this.isRunning = true;
        
        while (this.isRunning) {
            await this.sendBatch();
            await new Promise(resolve => setTimeout(resolve, intervalSeconds * 1000));
        }
    }

    async stopSimulation() {
        console.log('Stopping IoT device simulation...');
        this.isRunning = false;
        await this.producer.close();
    }
}

// Usage
const simulator = new IoTDeviceSimulator();

// Handle graceful shutdown
process.on('SIGINT', async () => {
    console.log('\nReceived SIGINT, shutting down gracefully...');
    await simulator.stopSimulation();
    process.exit(0);
});

// Start simulation
simulator.startSimulation(5); // Send every 5 seconds
                        </div>
                    </div>

                    <div class="substep">
                        <div class="substep-number">2</div>
                        <div class="substep-title">Create User Activity Producer</div>
                        <p>Build a producer for user activity events:</p>
                        
                        <div class="code-block">
# user-activity-producer.js
const { EventHubProducerClient } = require('@azure/event-hubs');

class UserActivityProducer {
    constructor() {
        this.producer = new EventHubProducerClient(connectionString);
        this.userActions = [
            'login', 'logout', 'page_view', 'click', 'purchase', 
            'search', 'download', 'upload', 'share', 'comment'
        ];
        this.users = [
            'user001', 'user002', 'user003', 'user004', 'user005',
            'user006', 'user007', 'user008', 'user009', 'user010'
        ];
    }

    generateUserActivity() {
        const action = this.userActions[Math.floor(Math.random() * this.userActions.length)];
        const userId = this.users[Math.floor(Math.random() * this.users.length)];
        
        return {
            eventType: 'user_activity',
            userId: userId,
            sessionId: crypto.randomUUID(),
            action: action,
            timestamp: new Date().toISOString(),
            details: {
                userAgent: 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
                ipAddress: `192.168.1.${Math.floor(Math.random() * 255)}`,
                page: `/app/${action}`,
                referrer: action === 'login' ? null : '/app/dashboard',
                duration: Math.floor(Math.random() * 300000) // 0-5 minutes in ms
            },
            metadata: {
                source: 'web-app',
                version: '2.1.0',
                feature_flags: ['new_ui', 'enhanced_search']
            }
        };
    }

    async sendUserActivities(count = 5) {
        try {
            const batch = await this.producer.createBatch();
            
            for (let i = 0; i < count; i++) {
                const activity = this.generateUserActivity();
                
                const eventData = {
                    body: activity,
                    properties: {
                        messageType: 'user_activity',
                        action: activity.action,
                        userId: activity.userId
                    }
                };
                
                batch.tryAdd(eventData, { partitionKey: activity.userId });
            }
            
            if (batch.count > 0) {
                await this.producer.sendBatch(batch);
                console.log(`Sent ${batch.count} user activity events`);
            }
            
        } catch (error) {
            console.error('Error sending user activities:', error);
        }
    }
}

# Package.json dependencies:
{
  "dependencies": {
    "@azure/event-hubs": "^5.11.0"
  }
}

# Run producers:
npm install
node iot-device-simulator.js
# In another terminal:
node user-activity-producer.js
                        </div>
                    </div>

                    <div class="substep">
                        <div class="substep-number">3</div>
                        <div class="substep-title">Monitor Event Ingestion</div>
                        <p>Monitor events flowing into Event Hub:</p>
                        
                        <p><strong>1. Azure Portal Metrics:</strong></p>
                        <ul>
                            <li>Go to Event Hub → Metrics</li>
                            <li>Key metrics to monitor:
                                <ul>
                                    <li>Incoming Requests</li>
                                    <li>Incoming Messages</li>
                                    <li>Outgoing Messages</li>
                                    <li>Throttled Requests</li>
                                    <li>Server Errors</li>
                                    <li>User Errors</li>
                                </ul>
                            </li>
                        </ul>

                        <p><strong>2. Real-time Monitoring:</strong></p>
                        <ul>
                            <li>Go to Event Hub → Process data → Explore</li>
                            <li>View live event stream</li>
                            <li>Sample incoming messages</li>
                        </ul>
                        
                        <p><strong>3. Capture Verification:</strong></p>
                        <ul>
                            <li>Go to Storage Account → Containers → eventhub-capture</li>
                            <li>Verify capture files are being created</li>
                            <li>Download and inspect captured data</li>
                        </ul>
                        
                        <p><strong>4. CLI Monitoring:</strong></p>
                        <div class="code-block">
az monitor metrics list \
    --resource /subscriptions/[sub]/resourceGroups/Streaming-RG/providers/Microsoft.EventHub/namespaces/enterprise-events-[random]/eventhubs/telemetry-events \
    --metric IncomingRequests \
    --start-time 2024-01-15T10:00:00Z \
    --end-time 2024-01-15T11:00:00Z
</div>
     --end-time 2024-01-15T11:00:00Z
                        </div>
                        
                        <div class="success">
                            <strong>✅ Ingestion Success:</strong> Events should appear in metrics, live stream should show messages, and capture files should be created in storage.
                        </div>
                    </div>
                </div>
            </div>

            <!-- Step 3 -->
            <div class="step-section">
                <div class="step-header">
                    <div class="step-title">Step 3: Building Event Consumers and Processing</div>
                    <div class="step-description">Create consumers to process streaming events in real-time</div>
                </div>
                <div class="step-content">
                    <div class="substep">
                        <div class="substep-number">1</div>
                        <div class="substep-title">Create Real-time Analytics Consumer</div>
                        <p>Build a consumer for real-time data analysis:</p>
                        
                        <div class="code-block">
# analytics-consumer.js
const { EventHubConsumerClient } = require('@azure/event-hubs');
const { BlobCheckpointStore } = require('@azure/eventhubs-checkpointstore-blob');
const { BlobServiceClient } = require('@azure/storage-blob');

class AnalyticsConsumer {
    constructor() {
        this.connectionString = "Endpoint=sb://enterprise-events-[random].servicebus.windows.net/;SharedAccessKeyName=EventConsumer;SharedAccessKey=[key];EntityPath=telemetry-events";
        this.storageConnectionString = "DefaultEndpointsProtocol=https;AccountName=[storage];AccountKey=[key];EndpointSuffix=core.windows.net";
        this.containerName = "event-checkpoints";
        this.consumerGroup = "analytics-consumer";
        
        this.metrics = {
            totalEvents: 0,
            eventsByType: {},
            avgTemperature: 0,
            highTempAlerts: 0,
            lastProcessedTime: null
        };
    }

    async initialize() {
        // Create checkpoint store for reliable processing
        const blobServiceClient = BlobServiceClient.fromConnectionString(this.storageConnectionString);
        const checkpointStore = new BlobCheckpointStore(blobServiceClient, this.containerName);
        
        this.consumerClient = new EventHubConsumerClient(
            this.consumerGroup,
            this.connectionString,
            checkpointStore
        );
        
        console.log('Analytics consumer initialized');
    }

    processEvent(eventData) {
        try {
            const event = eventData.body;
            this.metrics.totalEvents++;
            this.metrics.lastProcessedTime = new Date().toISOString();
            
            // Track events by type
            const eventType = event.eventType || 'telemetry';
            this.metrics.eventsByType[eventType] = (this.metrics.eventsByType[eventType] || 0) + 1;
            
            // Process telemetry events
            if (event.telemetry && event.telemetry.temperature) {
                const temp = event.telemetry.temperature;
                
                // Update running average (simplified)
                this.metrics.avgTemperature = (
                    (this.metrics.avgTemperature * (this.metrics.totalEvents - 1)) + temp
                ) / this.metrics.totalEvents;
                
                // High temperature alert
                if (temp > 35) {
                    this.metrics.highTempAlerts++;
                    console.log(`🔥 HIGH TEMP ALERT: Device ${event.deviceId} - ${temp}°C at ${event.timestamp}`);
                    
                    // In real scenario: send alert to monitoring system
                    this.sendAlert({
                        type: 'high_temperature',
                        deviceId: event.deviceId,
                        temperature: temp,
                        location: event.location,
                        timestamp: event.timestamp
                    });
                }
            }
            
            // Process user activity events
            if (event.eventType === 'user_activity') {
                if (event.action === 'purchase') {
                    console.log(`💰 Purchase event: User ${event.userId} at ${event.timestamp}`);
                }
            }
            
            // Log progress every 100 events
            if (this.metrics.totalEvents % 100 === 0) {
                this.logMetrics();
            }
            
        } catch (error) {
            console.error('Error processing event:', error);
        }
    }

    sendAlert(alert) {
        // Simulate sending alert to external system
        console.log('📢 ALERT SENT:', JSON.stringify(alert, null, 2));
        // In real scenario: integrate with Logic Apps, Functions, or external APIs
    }

    logMetrics() {
        console.log('\n📊 ANALYTICS METRICS:');
        console.log(`Total Events Processed: ${this.metrics.totalEvents}`);
        console.log(`Events by Type:`, this.metrics.eventsByType);
        console.log(`Average Temperature: ${this.metrics.avgTemperature.toFixed(2)}°C`);
        console.log(`High Temp Alerts: ${this.metrics.highTempAlerts}`);
        console.log(`Last Processed: ${this.metrics.lastProcessedTime}`);
        console.log('\n');
    }

    async startProcessing() {
        console.log('Starting analytics consumer...');
        
        const subscription = this.consumerClient.subscribe({
            processEvents: async (events, context) => {
                for (const event of events) {
                    this.processEvent(event);
                }
                
                // Update checkpoint to mark these events as processed
                if (events.length > 0) {
                    await context.updateCheckpoint(events[events.length - 1]);
                }
            },
            processError: async (err, context) => {
                console.error('Error in consumer:', err);
            }
        });
        
        // Log metrics every 30 seconds
        this.metricsInterval = setInterval(() => {
            if (this.metrics.totalEvents > 0) {
                this.logMetrics();
            }
        }, 30000);
        
        return subscription;
    }

    async stopProcessing() {
        console.log('Stopping analytics consumer...');
        if (this.metricsInterval) {
            clearInterval(this.metricsInterval);
        }
        await this.consumerClient.close();
    }
}

// Usage
const consumer = new AnalyticsConsumer();

process.on('SIGINT', async () => {
    console.log('\nShutting down consumer...');
    await consumer.stopProcessing();
    process.exit(0);
});

consumer.initialize().then(() => {
    consumer.startProcessing();
});
                        </div>
                    </div>

                    <div class="substep">
                        <div class="substep-number">2</div>
                        <div class="substep-title">Configure Consumer Groups</div>
                        <p>Set up multiple consumer groups for different processing needs:</p>
                        
                        <p><strong>1. Create consumer groups in Azure Portal:</strong></p>
                        <ul>
                            <li>Go to Event Hub → Consumer groups</li>
                            <li>Add new consumer groups:
                                <ul>
                                    <li>analytics-consumer (real-time analytics)</li>
                                    <li>monitoring-consumer (system monitoring)</li>
                                    <li>archive-consumer (long-term storage)</li>
                                    <li>ml-consumer (machine learning pipeline)</li>
                                </ul>
                            </li>
                        </ul>

                        <p><strong>2. Consumer Group Benefits:</strong></p>
                        <ul>
                            <li>Each consumer group maintains independent position</li>
                            <li>Allows parallel processing of same events</li>
                            <li>Different processing speeds and patterns</li>
                            <li>Independent checkpoint management</li>
                        </ul>
                        
                        <p><strong>3. Consumer Group Best Practices:</strong></p>
                        <ul>
                            <li>One consumer group per application/use case</li>
                            <li>Use descriptive names</li>
                            <li>Monitor consumer lag</li>
                            <li>Plan for scaling (multiple instances per group)</li>
                        </ul>

                        <p><strong>Create consumer groups via CLI:</strong></p>
                        
                        <div class="code-block">
az eventhubs eventhub consumer-group create \
    --resource-group Streaming-RG \
    --namespace-name enterprise-events-[random] \
    --eventhub-name telemetry-events \
    --name analytics-consumer
</div>

                        <div class="code-block">
az eventhubs eventhub consumer-group create \
    --resource-group Streaming-RG \
    --namespace-name enterprise-events-[random] \
    --eventhub-name telemetry-events \
    --name monitoring-consumer
</div>
                    </div>

                    <div class="substep">
                        <div class="substep-number">3</div>
                        <div class="substep-title">Monitor Consumer Performance</div>
                        <p>Track consumer performance and processing lag:</p>
                        
                        <div class="code-block">
Consumer Monitoring:

1. Key metrics to monitor:
   - Consumer lag (events behind)
   - Processing rate (events/second)
   - Error rate and failed events
   - Checkpoint frequency
   - Processing latency

2. Azure Portal monitoring:
   - Go to Event Hub > Metrics
   - Select "Consumer Lag" metric
   - Filter by consumer group
   - Set up alerts for high lag

3. Custom monitoring in consumer:
   - Track processing time per event
   - Log throughput metrics
   - Monitor memory and CPU usage
   - Alert on processing errors

4. Scaling considerations:
   - Scale out consumers when lag increases
   - One consumer per partition max efficiency
   - Use Azure Container Instances or App Service
   - Implement auto-scaling based on lag metrics

# Example monitoring query (Log Analytics):
Event
| where Source == "AnalyticsConsumer"
| summarize 
    EventsProcessed = count(),
    AvgProcessingTime = avg(ProcessingTimeMs),
    ErrorRate = countif(Level == "Error") * 100.0 / count()
  by bin(TimeGenerated, 5m)
| order by TimeGenerated desc
                        </div>
                        
                        <div class="success">
                            <strong>✅ Processing Success:</strong> Consumers should process events with low lag, metrics should show healthy processing rates, and alerts should trigger for high temperatures.
                        </div>
                    </div>
                </div>
            </div>

            <!-- Validation Section -->
            <div class="validation-section">
                <h4>🔍 Validation & Testing</h4>
                <ol>
                    <li><strong>Event Hub Created:</strong> Verify namespace and event hub are operational</li>
                    <li><strong>Producers Sending:</strong> Confirm events are being sent successfully</li>
                    <li><strong>Event Ingestion:</strong> Verify events appear in Event Hub metrics</li>
                    <li><strong>Capture Working:</strong> Check that events are captured to storage</li>
                    <li><strong>Consumers Processing:</strong> Verify consumers receive and process events</li>
                    <li><strong>Consumer Groups:</strong> Confirm multiple consumer groups work independently</li>
                    <li><strong>Checkpointing:</strong> Verify checkpoint store maintains processing position</li>
                    <li><strong>Monitoring Active:</strong> Check metrics, alerts, and performance monitoring</li>
                </ol>
            </div>

            <div class="troubleshooting">
                <h4>🛠️ Common Issues & Solutions</h4>
                <ul>
                    <li><strong>Connection failures:</strong> Verify connection strings and access policies</li>
                    <li><strong>Throttling errors:</strong> Increase throughput units or reduce send rate</li>
                    <li><strong>Consumer lag:</strong> Scale out consumers or optimize processing logic</li>
                    <li><strong>Checkpoint failures:</strong> Check storage account access and container existence</li>
                    <li><strong>High costs:</strong> Monitor throughput unit usage and optimize auto-inflate settings</li>
                    <li><strong>Message loss:</strong> Verify capture settings and retention policies</li>
                </ul>
            </div>

            <div class="resource-links">
                <h4>📚 Additional Resources</h4>
                <ul>
                    <li><a href="https://docs.microsoft.com/en-us/azure/event-hubs/" target="_blank">Azure Event Hubs Documentation</a></li>
                    <li><a href="https://docs.microsoft.com/en-us/azure/event-hubs/event-hubs-node-get-started-send" target="_blank">Node.js Quick Start Guide</a></li>
                    <li><a href="https://docs.microsoft.com/en-us/azure/event-hubs/event-hubs-capture-overview" target="_blank">Event Hubs Capture Overview</a></li>
                    <li><a href="https://docs.microsoft.com/en-us/azure/event-hubs/event-hubs-scalability" target="_blank">Scaling and Performance Guide</a></li>
                </ul>
            </div>
        </div>

        <div id="solution16" class="solution-card">
            <div class="solution-header">
                <div class="solution-title">Assignment 16: Azure API Management</div>
                <div class="solution-meta">
                    <span class="meta-tag">� API Gateway</span>
                    <span class="meta-tag">⏱️ 90 minutes</span>
                    <span class="meta-tag">📊 Advanced</span>
                </div>
            </div>
            
            <div class="time-estimate">
                <strong>⏱️ Estimated Time:</strong> 1 hour 30 minutes<br>
                <strong>🎯 Learning Objective:</strong> Build enterprise API gateway with security, monitoring, and developer portal
            </div>

            <div class="prerequisites">
                <h4>📋 Prerequisites</h4>
                <ul>
                    <li>Understanding of REST APIs and HTTP protocols</li>
                    <li>Familiarity with API design patterns and OpenAPI specification</li>
                    <li>Completed Assignment 4 (Web App) or Assignment 13 (Functions) for backend API</li>
                    <li>Basic knowledge of JSON, XML, and API security concepts</li>
                    <li>Understanding of OAuth 2.0 and JWT tokens (beneficial)</li>
                </ul>
            </div>

            <!-- Step 1 -->
            <div class="step-section">
                <div class="step-header">
                    <div class="step-title">Step 1: Creating API Management Instance</div>
                    <div class="step-description">Set up API gateway infrastructure with developer portal</div>
                </div>
                <div class="step-content">
                    <div class="substep">
                        <div class="substep-number">1</div>
                        <div class="substep-title">Create API Management Service</div>
                        <p>Set up the API Management instance:</p>
                        <ol>
                            <li>Go to "Create a resource" > "API Management"</li>
                            <li>Configure the service:</li>
                        </ol>
                        
                        <div class="code-block">
Subscription: (Your subscription)
Resource Group: Create new "API-RG"
Service name: enterprise-apim-[random]
Location: East US
Organization name: Enterprise Corp
Administrator email: admin@company.com

Pricing tier: Developer (no SLA)
  - For production, use Standard or Premium
  - Developer tier is cost-effective for testing
  - Includes all features except SLA and scaling

Virtual network: None (for simplicity)
Application Insights: Create new
  Name: apim-insights
  Location: East US
                        </div>
                        
                        <p>Click "Review + Create" and then "Create" (deployment takes 30-45 minutes).</p>
                        
                        <div class="warning">
                            <strong>⚠️ Deployment Time:</strong> API Management takes significantly longer to deploy than other services. Use this time to prepare backend APIs.
                        </div>
                    </div>

                    <div class="substep">
                        <div class="substep-number">2</div>
                        <div class="substep-title">Prepare Backend API</div>
                        <p>While APIM deploys, prepare a backend API to manage:</p>
                        
                        <div class="code-block">
# Sample Node.js backend API (if you don't have one from previous assignments)

# server.js
const express = require('express');
const app = express();
const port = process.env.PORT || 3000;

app.use(express.json());

// Sample data
let products = [
    { id: 1, name: 'Laptop', category: 'Electronics', price: 999.99, stock: 50 },
    { id: 2, name: 'Smartphone', category: 'Electronics', price: 699.99, stock: 100 },
    { id: 3, name: 'Desk Chair', category: 'Furniture', price: 199.99, stock: 25 },
    { id: 4, name: 'Coffee Maker', category: 'Appliances', price: 79.99, stock: 30 }
];

// API endpoints
app.get('/api/products', (req, res) => {
    const { category, limit } = req.query;
    let filtered = products;
    
    if (category) {
        filtered = products.filter(p => p.category.toLowerCase() === category.toLowerCase());
    }
    
    if (limit) {
        filtered = filtered.slice(0, parseInt(limit));
    }
    
    res.json({
        products: filtered,
        total: filtered.length,
        timestamp: new Date().toISOString()
    });
});

app.get('/api/products/:id', (req, res) => {
    const product = products.find(p => p.id === parseInt(req.params.id));
    if (!product) {
        return res.status(404).json({ error: 'Product not found' });
    }
    res.json(product);
});

app.post('/api/products', (req, res) => {
    const { name, category, price, stock } = req.body;
    const newProduct = {
        id: Math.max(...products.map(p => p.id)) + 1,
        name,
        category,
        price: parseFloat(price),
        stock: parseInt(stock)
    };
    products.push(newProduct);
    res.status(201).json(newProduct);
});

app.get('/api/health', (req, res) => {
    res.json({ 
        status: 'healthy', 
        timestamp: new Date().toISOString(),
        version: '1.0.0'
    });
});

app.listen(port, () => {
    console.log(`Product API running on port ${port}`);
});

# Deploy this to Azure App Service or use existing backend from Assignment 4/13
                        </div>
                    </div>

                    <div class="substep">
                        <div class="substep-number">3</div>
                        <div class="substep-title">Access API Management Portal</div>
                        <p>Once deployed, access the management interfaces:</p>
                        
                        <div class="code-block">
API Management URLs:

1. Azure Portal Management:
   https://portal.azure.com -> Your APIM service

2. API Management Gateway:
   https://enterprise-apim-[random].azure-api.net

3. Developer Portal (for API consumers):
   https://enterprise-apim-[random].developer.azure-api.net

4. Management Portal (legacy):
   https://enterprise-apim-[random].management.azure-api.net

5. Git repository (for version control):
   https://enterprise-apim-[random].scm.azure-api.net

Login credentials:
- Use Azure AD account for management
- Developer portal supports various authentication methods
                        </div>
                        
                        <div class="tip">
                            <strong>💡 Portal Access:</strong> The developer portal is where external developers will discover and test your APIs. The Azure portal is for service management.
                        </div>
                    </div>
                </div>
            </div>

            <!-- Step 2 -->
            <div class="step-section">
                <div class="step-header">
                    <div class="step-title">Step 2: Importing and Configuring APIs</div>
                    <div class="step-description">Add backend APIs with transformations and policies</div>
                </div>
                <div class="step-content">
                    <div class="substep">
                        <div class="substep-number">1</div>
                        <div class="substep-title">Import Backend API</div>
                        <p>Add your backend API to API Management:</p>
                        <ol>
                            <li>Go to API Management service > APIs</li>
                            <li>Click "+ Add API"</li>
                            <li>Choose import method:</li>
                        </ol>
                        
                        <div class="code-block">
API Import Options:

1. OpenAPI specification (recommended):
   - If your backend exposes /swagger or OpenAPI spec
   - URL: https://your-backend-app.azurewebsites.net/swagger/v1/swagger.json

2. Manual import:
   - API type: HTTP
   - Display name: Products API
   - Name: products-api
   - Description: Enterprise product catalog API
   - API URL suffix: products
   - Base URL: https://your-backend-app.azurewebsites.net/api
   
3. Import from existing App Service:
   - Select your App Service from Assignment 4
   - APIM will discover available endpoints

API Settings:
- Version scheme: Header (recommended)
- Version header name: Api-Version
- Default version: v1
- Products: Unlimited (for now)
                        </div>
                    </div>

                    <div class="substep">
                        <div class="substep-number">2</div>
                        <div class="substep-title">Configure API Operations</div>
                        <p>Define and configure individual API operations:</p>
                        
                        <div class="code-block">
Manual Operation Configuration:

1. GET /products
   Display name: Get Products
   Description: Retrieve all products with optional filtering
   URL template: /products
   Parameters:
     - category (query, string, optional)
     - limit (query, integer, optional)
   
2. GET /products/{id}
   Display name: Get Product by ID
   Description: Retrieve a specific product
   URL template: /products/{id}
   Parameters:
     - id (path, integer, required)
   
3. POST /products
   Display name: Create Product
   Description: Add a new product
   URL template: /products
   Request body schema:
   {
     "name": "string",
     "category": "string", 
     "price": "number",
     "stock": "integer"
   }

4. GET /health
   Display name: Health Check
   Description: API health status
   URL template: /health

Response examples for each operation:
- Add sample responses for 200, 400, 404, 500
- Include JSON schema definitions
- Document expected response headers
                        </div>
                    </div>

                    <div class="substep">
                        <div class="substep-number">3</div>
                        <div class="substep-title">Apply Transformation Policies</div>
                        <p>Configure policies for request/response transformation:</p>
                        
                        <div class="code-block">
API-Level Policies:

1. Inbound policies (All operations):
<policies>
    <inbound>
        <base />
        <!-- CORS for web applications -->
        <cors allow-credentials="false">
            <allowed-origins>
                <origin>*</origin>
            </allowed-origins>
            <allowed-methods>
                <method>GET</method>
                <method>POST</method>
                <method>PUT</method>
                <method>DELETE</method>
                <method>OPTIONS</method>
            </allowed-methods>
            <allowed-headers>
                <header>Content-Type</header>
                <header>Authorization</header>
                <header>Api-Version</header>
            </allowed-headers>
        </cors>
        
        <!-- Rate limiting -->
        <rate-limit calls="100" renewal-period="60" />
        
        <!-- Add custom headers -->
        <set-header name="X-API-Gateway" exists-action="override">
            <value>Azure API Management</value>
        </set-header>
        
        <!-- Request transformation -->
        <set-query-parameter name="source" exists-action="override">
            <value>apim</value>
        </set-query-parameter>
    </inbound>
    <backend>
        <base />
    </backend>
    <outbound>
        <base />
        <!-- Response transformation -->
        <set-header name="X-Powered-By" exists-action="delete" />
        <set-header name="X-Response-Time" exists-action="override">
            <value>@(context.Response.Headers.GetValueOrDefault("Date",""))</value>
        </set-header>
        
        <!-- JSON response transformation -->
        <choose>
            <when condition="@(context.Response.StatusCode == 200)">
                <set-body>@{
                    var body = context.Response.Body.As<JObject>();
                    body["metadata"] = new JObject()
                    {
                        ["gateway"] = "Azure API Management",
                        ["timestamp"] = DateTime.UtcNow.ToString("yyyy-MM-ddTHH:mm:ssZ"),
                        ["requestId"] = context.RequestId
                    };
                    return body.ToString();
                }</set-body>
            </when>
        </choose>
    </outbound>
    <on-error>
        <base />
        <!-- Error handling -->
        <set-body>@{
            return new JObject()
            {
                ["error"] = new JObject()
                {
                    ["code"] = context.Response.StatusCode.ToString(),
                    ["message"] = context.LastError?.Message ?? "An error occurred",
                    ["timestamp"] = DateTime.UtcNow.ToString("yyyy-MM-ddTHH:mm:ssZ"),
                    ["requestId"] = context.RequestId
                }
            }.ToString();
        }</set-body>
    </on-error>
</policies>

2. Operation-specific policies (GET /products):
<policies>
    <inbound>
        <base />
        <!-- Caching for product list -->
        <cache-lookup vary-by-developer="false" vary-by-developer-groups="false">
            <vary-by-query-parameter>category</vary-by-query-parameter>
            <vary-by-query-parameter>limit</vary-by-query-parameter>
        </cache-lookup>
    </inbound>
    <backend>
        <base />
    </backend>
    <outbound>
        <base />
        <!-- Cache response for 5 minutes -->
        <cache-store duration="300" />
    </outbound>
</policies>
                        </div>
                    </div>
                </div>
            </div>

            <!-- Step 3 -->
            <div class="step-section">
                <div class="step-header">
                    <div class="step-title">Step 3: Security and Developer Portal</div>
                    <div class="step-description">Configure API security and set up developer portal</div>
                </div>
                <div class="step-content">
                    <div class="substep">
                        <div class="substep-number">1</div>
                        <div class="substep-title">Configure API Security</div>
                        <p>Set up authentication and authorization:</p>
                        
                        <div class="code-block">
Security Configuration:

1. Subscription Keys:
   - Go to Products > Add Product
   - Product name: Premium APIs
   - Description: Premium tier with enhanced features
   - Requires subscription: Yes
   - Requires approval: Yes
   - Subscription limit: 1000 calls/month
   - Add APIs: Products API

2. OAuth 2.0 (Optional advanced security):
   - Go to APIs > Security > OAuth 2.0
   - Authorization server: Azure AD
   - Client credentials or Authorization code flow
   
3. IP whitelisting:
   <ip-filter action="allow">
       <address>203.0.113.0/24</address>
       <address>198.51.100.14</address>
   </ip-filter>

4. Header-based authentication:
   <policies>
       <inbound>
           <base />
           <check-header name="X-API-Key" failed-check-httpcode="401" failed-check-error-message="Missing or invalid API key">
               <value>SecretKey123</value>
           </check-header>
       </inbound>
   </policies>

5. JWT token validation:
   <validate-jwt header-name="Authorization" failed-validation-httpcode="401">
       <openid-config url="https://login.microsoftonline.com/common/.well-known/openid_configuration" />
       <required-claims>
           <claim name="aud">
               <value>your-api-audience</value>
           </claim>
       </required-claims>
   </validate-jwt>
                        </div>
                    </div>

                    <div class="substep">
                        <div class="substep-number">2</div>
                        <div class="substep-title">Customize Developer Portal</div>
                        <p>Configure the developer portal for API consumers:</p>
                        
                        <div class="code-block">
Developer Portal Setup:

1. Access developer portal:
   https://enterprise-apim-[random].developer.azure-api.net

2. Customize portal content:
   - Go to API Management > Developer portal
   - Edit welcome page
   - Add company branding
   - Configure contact information
   - Add API documentation

3. Configure user registration:
   - Enable self-service signup
   - Configure email templates
   - Set approval workflow
   - Define user groups (Developers, Partners, Internal)

4. API documentation:
   - Add operation descriptions
   - Include code samples
   - Provide example requests/responses
   - Add getting started guides

5. Test API in portal:
   - Interactive API testing
   - Try-it functionality
   - Code generation for multiple languages
   - Download OpenAPI specifications

Sample API documentation content:
---
# Products API Guide

## Authentication
All API calls require a subscription key. Include the key in the header:
```
Ocp-Apim-Subscription-Key: {your-subscription-key}
```

## Rate Limits
- Free tier: 100 calls/hour
- Premium tier: 1000 calls/hour

## Quick Start
1. Register for API access
2. Subscribe to a product
3. Get your subscription key
4. Make your first API call

## Code Examples
```javascript
// JavaScript
const response = await fetch('/api/products', {
    headers: {
        'Ocp-Apim-Subscription-Key': 'your-key'
    }
});
```
                        </div>
                    </div>

                    <div class="substep">
                        <div class="substep-number">3</div>
                        <div class="substep-title">Test API Management</div>
                        <p>Test the complete API Management setup:</p>
                        
                        <div class="code-block">
Testing Scenarios:

1. Test via API Management gateway:
   GET https://enterprise-apim-[random].azure-api.net/products/api/products
   Headers:
     Ocp-Apim-Subscription-Key: {subscription-key}
     Api-Version: v1

2. Test rate limiting:
   # Make rapid requests to trigger rate limit
   for i in {1..105}; do
     curl -H "Ocp-Apim-Subscription-Key: {key}" \
          "https://enterprise-apim-[random].azure-api.net/products/api/products"
   done

3. Test caching:
   # First request - hits backend
   curl -H "Ocp-Apim-Subscription-Key: {key}" \
        "https://enterprise-apim-[random].azure-api.net/products/api/products?category=Electronics"
   
   # Second request - served from cache (faster response)
   curl -H "Ocp-Apim-Subscription-Key: {key}" \
        "https://enterprise-apim-[random].azure-api.net/products/api/products?category=Electronics"

4. Test error handling:
   # Invalid product ID
   curl -H "Ocp-Apim-Subscription-Key: {key}" \
        "https://enterprise-apim-[random].azure-api.net/products/api/products/999"

5. Test without subscription key:
   curl "https://enterprise-apim-[random].azure-api.net/products/api/products"
   # Should return 401 Unauthorized

6. Monitor in Azure Portal:
   - Go to API Management > Analytics
   - View request metrics, response times, error rates
   - Check Application Insights for detailed logs
                        </div>
                        
                        <div class="success">
                            <strong>✅ Testing Success:</strong> API should be accessible through APIM gateway, rate limiting should work, and metrics should appear in analytics.
                        </div>
                    </div>
                </div>
            </div>

            <!-- Validation Section -->
            <div class="validation-section">
                <h4>🔍 Validation & Testing</h4>
                <ol>
                    <li><strong>APIM Service Created:</strong> Verify API Management service is deployed and accessible</li>
                    <li><strong>API Imported:</strong> Confirm backend API is properly imported and configured</li>
                    <li><strong>Operations Defined:</strong> Verify all API operations are documented and working</li>
                    <li><strong>Policies Applied:</strong> Test transformation policies and rate limiting</li>
                    <li><strong>Security Working:</strong> Verify subscription key authentication is required</li>
                    <li><strong>Caching Active:</strong> Test response caching for GET operations</li>
                    <li><strong>Developer Portal:</strong> Confirm portal is accessible and properly configured</li>
                    <li><strong>Analytics Functional:</strong> Check that API metrics and logs are collected</li>
                </ol>
            </div>

            <div class="troubleshooting">
                <h4>🛠️ Common Issues & Solutions</h4>
                <ul>
                    <li><strong>Long deployment times:</strong> APIM deployment is normal to take 30-45 minutes</li>
                    <li><strong>Backend connectivity issues:</strong> Check backend URL and network connectivity</li>
                    <li><strong>401 Unauthorized errors:</strong> Verify subscription key is included in requests</li>
                    <li><strong>CORS errors:</strong> Configure CORS policy for web application access</li>
                    <li><strong>Policy errors:</strong> Validate XML syntax in policy configurations</li>
                    <li><strong>Developer portal not loading:</strong> Check service status and wait for full deployment</li>
                </ul>
            </div>

            <div class="resource-links">
                <h4>📚 Additional Resources</h4>
                <ul>
                    <li><a href="https://docs.microsoft.com/en-us/azure/api-management/" target="_blank">Azure API Management Documentation</a></li>
                    <li><a href="https://docs.microsoft.com/en-us/azure/api-management/api-management-policies" target="_blank">API Management Policies Reference</a></li>
                    <li><a href="https://docs.microsoft.com/en-us/azure/api-management/api-management-developer-portal-templates" target="_blank">Developer Portal Customization</a></li>
                    <li><a href="https://docs.microsoft.com/en-us/azure/api-management/api-management-security-controls" target="_blank">Security and Authentication Guide</a></li>
                </ul>
            </div>
        </div>
                        </div>
                    </div>
                </div>
            </div>

            <div class="validation-section">
                <h4>🔍 Validation</h4>
                <p>✅ Workflow executing, connectors working, error handling active, monitoring enabled</p>
            </div>
        </div>

        <div id="solution17" class="solution-card">
            <div class="solution-header">
                <div class="solution-title">Assignment 17: Azure Service Bus Advanced Messaging</div>
                <div class="solution-meta">
                    <span class="meta-tag">🚌 Messaging</span>
                    <span class="meta-tag">⏱️ 85 minutes</span>
                    <span class="meta-tag">📊 Advanced</span>
                </div>
            </div>
            
            <div class="time-estimate">
                <strong>⏱️ Estimated Time:</strong> 85 minutes<br>
                <strong>🎯 Learning Objective:</strong> Implement enterprise messaging patterns with Service Bus queues, topics, and dead letter handling
            </div>

            <div class="prerequisites-section">
                <h4>📋 Prerequisites</h4>
                <ul>
                    <li>Azure subscription with Contributor access</li>
                    <li>Visual Studio Code with Azure extensions</li>
                    <li>.NET 6.0 SDK or Node.js runtime</li>
                    <li>Azure CLI installed and configured</li>
                    <li>Basic understanding of messaging patterns</li>
                </ul>
            </div>

            <div class="step-section">
                <div class="step-header">
                    <div class="step-title">Step 1: Service Bus Namespace Setup</div>
                    <div class="step-description">Create and configure Service Bus namespace with proper security</div>
                </div>
                <div class="step-content">
                    <div class="substep">
                        <div class="substep-number">1</div>
                        <div class="substep-title">Create Resource Group and Service Bus</div>
                        <div class="code-block">
# Create resource group
az group create \
    --name ServiceBus-RG \
    --location eastus

# Create Service Bus namespace
az servicebus namespace create \
    --name enterprise-servicebus-ns \
    --resource-group ServiceBus-RG \
    --location eastus \
    --sku Standard

# Get connection string
az servicebus namespace authorization-rule keys list \
    --resource-group ServiceBus-RG \
    --namespace-name enterprise-servicebus-ns \
    --name RootManageSharedAccessKey \
    --query primaryConnectionString \
    --output tsv
                        </div>
  --name RootManageSharedAccessKey
                        </div>
                    </div>
                    <div class="substep">
                        <div class="substep-number">2</div>
                        <div class="substep-title">Configure Namespace Properties</div>
                        <div class="code-block">
# Enable zone redundancy (if supported in region)
az servicebus namespace update \
  --name enterprise-servicebus-ns \
  --resource-group ServiceBus-RG \
  --enable-zone-redundancy true

# Configure minimum TLS version
az servicebus namespace update \
  --name enterprise-servicebus-ns \
  --resource-group ServiceBus-RG \
  --minimum-tls-version 1.2
                        </div>
                    </div>
                    <div class="substep">
                        <div class="substep-number">3</div>
                        <div class="substep-title">Network Security Configuration</div>
                        <div class="code-block">
# Create network rule (if Premium tier)
# For Standard tier, configure IP filtering
az servicebus namespace network-rule add \
  --resource-group ServiceBus-RG \
  --namespace-name enterprise-servicebus-ns \
  --ip-address 203.0.113.0/24 \
  --action Allow
                        </div>
                    </div>
                </div>
            </div>

            <div class="step-section">
                <div class="step-header">
                    <div class="step-title">Step 2: Queues and Topics Configuration</div>
                    <div class="step-description">Set up queues for point-to-point messaging and topics for publish-subscribe patterns</div>
                </div>
                <div class="step-content">
                    <div class="substep">
                        <div class="substep-number">1</div>
                        <div class="substep-title">Create Queues with Advanced Features</div>
                        <p>Create order processing queue with advanced features:</p>
                        
                        <div class="code-block">
az servicebus queue create \
    --resource-group ServiceBus-RG \
    --namespace-name enterprise-servicebus-ns \
    --name order-processing \
    --max-size 5120 \
    --default-message-time-to-live P30D \
    --lock-duration PT5M \
    --max-delivery-count 10 \
    --enable-dead-lettering-on-message-expiration true \
    --enable-duplicate-detection true \
    --duplicate-detection-history-time-window PT10M
</div>

                        <p>Create high-priority queue with session support:</p>
                        
                        <div class="code-block">
az servicebus queue create \
    --resource-group ServiceBus-RG \
    --namespace-name enterprise-servicebus-ns \
    --name high-priority-orders \
    --max-size 2048 \
    --lock-duration PT2M \
    --enable-sessions true
</div>
                    </div>
                    <div class="substep">
                        <div class="substep-number">2</div>
                        <div class="substep-title">Create Topics and Subscriptions</div>
                        <p>Create notifications topic:</p>
                        
                        <div class="code-block">
az servicebus topic create \
    --resource-group ServiceBus-RG \
    --namespace-name enterprise-servicebus-ns \
    --name notifications \
    --max-size 5120 \
    --enable-duplicate-detection true
</div>

                        <p>Create email subscription with filter:</p>
                        
                        <div class="code-block">
az servicebus topic subscription create \
    --resource-group ServiceBus-RG \
    --namespace-name enterprise-servicebus-ns \
    --topic-name notifications \
    --name email-subscription \
    --max-delivery-count 5
</div>

                        <p>Add SQL filter for email subscription:</p>
                        
                        <div class="code-block">
az servicebus topic subscription rule create \
    --resource-group ServiceBus-RG \
    --namespace-name enterprise-servicebus-ns \
    --topic-name notifications \
    --subscription-name email-subscription \
    --name EmailFilter \
    --filter-sql-expression "NotificationType = 'Email'"
</div>

                        <p>Create SMS subscription:</p>
                        
                        <div class="code-block">
az servicebus topic subscription create \
    --resource-group ServiceBus-RG \
    --namespace-name enterprise-servicebus-ns \
    --topic-name notifications \
    --name sms-subscription
</div>

                        <p>Add correlation filter for SMS:</p>
                        
                        <div class="code-block">
az servicebus topic subscription rule create \
    --resource-group ServiceBus-RG \
    --namespace-name enterprise-servicebus-ns \
    --topic-name notifications \
    --subscription-name sms-subscription \
    --name SMSFilter \
    --correlation-filter NotificationType=SMS
</div>
                    </div>
                    <div class="substep">
                        <div class="substep-number">3</div>
                        <div class="substep-title">Configure Auto-forwarding</div>
                        <p>Create audit queue for forwarding:</p>
                        
                        <div class="code-block">
az servicebus queue create \
    --resource-group ServiceBus-RG \
    --namespace-name enterprise-servicebus-ns \
    --name audit-trail
</div>

                        <p>Configure auto-forwarding from order-processing to audit:</p>
                        
                        <div class="code-block">
az servicebus queue update \
    --resource-group ServiceBus-RG \
    --namespace-name enterprise-servicebus-ns \
    --name order-processing \
    --forward-to audit-trail
</div>
                    </div>
                </div>
            </div>

            <div class="step-section">
                <div class="step-header">
                    <div class="step-title">Step 3: Message Processing Implementation</div>
                    <div class="step-description">Implement message senders and receivers with proper error handling</div>
                </div>
                <div class="step-content">
                    <div class="substep">
                        <div class="substep-number">1</div>
                        <div class="substep-title">Create Message Sender (.NET)</div>
                        <div class="code-block">
using Azure.Messaging.ServiceBus;
using System.Text.Json;

public class OrderMessageSender
{
    private readonly ServiceBusClient _client;
    private readonly ServiceBusSender _sender;
    
    public OrderMessageSender(string connectionString, string queueName)
    {
        _client = new ServiceBusClient(connectionString);
        _sender = _client.CreateSender(queueName);
    }
    
    public async Task SendOrderAsync(Order order)
    {
        var message = new ServiceBusMessage(JsonSerializer.Serialize(order))
        {
            MessageId = Guid.NewGuid().ToString(),
            ContentType = "application/json",
            TimeToLive = TimeSpan.FromHours(24),
            Subject = "OrderProcessing"
        };
        
        // Add custom properties
        message.ApplicationProperties["OrderType"] = order.Type;
        message.ApplicationProperties["Priority"] = order.Priority;
        message.ApplicationProperties["CustomerId"] = order.CustomerId;
        
        // For session-enabled queues
        if (!string.IsNullOrEmpty(order.CustomerId))
        {
            message.SessionId = order.CustomerId;
        }
        
        await _sender.SendMessageAsync(message);
    }
    
    public async Task SendScheduledMessageAsync(Order order, DateTimeOffset scheduleTime)
    {
        var message = new ServiceBusMessage(JsonSerializer.Serialize(order))
        {
            MessageId = Guid.NewGuid().ToString(),
            Subject = "ScheduledOrder"
        };
        
        await _sender.ScheduleMessageAsync(message, scheduleTime);
    }
}

public class Order
{
    public string Id { get; set; }
    public string CustomerId { get; set; }
    public string Type { get; set; }
    public int Priority { get; set; }
    public decimal Amount { get; set; }
    public DateTime CreatedAt { get; set; }
}
                        </div>
                    </div>
                    <div class="substep">
                        <div class="substep-number">2</div>
                        <div class="substep-title">Create Message Receiver with Error Handling</div>
                        <div class="code-block">
using Azure.Messaging.ServiceBus;
using Microsoft.Extensions.Logging;

public class OrderMessageProcessor
{
    private readonly ServiceBusClient _client;
    private readonly ServiceBusProcessor _processor;
    private readonly ILogger<OrderMessageProcessor> _logger;
    
    public OrderMessageProcessor(string connectionString, string queueName, ILogger<OrderMessageProcessor> logger)
    {
        _client = new ServiceBusClient(connectionString);
        _processor = _client.CreateProcessor(queueName, new ServiceBusProcessorOptions
        {
            MaxConcurrentCalls = 5,
            AutoCompleteMessages = false,
            MaxAutoLockRenewalDuration = TimeSpan.FromMinutes(10)
        });
        _logger = logger;
        
        _processor.ProcessMessageAsync += ProcessMessageHandler;
        _processor.ProcessErrorAsync += ProcessErrorHandler;
    }
    
    private async Task ProcessMessageHandler(ProcessMessageEventArgs args)
    {
        try
        {
            var order = JsonSerializer.Deserialize<Order>(args.Message.Body.ToString());
            
            _logger.LogInformation($"Processing order {order.Id} for customer {order.CustomerId}");
            
            // Simulate order processing
            await ProcessOrder(order);
            
            // Complete the message
            await args.CompleteMessageAsync(args.Message);
            
            _logger.LogInformation($"Successfully processed order {order.Id}");
        }
        catch (JsonException ex)
        {
            _logger.LogError(ex, "Failed to deserialize message");
            await args.DeadLetterMessageAsync(args.Message, "DeserializationError", ex.Message);
        }
        catch (Exception ex)
        {
            _logger.LogError(ex, "Error processing message");
            
            if (args.Message.DeliveryCount >= 3)
            {
                await args.DeadLetterMessageAsync(args.Message, "ProcessingFailed", ex.Message);
            }
            else
            {
                await args.AbandonMessageAsync(args.Message);
            }
        }
    }
    
    private async Task ProcessErrorHandler(ProcessErrorEventArgs args)
    {
        _logger.LogError(args.Exception, "Service Bus processing error");
        await Task.CompletedTask;
    }
    
    private async Task ProcessOrder(Order order)
    {
        // Simulate processing time
        await Task.Delay(1000);
        
        // Business logic here
        if (order.Amount > 10000)
        {
            throw new InvalidOperationException("Order amount exceeds limit");
        }
    }
    
    public async Task StartProcessingAsync()
    {
        await _processor.StartProcessingAsync();
    }
    
    public async Task StopProcessingAsync()
    {
        await _processor.StopProcessingAsync();
        await _processor.DisposeAsync();
        await _client.DisposeAsync();
    }
}
                        </div>
                    </div>
                    <div class="substep">
                        <div class="substep-number">3</div>
                        <div class="substep-title">Topic Publisher and Subscriber</div>
                        <div class="code-block">
public class NotificationPublisher
{
    private readonly ServiceBusClient _client;
    private readonly ServiceBusSender _sender;
    
    public NotificationPublisher(string connectionString, string topicName)
    {
        _client = new ServiceBusClient(connectionString);
        _sender = _client.CreateSender(topicName);
    }
    
    public async Task PublishNotificationAsync(string notificationType, string content, string userId)
    {
        var notification = new
        {
            Id = Guid.NewGuid().ToString(),
            Type = notificationType,
            Content = content,
            UserId = userId,
            Timestamp = DateTime.UtcNow
        };
        
        var message = new ServiceBusMessage(JsonSerializer.Serialize(notification))
        {
            Subject = "UserNotification",
            ContentType = "application/json"
        };
        
        message.ApplicationProperties["NotificationType"] = notificationType;
        message.ApplicationProperties["UserId"] = userId;
        
        await _sender.SendMessageAsync(message);
    }
}

public class NotificationSubscriber
{
    private readonly ServiceBusClient _client;
    private readonly ServiceBusProcessor _processor;
    
    public NotificationSubscriber(string connectionString, string topicName, string subscriptionName)
    {
        _client = new ServiceBusClient(connectionString);
        _processor = _client.CreateProcessor(topicName, subscriptionName);
        
        _processor.ProcessMessageAsync += async args =>
        {
            var content = args.Message.Body.ToString();
            var notificationType = args.Message.ApplicationProperties["NotificationType"].ToString();
            
            Console.WriteLine($"Received {notificationType} notification: {content}");
            
            await args.CompleteMessageAsync(args.Message);
        };
        
        _processor.ProcessErrorAsync += async args =>
        {
            Console.WriteLine($"Error: {args.Exception.Message}");
            await Task.CompletedTask;
        };
    }
    
    public async Task StartAsync() => await _processor.StartProcessingAsync();
    public async Task StopAsync() => await _processor.StopProcessingAsync();
}
                        </div>
                    </div>
                </div>
            </div>

            <div class="step-section">
                <div class="step-header">
                    <div class="step-title">Step 4: Dead Letter Queue Management</div>
                    <div class="step-description">Handle failed messages and implement retry policies</div>
                </div>
                <div class="step-content">
                    <div class="substep">
                        <div class="substep-number">1</div>
                        <div class="substep-title">Dead Letter Queue Processor</div>
                        <div class="code-block">
public class DeadLetterProcessor
{
    private readonly ServiceBusClient _client;
    private readonly ServiceBusReceiver _receiver;
    
    public DeadLetterProcessor(string connectionString, string queueName)
    {
        _client = new ServiceBusClient(connectionString);
        _receiver = _client.CreateReceiver(queueName, new ServiceBusReceiverOptions
        {
            SubQueue = SubQueue.DeadLetter
        });
    }
    
    public async Task ProcessDeadLetterMessagesAsync()
    {
        var messages = await _receiver.ReceiveMessagesAsync(10, TimeSpan.FromSeconds(5));
        
        foreach (var message in messages)
        {
            Console.WriteLine($"Dead letter message: {message.MessageId}");
            Console.WriteLine($"Reason: {message.ApplicationProperties["DeadLetterReason"]}");
            Console.WriteLine($"Error: {message.ApplicationProperties["DeadLetterErrorDescription"]}");
            Console.WriteLine($"Body: {message.Body}");
            
            // Decide whether to requeue or log for manual intervention
            if (ShouldRetry(message))
            {
                await RequeueMessage(message);
            }
            
            await _receiver.CompleteMessageAsync(message);
        }
    }
    
    private bool ShouldRetry(ServiceBusReceivedMessage message)
    {
        var reason = message.ApplicationProperties["DeadLetterReason"]?.ToString();
        return reason != "ProcessingFailed"; // Don't retry processing failures
    }
    
    private async Task RequeueMessage(ServiceBusReceivedMessage deadLetterMessage)
    {
        var originalSender = _client.CreateSender("order-processing");
        
        var newMessage = new ServiceBusMessage(deadLetterMessage.Body)
        {
            MessageId = Guid.NewGuid().ToString(),
            Subject = deadLetterMessage.Subject,
            ContentType = deadLetterMessage.ContentType
        };
        
        // Copy application properties
        foreach (var prop in deadLetterMessage.ApplicationProperties)
        {
            newMessage.ApplicationProperties[prop.Key] = prop.Value;
        }
        
        await originalSender.SendMessageAsync(newMessage);
    }
}
                        </div>
                    </div>
                    <div class="substep">
                        <div class="substep-number">2</div>
                        <div class="substep-title">Monitoring and Metrics Setup</div>
                        <div class="code-block">
# Enable diagnostic settings
az monitor diagnostic-settings create \
  --name ServiceBusLogs \
  --resource "/subscriptions/{subscription-id}/resourceGroups/ServiceBus-RG/providers/Microsoft.ServiceBus/namespaces/enterprise-servicebus-ns" \
  --logs '[{"category":"OperationalLogs","enabled":true},{"category":"RuntimeAuditLogs","enabled":true}]' \
  --metrics '[{"category":"AllMetrics","enabled":true}]' \
  --workspace "/subscriptions/{subscription-id}/resourceGroups/ServiceBus-RG/providers/Microsoft.OperationalInsights/workspaces/servicebus-workspace"

# Create alert rules
az monitor metrics alert create \
  --name "High Message Count" \
  --resource-group ServiceBus-RG \
  --scopes "/subscriptions/{subscription-id}/resourceGroups/ServiceBus-RG/providers/Microsoft.ServiceBus/namespaces/enterprise-servicebus-ns" \
  --condition "avg ActiveMessages > 1000" \
  --description "Alert when active messages exceed 1000" \
  --evaluation-frequency 5m \
  --window-size 15m
                        </div>
                    </div>
                </div>
            </div>

            <div class="validation-section">
                <h4>🔍 Validation Checklist</h4>
                <ul>
                    <li>✅ Service Bus namespace created with Standard tier</li>
                    <li>✅ Queues configured with dead lettering and duplicate detection</li>
                    <li>✅ Topics and subscriptions with proper filters</li>
                    <li>✅ Message sender successfully sends messages</li>
                    <li>✅ Message processor handles messages and errors correctly</li>
                    <li>✅ Dead letter queue processor can handle failed messages</li>
                    <li>✅ Auto-forwarding working between queues</li>
                    <li>✅ Monitoring and alerts configured</li>
                </ul>
            </div>

            <div class="tips-section">
                <h4>💡 Pro Tips</h4>
                <ul>
                    <li><strong>Message Deduplication:</strong> Use duplicate detection window to prevent duplicate processing</li>
                    <li><strong>Session Handling:</strong> Use sessions for ordered message processing per customer</li>
                    <li><strong>Lock Renewal:</strong> Implement lock renewal for long-running message processing</li>
                    <li><strong>Batch Processing:</strong> Process messages in batches for better throughput</li>
                    <li><strong>Connection Pooling:</strong> Reuse ServiceBusClient instances across your application</li>
                </ul>
            </div>

            <div class="troubleshooting-section">
                <h4>🔧 Troubleshooting Guide</h4>
                <div class="troubleshoot-item">
                    <strong>Issue:</strong> Messages stuck in active state<br>
                    <strong>Solution:</strong> Check for unhandled exceptions in message processor, implement proper error handling
                </div>
                <div class="troubleshoot-item">
                    <strong>Issue:</strong> High message delivery count<br>
                    <strong>Solution:</strong> Review message processing logic, consider increasing lock duration
                </div>
                <div class="troubleshoot-item">
                    <strong>Issue:</strong> Subscription not receiving messages<br>
                    <strong>Solution:</strong> Verify filter rules, check subscription configuration
                </div>
            </div>

            <div class="resources-section">
                <h4>📚 Additional Resources</h4>
                <ul>
                    <li><a href="https://docs.microsoft.com/azure/service-bus-messaging/" target="_blank">Service Bus Documentation</a></li>
                    <li><a href="https://docs.microsoft.com/azure/service-bus-messaging/service-bus-dead-letter-queues" target="_blank">Dead Letter Queues</a></li>
                    <li><a href="https://docs.microsoft.com/azure/service-bus-messaging/service-bus-performance-improvements" target="_blank">Performance Best Practices</a></li>
                </ul>
            </div>
        </div>

        <div id="solution18" class="solution-card">
            <div class="solution-header">
                <div class="solution-title">Assignment 18: Azure Event Hubs Real-time Streaming</div>
                <div class="solution-meta">
                    <span class="meta-tag">⚡ Streaming</span>
                    <span class="meta-tag">⏱️ 80 minutes</span>
                    <span class="meta-tag">📊 Advanced</span>
                </div>
            </div>
            
            <div class="time-estimate">
                <strong>⏱️ Estimated Time:</strong> 80 minutes<br>
                <strong>🎯 Learning Objective:</strong> Implement high-throughput event streaming and real-time analytics with Event Hubs
            </div>

            <div class="prerequisites-section">
                <h4>📋 Prerequisites</h4>
                <ul>
                    <li>Azure subscription with Contributor access</li>
                    <li>Visual Studio Code with Azure extensions</li>
                    <li>.NET 6.0 SDK or Python 3.8+</li>
                    <li>Azure CLI installed</li>
                    <li>Understanding of streaming concepts</li>
                </ul>
            </div>

            <div class="step-section">
                <div class="step-header">
                    <div class="step-title">Step 1: Event Hubs Namespace and Hub Setup</div>
                    <div class="step-description">Create Event Hubs infrastructure for high-throughput streaming</div>
                </div>
                <div class="step-content">
                    <div class="substep">
                        <div class="substep-number">1</div>
                        <div class="substep-title">Create Event Hubs Namespace</div>
                        <div class="code-block">
# Create resource group
az group create \
    --name EventHubs-RG \
    --location eastus

# Create Event Hubs namespace
az eventhubs namespace create \
    --name enterprise-eventhubs-ns \
    --resource-group EventHubs-RG \
    --location eastus \
    --sku Standard \
    --throughput-units 2 \
    --enable-auto-inflate true \
    --maximum-throughput-units 10
                        </div>
  --maximum-throughput-units 10

# Enable zone redundancy for high availability
az eventhubs namespace update \
  --name enterprise-eventhubs-ns \
  --resource-group EventHubs-RG \
  --enable-zone-redundant true
                        </div>
                    </div>
                    <div class="substep">
                        <div class="substep-number">2</div>
                        <div class="substep-title">Create Event Hubs</div>
                        <div class="code-block">
# Create telemetry event hub
az eventhubs eventhub create \
  --name telemetry-events \
  --resource-group EventHubs-RG \
  --namespace-name enterprise-eventhubs-ns \
  --partition-count 8 \
  --message-retention 7 \
  --enable-capture true \
  --capture-destination-name EventHubArchive.AzureBlockBlob \
  --capture-destination-blob-container captured-events \
  --capture-destination-storage-account-resource-id "/subscriptions/{subscription-id}/resourceGroups/EventHubs-RG/providers/Microsoft.Storage/storageAccounts/eventhubstorage"

# Create user activity event hub
az eventhubs eventhub create \
  --name user-activity \
  --resource-group EventHubs-RG \
  --namespace-name enterprise-eventhubs-ns \
  --partition-count 4 \
  --message-retention 3

# Create IoT sensor data hub
az eventhubs eventhub create \
  --name iot-sensors \
  --resource-group EventHubs-RG \
  --namespace-name enterprise-eventhubs-ns \
  --partition-count 16 \
  --message-retention 1
                        </div>
                    </div>
                    <div class="substep">
                        <div class="substep-number">3</div>
                        <div class="substep-title">Configure Security and Access Policies</div>
                        <div class="code-block">
# Create shared access policy for sending
az eventhubs eventhub authorization-rule create \
  --name SendPolicy \
  --resource-group EventHubs-RG \
  --namespace-name enterprise-eventhubs-ns \
  --eventhub-name telemetry-events \
  --rights Send

# Create shared access policy for listening
az eventhubs eventhub authorization-rule create \
  --name ListenPolicy \
  --resource-group EventHubs-RG \
  --namespace-name enterprise-eventhubs-ns \
  --eventhub-name telemetry-events \
  --rights Listen

# Get connection strings
az eventhubs eventhub authorization-rule keys list \
  --name SendPolicy \
  --resource-group EventHubs-RG \
  --namespace-name enterprise-eventhubs-ns \
  --eventhub-name telemetry-events
                        </div>
                    </div>
                </div>
            </div>

            <div class="step-section">
                <div class="step-header">
                    <div class="step-title">Step 2: Event Producer Implementation</div>
                    <div class="step-description">Create high-performance event producers with batching and partitioning</div>
                </div>
                <div class="step-content">
                    <div class="substep">
                        <div class="substep-number">1</div>
                        <div class="substep-title">Telemetry Event Producer (.NET)</div>
                        <div class="code-block">
using Azure.Messaging.EventHubs;
using Azure.Messaging.EventHubs.Producer;
using System.Text.Json;

public class TelemetryEventProducer
{
    private readonly EventHubProducerClient _producer;
    private readonly Timer _batchTimer;
    private readonly List<EventData> _batchBuffer;
    private readonly object _lockObject = new object();
    private const int MaxBatchSize = 100;
    
    public TelemetryEventProducer(string connectionString, string eventHubName)
    {
        _producer = new EventHubProducerClient(connectionString, eventHubName);
        _batchBuffer = new List<EventData>();
        
        // Send batches every 5 seconds
        _batchTimer = new Timer(SendBatch, null, TimeSpan.FromSeconds(5), TimeSpan.FromSeconds(5));
    }
    
    public async Task SendTelemetryAsync(TelemetryData telemetry)
    {
        var eventData = new EventData(JsonSerializer.SerializeToUtf8Bytes(telemetry))
        {
            ContentType = "application/json",
            MessageId = Guid.NewGuid().ToString()
        };
        
        // Add custom properties
        eventData.Properties["DeviceId"] = telemetry.DeviceId;
        eventData.Properties["EventType"] = telemetry.EventType;
        eventData.Properties["Timestamp"] = telemetry.Timestamp.ToString("O");
        
        lock (_lockObject)
        {
            _batchBuffer.Add(eventData);
            
            if (_batchBuffer.Count >= MaxBatchSize)
            {
                Task.Run(() => SendBatch(null));
            }
        }
    }
    
    public async Task SendTelemetryWithPartitionKeyAsync(TelemetryData telemetry)
    {
        var eventData = new EventData(JsonSerializer.SerializeToUtf8Bytes(telemetry));
        
        var sendOptions = new SendEventOptions
        {
            PartitionKey = telemetry.DeviceId // Events with same key go to same partition
        };
        
        await _producer.SendAsync(new[] { eventData }, sendOptions);
    }
    
    private async void SendBatch(object state)
    {
        List<EventData> eventsToSend;
        
        lock (_lockObject)
        {
            if (_batchBuffer.Count == 0) return;
            
            eventsToSend = new List<EventData>(_batchBuffer);
            _batchBuffer.Clear();
        }
        
        try
        {
            using var eventBatch = await _producer.CreateBatchAsync();
            
            foreach (var eventData in eventsToSend)
            {
                if (!eventBatch.TryAdd(eventData))
                {
                    // Send current batch and create new one
                    if (eventBatch.Count > 0)
                    {
                        await _producer.SendAsync(eventBatch);
                    }
                    
                    using var newBatch = await _producer.CreateBatchAsync();
                    if (!newBatch.TryAdd(eventData))
                    {
                        Console.WriteLine($"Event too large for batch: {eventData.MessageId}");
                    }
                    else
                    {
                        await _producer.SendAsync(newBatch);
                    }
                }
            }
            
            if (eventBatch.Count > 0)
            {
                await _producer.SendAsync(eventBatch);
                Console.WriteLine($"Sent batch of {eventBatch.Count} events");
            }
        }
        catch (Exception ex)
        {
            Console.WriteLine($"Error sending batch: {ex.Message}");
        }
    }
    
    public async Task DisposeAsync()
    {
        _batchTimer?.Dispose();
        SendBatch(null); // Send final batch
        await _producer.DisposeAsync();
    }
}

public class TelemetryData
{
    public string DeviceId { get; set; }
    public string EventType { get; set; }
    public DateTime Timestamp { get; set; }
    public double Temperature { get; set; }
    public double Humidity { get; set; }
    public string Location { get; set; }
    public Dictionary<string, object> CustomProperties { get; set; }
}
                        </div>
                    </div>
                    <div class="substep">
                        <div class="substep-number">2</div>
                        <div class="substep-title">IoT Sensor Data Generator</div>
                        <div class="code-block">
public class IoTSensorSimulator
{
    private readonly TelemetryEventProducer _producer;
    private readonly Random _random = new Random();
    private readonly Timer _simulationTimer;
    
    public IoTSensorSimulator(TelemetryEventProducer producer)
    {
        _producer = producer;
        _simulationTimer = new Timer(GenerateSensorData, null, TimeSpan.Zero, TimeSpan.FromSeconds(1));
    }
    
    private async void GenerateSensorData(object state)
    {
        var devices = new[] { "device-001", "device-002", "device-003", "device-004", "device-005" };
        var locations = new[] { "Building-A", "Building-B", "Building-C" };
        
        foreach (var deviceId in devices)
        {
            var telemetry = new TelemetryData
            {
                DeviceId = deviceId,
                EventType = "SensorReading",
                Timestamp = DateTime.UtcNow,
                Temperature = 20 + (_random.NextDouble() * 10), // 20-30°C
                Humidity = 40 + (_random.NextDouble() * 20),    // 40-60%
                Location = locations[_random.Next(locations.Length)],
                CustomProperties = new Dictionary<string, object>
                {
                    { "BatteryLevel", _random.Next(10, 100) },
                    { "SignalStrength", _random.Next(-80, -30) }
                }
            };
            
            await _producer.SendTelemetryAsync(telemetry);
        }
    }
    
    public void Stop()
    {
        _simulationTimer?.Dispose();
    }
}
                        </div>
                    </div>
                    <div class="substep">
                        <div class="substep-number">3</div>
                        <div class="substep-title">User Activity Event Producer</div>
                        <div class="code-block">
public class UserActivityProducer
{
    private readonly EventHubProducerClient _producer;
    
    public UserActivityProducer(string connectionString, string eventHubName)
    {
        _producer = new EventHubProducerClient(connectionString, eventHubName);
    }
    
    public async Task SendUserActivityAsync(UserActivity activity)
    {
        var eventData = new EventData(JsonSerializer.SerializeToUtf8Bytes(activity))
        {
            ContentType = "application/json",
            MessageId = Guid.NewGuid().ToString()
        };
        
        eventData.Properties["UserId"] = activity.UserId;
        eventData.Properties["ActivityType"] = activity.ActivityType;
        eventData.Properties["SessionId"] = activity.SessionId;
        
        var sendOptions = new SendEventOptions
        {
            PartitionKey = activity.UserId // Group events by user
        };
        
        await _producer.SendAsync(new[] { eventData }, sendOptions);
    }
}

public class UserActivity
{
    public string UserId { get; set; }
    public string SessionId { get; set; }
    public string ActivityType { get; set; }
    public DateTime Timestamp { get; set; }
    public string PageUrl { get; set; }
    public TimeSpan Duration { get; set; }
    public Dictionary<string, string> Properties { get; set; }
}
                        </div>
                    </div>
                </div>
            </div>

            <div class="step-section">
                <div class="step-header">
                    <div class="step-title">Step 3: Event Consumer Implementation</div>
                    <div class="step-description">Process events with checkpointing and scaling</div>
                </div>
                <div class="step-content">
                    <div class="substep">
                        <div class="substep-number">1</div>
                        <div class="substep-title">Event Processor with Checkpointing</div>
                        <div class="code-block">
using Azure.Messaging.EventHubs;
using Azure.Messaging.EventHubs.Consumer;
using Azure.Messaging.EventHubs.Processor;
using Azure.Storage.Blobs;

public class TelemetryEventProcessor
{
    private readonly EventProcessorClient _processor;
    private readonly ILogger<TelemetryEventProcessor> _logger;
    
    public TelemetryEventProcessor(string eventHubsConnectionString, string eventHubName,
                                  string storageConnectionString, string blobContainerName,
                                  ILogger<TelemetryEventProcessor> logger)
    {
        var blobClient = new BlobContainerClient(storageConnectionString, blobContainerName);
        
        _processor = new EventProcessorClient(blobClient, "telemetry-processor",
                                            eventHubsConnectionString, eventHubName);
        _logger = logger;
        
        _processor.ProcessEventAsync += ProcessEventHandler;
        _processor.ProcessErrorAsync += ProcessErrorHandler;
    }
    
    private async Task ProcessEventHandler(ProcessEventArgs eventArgs)
    {
        try
        {
            var telemetryJson = eventArgs.Data.EventBody.ToString();
            var telemetry = JsonSerializer.Deserialize<TelemetryData>(telemetryJson);
            
            _logger.LogInformation($"Processing event from device {telemetry.DeviceId} at {telemetry.Timestamp}");
            
            // Process the telemetry data
            await ProcessTelemetryData(telemetry, eventArgs.Data);
            
            // Update checkpoint every 10 events for better performance
            if (eventArgs.Data.SequenceNumber % 10 == 0)
            {
                await eventArgs.UpdateCheckpointAsync(eventArgs.CancellationToken);
                _logger.LogDebug($"Checkpoint updated for partition {eventArgs.Partition.PartitionId}");
            }
        }
        catch (Exception ex)
        {
            _logger.LogError(ex, "Error processing event");
            // Don't update checkpoint on error - event will be reprocessed
        }
    }
    
    private async Task ProcessTelemetryData(TelemetryData telemetry, EventData eventData)
    {
        // Example processing: detect anomalies
        if (telemetry.Temperature > 35 || telemetry.Temperature < 10)
        {
            _logger.LogWarning($"Temperature anomaly detected: {telemetry.Temperature}°C from device {telemetry.DeviceId}");
            await SendAlert(telemetry);
        }
        
        // Store in time-series database
        await StoreInTimeSeriesDB(telemetry);
        
        // Update real-time dashboard
        await UpdateDashboard(telemetry);
    }
    
    private async Task SendAlert(TelemetryData telemetry)
    {
        // Send to Service Bus or Event Grid for alerting
        _logger.LogInformation($"Alert sent for device {telemetry.DeviceId}");
        await Task.CompletedTask;
    }
    
    private async Task StoreInTimeSeriesDB(TelemetryData telemetry)
    {
        // Store in Azure Data Explorer, CosmosDB, or other time-series store
        _logger.LogDebug($"Stored telemetry for device {telemetry.DeviceId}");
        await Task.CompletedTask;
    }
    
    private async Task UpdateDashboard(TelemetryData telemetry)
    {
        // Update SignalR hub for real-time dashboard
        _logger.LogDebug($"Dashboard updated for device {telemetry.DeviceId}");
        await Task.CompletedTask;
    }
    
    private async Task ProcessErrorHandler(ProcessErrorEventArgs eventArgs)
    {
        _logger.LogError(eventArgs.Exception, "Event processing error on partition {PartitionId}",
                        eventArgs.PartitionId);
        await Task.CompletedTask;
    }
    
    public async Task StartProcessingAsync()
    {
        await _processor.StartProcessingAsync();
        _logger.LogInformation("Event processing started");
    }
    
    public async Task StopProcessingAsync()
    {
        await _processor.StopProcessingAsync();
        _logger.LogInformation("Event processing stopped");
    }
}
                        </div>
                    </div>
                    <div class="substep">
                        <div class="substep-number">2</div>
                        <div class="substep-title">Stream Analytics Integration</div>
                        <div class="code-block">
# Create Stream Analytics job
az stream-analytics job create \
  --name telemetry-analytics \
  --resource-group EventHubs-RG \
  --location eastus \
  --sku Standard

# Create input from Event Hub
az stream-analytics input create \
  --job-name telemetry-analytics \
  --resource-group EventHubs-RG \
  --name TelemetryInput \
  --type Stream \
  --datasource @input-config.json

# input-config.json
{
  "type": "Microsoft.ServiceBus/EventHub",
  "properties": {
    "eventHubNamespace": "enterprise-eventhubs-ns",
    "eventHubName": "telemetry-events",
    "consumerGroupName": "analytics",
    "authenticationMode": "ConnectionString",
    "sharedAccessPolicyName": "ListenPolicy"
  }
}

# Create output to SQL Database
az stream-analytics output create \
  --job-name telemetry-analytics \
  --resource-group EventHubs-RG \
  --name SqlOutput \
  --datasource @output-config.json

# Stream Analytics Query
SELECT 
    DeviceId,
    AVG(Temperature) as AvgTemperature,
    MAX(Temperature) as MaxTemperature,
    MIN(Temperature) as MinTemperature,
    COUNT(*) as EventCount,
    System.Timestamp() as WindowEnd
INTO SqlOutput
FROM TelemetryInput
TIMESTAMP BY Timestamp
GROUP BY DeviceId, TumblingWindow(minute, 5)

# Start the job
az stream-analytics job start \
  --name telemetry-analytics \
  --resource-group EventHubs-RG
                        </div>
                    </div>
                    <div class="substep">
                        <div class="substep-number">3</div>
                        <div class="substep-title">Consumer Groups and Scaling</div>
                        <div class="code-block">
# Create consumer groups for different processors
az eventhubs eventhub consumer-group create \
  --name analytics \
  --resource-group EventHubs-RG \
  --namespace-name enterprise-eventhubs-ns \
  --eventhub-name telemetry-events

az eventhubs eventhub consumer-group create \
  --name alerting \
  --resource-group EventHubs-RG \
  --namespace-name enterprise-eventhubs-ns \
  --eventhub-name telemetry-events

az eventhubs eventhub consumer-group create \
  --name archiving \
  --resource-group EventHubs-RG \
  --namespace-name enterprise-eventhubs-ns \
  --eventhub-name telemetry-events

# Multiple processor instances for scaling
public class ScaledEventProcessor
{
    public static async Task RunMultipleProcessors(int instanceCount)
    {
        var tasks = new List<Task>();
        
        for (int i = 0; i < instanceCount; i++)
        {
            var processor = new TelemetryEventProcessor(
                connectionString,
                eventHubName,
                storageConnectionString,
                blobContainerName,
                logger);
            
            tasks.Add(Task.Run(async () =>
            {
                await processor.StartProcessingAsync();
                
                // Keep running until cancelled
                await Task.Delay(Timeout.Infinite);
            }));
        }
        
        await Task.WhenAll(tasks);
    }
}
                        </div>
                    </div>
                </div>
            </div>

            <div class="validation-section">
                <h4>🔍 Validation Checklist</h4>
                <ul>
                    <li>✅ Event Hubs namespace created with appropriate throughput units</li>
                    <li>✅ Multiple event hubs configured with proper partition counts</li>
                    <li>✅ Event producers sending events with proper partitioning</li>
                    <li>✅ Event consumers processing with checkpointing</li>
                    <li>✅ Stream Analytics job processing real-time data</li>
                    <li>✅ Consumer groups isolating different processing workloads</li>
                    <li>✅ Capture feature archiving events to storage</li>
                    <li>✅ Monitoring and metrics showing throughput</li>
                </ul>
            </div>

            <div class="tips-section">
                <h4>💡 Pro Tips</h4>
                <ul>
                    <li><strong>Partition Strategy:</strong> Use consistent partition keys to maintain event order per entity</li>
                    <li><strong>Throughput Units:</strong> Monitor and adjust TUs based on actual throughput needs</li>
                    <li><strong>Checkpointing:</strong> Balance checkpoint frequency vs. processing performance</li>
                    <li><strong>Consumer Groups:</strong> Use separate consumer groups for different processing scenarios</li>
                    <li><strong>Error Handling:</strong> Implement exponential backoff for transient failures</li>
                </ul>
            </div>

            <div class="troubleshooting-section">
                <h4>🔧 Troubleshooting Guide</h4>
                <div class="troubleshoot-item">
                    <strong>Issue:</strong> Events backing up in partitions<br>
                    <strong>Solution:</strong> Scale out consumers, check processing logic performance, increase throughput units
                </div>
                <div class="troubleshoot-item">
                    <strong>Issue:</strong> Lost events after consumer restart<br>
                    <strong>Solution:</strong> Verify checkpoint storage configuration, ensure proper error handling
                </div>
                <div class="troubleshoot-item">
                    <strong>Issue:</strong> Uneven partition load<br>
                    <strong>Solution:</strong> Review partition key strategy, ensure even distribution
                </div>
            </div>

            <div class="resources-section">
                <h4>📚 Additional Resources</h4>
                <ul>
                    <li><a href="https://docs.microsoft.com/azure/event-hubs/" target="_blank">Event Hubs Documentation</a></li>
                    <li><a href="https://docs.microsoft.com/azure/event-hubs/event-hubs-scalability" target="_blank">Scaling and Performance</a></li>
                    <li><a href="https://docs.microsoft.com/azure/stream-analytics/" target="_blank">Stream Analytics Integration</a></li>
                </ul>
            </div>
        </div>

        <div id="solution19" class="solution-card">
            <div class="solution-header">
                <div class="solution-title">Assignment 19: Azure Cosmos DB</div>
                <div class="solution-meta">
                    <span class="meta-tag">🌍 NoSQL</span>
                    <span class="meta-tag">⏱️ 90 minutes</span>
                    <span class="meta-tag">📊 Advanced</span>
                </div>
            </div>
            
            <div class="time-estimate">
                <strong>⏱️ Estimated Time:</strong> 1 hour 30 minutes<br>
                <strong>🎯 Learning Objective:</strong> Build globally distributed NoSQL database with multi-region replication, automatic failover, and optimized performance
            </div>

            <div class="prerequisites">
                <h4>📋 Prerequisites</h4>
                <ul>
                    <li>Understanding of NoSQL database concepts and document storage</li>
                    <li>Familiarity with JSON data structures and querying</li>
                    <li>Basic knowledge of database partitioning and indexing</li>
                    <li>Understanding of consistency models and CAP theorem</li>
                    <li>Experience with REST APIs and SDKs (optional for advanced scenarios)</li>
                </ul>
            </div>

            <!-- Step 1 -->
            <div class="step-section">
                <div class="step-header">
                    <div class="step-title">Step 1: Creating Cosmos DB Account with Global Distribution</div>
                    <div class="step-description">Set up globally distributed NoSQL database account with multi-region configuration</div>
                </div>
                <div class="step-content">
                    <div class="substep">
                        <div class="substep-number">1</div>
                        <div class="substep-title">Create Cosmos DB Account</div>
                        <p>Set up the primary Cosmos DB account with global distribution:</p>
                        
                        <div class="code-block">
# Navigate to Azure Portal > Create a resource > Azure Cosmos DB

Basics:
  Subscription: (Your subscription)
  Resource Group: Database-RG (create new)
  Account Name: enterprise-cosmos-[random]
  API: Core (SQL) - NoSQL
  Location: East US (primary region)
  Capacity mode: Provisioned throughput
  Apply Free Tier Discount: Yes (if available)
  Limit total account throughput: 1000 RU/s

Global Distribution:
  Geo-Redundancy: Enable
  Multi-region Writes: Enable
  Availability Zones: Enable
                        </div>
                        
                        <div class="tip">
                            <strong>💡 Cost Optimization:</strong> Start with Free Tier if available. You can upgrade later without data migration.
                        </div>
                    </div>

                    <div class="substep">
                        <div class="substep-number">2</div>
                        <div class="substep-title">Configure Consistency Level</div>
                        <p>Select appropriate consistency model for your use case:</p>
                        
                        <div class="code-block">
Networking:
  Connectivity method: All networks
  
Backup Policy:
  Backup Policy: Periodic
  Backup Interval: 4 hours
  Backup Retention: 8 hours
  
Consistency:
  Default Consistency Level: Session
  # Options explanation:
  # - Strong: Highest consistency, highest latency
  # - Bounded Staleness: Configurable staleness window
  # - Session: Read your own writes consistency
  # - Consistent Prefix: Reads never see out-of-order writes
  # - Eventual: Lowest latency, eventual consistency
                        </div>
                        
                        <div class="warning">
                            <strong>⚠️ Consistency vs Performance:</strong> Stronger consistency levels increase latency. Session is recommended for most applications.
                        </div>
                    </div>

                    <div class="substep">
                        <div class="substep-number">3</div>
                        <div class="substep-title">Add Additional Regions</div>
                        <p>Configure multi-region distribution for high availability:</p>
                        
                        <div class="code-block">
# After account creation, go to Replicate data globally

Add regions:
  1. West US 2 (secondary region)
  2. Europe West (tertiary region)
  
Failover priorities:
  1. East US (Priority 1)
  2. West US 2 (Priority 2) 
  3. Europe West (Priority 3)
  
Automatic Failover: Enable
Service-Managed Failover: Enable

# Verify regions in Data Explorer
Region Status: All regions should show "Online"
Write Region: East US (primary)
Read Regions: All regions available
                        </div>
                        
                        <div class="success">
                            <strong>✅ Success Indicator:</strong> All regions show "Online" status and you can see read/write region distribution.
                        </div>
                    </div>
                </div>
            </div>

            <!-- Step 2 -->
            <div class="step-section">
                <div class="step-header">
                    <div class="step-title">Step 2: Creating Database and Container with Optimized Partitioning</div>
                    <div class="step-description">Design database schema with efficient partitioning and indexing strategies</div>
                </div>
                <div class="step-content">
                    <div class="substep">
                        <div class="substep-number">1</div>
                        <div class="substep-title">Create Database</div>
                        <p>Set up database with shared throughput:</p>
                        
                        <div class="code-block">
# In Data Explorer, click "New Database"

Database id: EnterpriseData
Provision database throughput: Yes
Database throughput (autoscale): 400-4000 RU/s
# Note: This allows sharing throughput across containers

# Alternative CLI command:
az cosmosdb sql database create \
  --account-name enterprise-cosmos-[random] \
  --resource-group Database-RG \
  --name EnterpriseData \
  --throughput 400
                        </div>
                    </div>

                    <div class="substep">
                        <div class="substep-number">2</div>
                        <div class="substep-title">Create Container with Partition Strategy</div>
                        <p>Design container with optimal partition key:</p>
                        
                        <div class="code-block">
# Create Users container
Container id: Users
Partition key: /userId
# Rationale: User ID provides good distribution and aligns with access patterns

Indexing Policy: Automatic
# Custom indexing for better performance:
{
  "indexingMode": "consistent",
  "automatic": true,
  "includedPaths": [
    {
      "path": "/*"
    }
  ],
  "excludedPaths": [
    {
      "path": "/\"_etag\"/?"
    },
    {
      "path": "/largeTextField/*"
    }
  ]
}

Unique Key Policy: 
# Add unique constraint on email
[
  {
    "paths": ["/email"]
  }
]
                        </div>
                        
                        <div class="tip">
                            <strong>💡 Partition Key Selection:</strong> Choose a partition key that distributes data evenly and aligns with your query patterns. Avoid hot partitions.
                        </div>
                    </div>

                    <div class="substep">
                        <div class="substep-number">3</div>
                        <div class="substep-title">Insert Sample Data</div>
                        <p>Add sample documents to test the container:</p>
                        
                        <div class="code-block">
# In Data Explorer > EnterpriseData > Users > Items
# Click "New Item" and add:

{
  "id": "user001",
  "userId": "user001",
  "firstName": "John",
  "lastName": "Doe",
  "email": "john.doe@enterprise.com",
  "department": "Engineering",
  "location": {
    "country": "USA",
    "city": "Seattle",
    "timezone": "PST"
  },
  "preferences": {
    "theme": "dark",
    "notifications": true,
    "language": "en-US"
  },
  "createdDate": "2024-01-15T10:30:00Z",
  "lastLoginDate": "2024-01-20T14:25:00Z",
  "isActive": true
}

# Add 4-5 more sample users with different userIds
# Verify data distribution across partitions
                        </div>
                    </div>
                </div>
            </div>

            <!-- Step 3 -->
            <div class="step-section">
                <div class="step-header">
                    <div class="step-title">Step 3: Implementing Advanced Querying and Performance Optimization</div>
                    <div class="step-description">Create efficient queries and optimize performance with indexing and throughput management</div>
                </div>
                <div class="step-content">
                    <div class="substep">
                        <div class="substep-number">1</div>
                        <div class="substep-title">SQL Query Development</div>
                        <p>Create and optimize SQL queries for Cosmos DB:</p>
                        
                        <div class="code-block">
# Basic queries in Data Explorer

# 1. Find active users
SELECT * FROM c WHERE c.isActive = true

# 2. Users by department with projection
SELECT c.firstName, c.lastName, c.email, c.department 
FROM c 
WHERE c.department = "Engineering"

# 3. Users in specific location
SELECT c.firstName, c.lastName, c.location.city 
FROM c 
WHERE c.location.country = "USA" AND c.location.city = "Seattle"

# 4. Complex query with JOIN (within document)
SELECT c.firstName, p.theme, p.language
FROM c 
JOIN p IN c.preferences
WHERE c.isActive = true

# 5. Aggregation query
SELECT c.department, COUNT(1) as userCount
FROM c 
WHERE c.isActive = true
GROUP BY c.department

# 6. Time-based query
SELECT * FROM c 
WHERE c.lastLoginDate >= "2024-01-01T00:00:00Z"
ORDER BY c.lastLoginDate DESC
                        </div>
                        
                        <div class="tip">
                            <strong>💡 Query Optimization:</strong> Use partition key in WHERE clauses when possible to avoid cross-partition queries.
                        </div>
                    </div>

                    <div class="substep">
                        <div class="substep-number">2</div>
                        <div class="substep-title">Monitor and Optimize Performance</div>
                        <p>Use metrics and insights to optimize performance:</p>
                        
                        <div class="code-block">
# In Azure Portal > Cosmos DB Account > Insights

Key Metrics to Monitor:
1. Request Units (RU) consumption
2. Throttling events (429 errors)
3. Latency (P99, P95)
4. Availability
5. Storage utilization

# Query Statistics in Data Explorer:
# After running queries, check:
- Request Charge (RU consumed)
- Query execution time
- Index hit ratio

# Optimize high RU queries:
1. Add WHERE clauses with partition key
2. Limit result set with TOP
3. Use composite indexes for complex queries
4. Avoid SELECT * in production

# Example optimized query:
SELECT TOP 10 c.firstName, c.lastName 
FROM c 
WHERE c.userId = "user001" AND c.isActive = true
                        </div>
                    </div>

                    <div class="substep">
                        <div class="substep-number">3</div>
                        <div class="substep-title">Configure Automatic Scaling</div>
                        <p>Set up autoscale for dynamic throughput management:</p>
                        
                        <div class="code-block">
# In Settings > Scale & Settings

Throughput Settings:
  Autoscale: Enable
  Maximum RU/s: 4000
  # Will scale from 400 to 4000 RU/s based on usage
  
Scaling Triggers:
  - High RU consumption (>70%)
  - Request rate throttling
  - Queue depth increases
  
# Create stored procedure for batch operations:
function bulkInsert(docs) {
    var collection = getContext().getCollection();
    var response = getContext().getResponse();
    var count = 0;
    
    if (!docs || docs.length === 0) {
        response.setBody(0);
        return;
    }
    
    var docsLength = docs.length;
    
    for (var i = 0; i < docsLength; i++) {
        var accepted = collection.createDocument(
            collection.getSelfLink(),
            docs[i],
            function(err, doc) {
                if (err) throw new Error('Error: ' + err.message);
                count++;
                if (count === docsLength) {
                    response.setBody(count);
                }
            }
        );
        
        if (!accepted) break;
    }
}
                        </div>
                        
                        <div class="warning">
                            <strong>⚠️ Cost Management:</strong> Monitor autoscale settings to avoid unexpected costs. Set up budget alerts.
                        </div>
                    </div>
                </div>
            </div>

            <!-- Step 4 -->
            <div class="step-section">
                <div class="step-header">
                    <div class="step-title">Step 4: Testing Global Distribution and Failover</div>
                    <div class="step-description">Validate multi-region setup and test automatic failover capabilities</div>
                </div>
                <div class="step-content">
                    <div class="substep">
                        <div class="substep-number">1</div>
                        <div class="substep-title">Test Regional Access</div>
                        <p>Verify data accessibility from different regions:</p>
                        
                        <div class="code-block">
# Connection strings for different regions:

# Primary (East US):
AccountEndpoint=https://enterprise-cosmos-[random].documents.azure.com:443/

# Test read from different regions using SDK:
# (Example in .NET)
CosmosClientOptions options = new CosmosClientOptions()
{
    ApplicationRegion = Regions.WestUS2,
    ConsistencyLevel = ConsistencyLevel.Session
};

# Verify read preference:
1. Connect from East US - should read from East US
2. Connect from West Coast - should read from West US 2
3. Connect from Europe - should read from Europe West

# Check in Portal > Replicate data globally:
- Observe read and write region distribution
- Monitor request routing
- Check latency from different regions
                        </div>
                    </div>

                    <div class="substep">
                        <div class="substep-number">2</div>
                        <div class="substep-title">Test Manual Failover</div>
                        <p>Simulate failover scenarios:</p>
                        
                        <div class="code-block">
# Manual Failover Test:
1. Go to "Replicate data globally" > "Manual Failover"
2. Select West US 2 as new write region
3. Confirm failover (takes 1-2 minutes)
4. Verify:
   - Write region changed to West US 2
   - East US becomes read-only
   - Applications continue working
   - Data consistency maintained

# Automatic Failover Test:
1. Enable "Automatic Failover" in settings
2. Monitor failover scenarios in metrics
3. Check Service Health for region outages

# Failover Priority Configuration:
Region Priority Order:
1. East US (default)
2. West US 2 
3. Europe West

# Recovery after failover:
1. Monitor when primary region recovers
2. Consider manual failback if needed
3. Update connection strings if using region-specific endpoints
                        </div>
                        
                        <div class="success">
                            <strong>✅ Success Indicator:</strong> Failover completes without data loss and applications continue functioning.
                        </div>
                    </div>

                    <div class="substep">
                        <div class="substep-number">3</div>
                        <div class="substep-title">Validate Conflict Resolution</div>
                        <p>Test conflict resolution for multi-region writes:</p>
                        
                        <div class="code-block">
# Conflict Resolution Policies:

# 1. Last Writer Wins (default)
# Updates document with highest timestamp

# 2. Custom Policy (JavaScript function)
function resolver(incomingRecord, existingRecord, isTombstone, conflictingRecords) {
    if (incomingRecord.priority > existingRecord.priority) {
        return incomingRecord;
    } else {
        return existingRecord;
    }
}

# 3. Manual Resolution
# Store conflicts for manual review

# Test Scenario:
1. Update same document from two regions simultaneously
2. Verify conflict resolution
3. Check conflict feed if using manual resolution
4. Monitor metrics for conflict rates

# Check conflicts in Data Explorer:
# Look for _conflicts container (auto-created)
# Review conflicted documents
# Understand resolution outcomes
                        </div>
                    </div>
                </div>
            </div>

            <!-- Validation Section -->
            <div class="validation-section">
                <h4>🔍 Validation & Testing</h4>
                <ol>
                    <li><strong>Multi-Region Setup:</strong> Verify all regions are online and data is replicated</li>
                    <li><strong>Query Performance:</strong> Test SQL queries and monitor RU consumption</li>
                    <li><strong>Partition Distribution:</strong> Confirm data is evenly distributed across partitions</li>
                    <li><strong>Consistency Levels:</strong> Validate chosen consistency model works for your use case</li>
                    <li><strong>Failover Testing:</strong> Verify manual and automatic failover scenarios</li>
                    <li><strong>Global Access:</strong> Test read/write operations from different geographic regions</li>
                    <li><strong>Performance Monitoring:</strong> Ensure latency and throughput meet requirements</li>
                    <li><strong>Cost Optimization:</strong> Verify autoscale settings and monitor RU consumption</li>
                </ol>
            </div>

            <div class="troubleshooting">
                <h4>🛠️ Common Issues & Solutions</h4>
                <ul>
                    <li><strong>High RU consumption:</strong> Optimize queries, use partition key in WHERE clauses, implement pagination</li>
                    <li><strong>429 Throttling errors:</strong> Increase throughput, implement retry logic, optimize query patterns</li>
                    <li><strong>Hot partitions:</strong> Review partition key strategy, redistribute data, use synthetic partition keys</li>
                    <li><strong>Long query times:</strong> Add appropriate indexes, limit result sets, use projections</li>
                    <li><strong>Failover issues:</strong> Check region health, verify automatic failover settings, review priority order</li>
                    <li><strong>Connection timeouts:</strong> Optimize connection settings, implement connection pooling, check network latency</li>
                    <li><strong>Data consistency issues:</strong> Review consistency level choice, understand eventual consistency trade-offs</li>
                </ul>
            </div>

            <div class="resource-links">
                <h4>📚 Additional Resources</h4>
                <ul>
                    <li><a href="https://docs.microsoft.com/en-us/azure/cosmos-db/" target="_blank">Azure Cosmos DB Documentation</a></li>
                    <li><a href="https://docs.microsoft.com/en-us/azure/cosmos-db/sql/sql-query-getting-started" target="_blank">SQL Query Language Reference</a></li>
                    <li><a href="https://docs.microsoft.com/en-us/azure/cosmos-db/partitioning-overview" target="_blank">Partitioning and Scale Strategies</a></li>
                    <li><a href="https://docs.microsoft.com/en-us/azure/cosmos-db/consistency-levels" target="_blank">Consistency Levels Guide</a></li>
                    <li><a href="https://docs.microsoft.com/en-us/azure/cosmos-db/high-availability" target="_blank">High Availability and Global Distribution</a></li>
                    <li><a href="https://docs.microsoft.com/en-us/azure/cosmos-db/monitor-cosmos-db" target="_blank">Monitoring and Performance Optimization</a></li>
                </ul>
            </div>
        </div>

        <div id="solution20" class="solution-card">
            <div class="solution-header">
                <div class="solution-title">Assignment 20: Azure Data Factory</div>
                <div class="solution-meta">
                    <span class="meta-tag">🏭 ETL</span>
                    <span class="meta-tag">⏱️ 100 minutes</span>
                    <span class="meta-tag">📊 Advanced</span>
                </div>
            </div>
            
            <div class="time-estimate">
                <strong>⏱️ Estimated Time:</strong> 1 hour 40 minutes<br>
                <strong>🎯 Learning Objective:</strong> Build enterprise-grade ETL pipelines with data integration, transformation, monitoring, and scheduling
            </div>

            <div class="prerequisites">
                <h4>📋 Prerequisites</h4>
                <ul>
                    <li>Understanding of ETL (Extract, Transform, Load) concepts and data integration patterns</li>
                    <li>Familiarity with SQL databases and data lake storage concepts</li>
                    <li>Completed Assignment 2 (Storage Account) for data lake integration</li>
                    <li>Basic knowledge of data transformation and pipeline orchestration</li>
                    <li>Understanding of JSON and Azure Resource Manager (ARM) templates (helpful for advanced scenarios)</li>
                </ul>
            </div>

            <!-- Step 1 -->
            <div class="step-section">
                <div class="step-header">
                    <div class="step-title">Step 1: Setting Up Data Factory and Source/Destination Services</div>
                    <div class="step-description">Create data factory instance and configure supporting data services</div>
                </div>
                <div class="step-content">
                    <div class="substep">
                        <div class="substep-number">1</div>
                        <div class="substep-title">Create Azure Data Factory</div>
                        <p>Set up the core data integration service:</p>
                        
                        <div class="code-block">
# Navigate to Azure Portal > Create a resource > Data Factory

Basics:
  Subscription: (Your subscription)
  Resource Group: DataProcessing-RG (create new)
  Name: enterprise-datafactory-[random]
  Region: East US
  Version: V2

Git Configuration (Optional but recommended):
  Configure Git later: Yes
  # After creation, can connect to:
  # - Azure DevOps
  # - GitHub
  # For version control of pipelines

Tags:
  Environment: Development
  Project: DataIntegration
  Owner: DataTeam
                        </div>
                        
                        <div class="tip">
                            <strong>💡 Naming Convention:</strong> Use descriptive names that include environment and purpose. This helps with governance and cost tracking.
                        </div>
                    </div>

                    <div class="substep">
                        <div class="substep-number">2</div>
                        <div class="substep-title">Create Source SQL Database</div>
                        <p>Set up source database with sample data:</p>
                        
                        <div class="code-block">
# Create Azure SQL Database for source data
# Navigate to Create a resource > SQL Database

Basics:
  Database name: SourceDB
  Server: Create new
    Server name: enterprise-sql-server-[random]
    Server admin login: sqladmin
    Password: ComplexPassword123!
    Location: East US
    Allow Azure services to access server: Yes
  
  Compute + storage: Basic (5 DTU)
  Backup storage redundancy: Locally-redundant

# After creation, add sample data via Query editor:
# Connect with SQL authentication

CREATE TABLE dbo.Employees (
    EmployeeID int IDENTITY(1,1) PRIMARY KEY,
    FirstName nvarchar(50) NOT NULL,
    LastName nvarchar(50) NOT NULL,
    Email nvarchar(100) NOT NULL,
    Department nvarchar(50) NOT NULL,
    Salary decimal(10,2) NOT NULL,
    HireDate datetime2 NOT NULL,
    IsActive bit NOT NULL DEFAULT 1,
    CreatedDate datetime2 DEFAULT GETDATE()
);

# Insert sample data:
INSERT INTO dbo.Employees (FirstName, LastName, Email, Department, Salary, HireDate) VALUES
('John', 'Smith', 'john.smith@company.com', 'Engineering', 75000, '2023-01-15'),
('Sarah', 'Johnson', 'sarah.j@company.com', 'Marketing', 65000, '2023-02-01'),
('Mike', 'Brown', 'mike.brown@company.com', 'Sales', 70000, '2023-01-10'),
('Lisa', 'Davis', 'lisa.davis@company.com', 'HR', 60000, '2023-03-01'),
('Tom', 'Wilson', 'tom.wilson@company.com', 'Engineering', 80000, '2023-01-20');
                        </div>
                    </div>

                    <div class="substep">
                        <div class="substep-number">3</div>
                        <div class="substep-title">Create Destination Data Lake Storage</div>
                        <p>Set up data lake for transformed data storage:</p>
                        
                        <div class="code-block">
# Create Azure Data Lake Storage Gen2
# Navigate to Create a resource > Storage account

Basics:
  Storage account name: enterprisedatalake[random]
  Region: East US
  Performance: Standard
  Redundancy: Locally-redundant storage (LRS)

Advanced:
  Hierarchical namespace: Enabled (this makes it Data Lake Gen2)
  Access tier: Hot
  
# After creation, create containers:
1. Go to Storage Explorer (preview)
2. Expand Blob Containers
3. Create containers:
   - raw-data (for source data)
   - processed-data (for transformed data)
   - archive-data (for historical data)

# Set up folder structure in raw-data container:
/raw-data/
  /employees/
    /year=2024/
      /month=01/
  /departments/
  /staging/

# In processed-data container:
/processed-data/
  /employees/
    /year=2024/
      /month=01/
  /aggregated/
  /reports/
                        </div>
                        
                        <div class="warning">
                            <strong>⚠️ Security Note:</strong> Enable firewall rules and consider private endpoints for production scenarios.
                        </div>
                    </div>
                </div>
            </div>

            <!-- Step 2 -->
            <div class="step-section">
                <div class="step-header">
                    <div class="step-title">Step 2: Creating Linked Services and Datasets</div>
                    <div class="step-description">Configure connections to data sources and destinations</div>
                </div>
                <div class="step-content">
                    <div class="substep">
                        <div class="substep-number">1</div>
                        <div class="substep-title">Create SQL Database Linked Service</div>
                        <p>Configure connection to source SQL database:</p>
                        
                        <div class="code-block">
# Open Data Factory Studio
# Click "Author" (pencil icon) > Linked services > + New

Linked Service Type: Azure SQL Database

Configuration:
  Name: SourceSQLDatabase
  Description: Connection to source employee database
  
Connection:
  Azure subscription: (Your subscription)
  Server name: enterprise-sql-server-[random].database.windows.net
  Database name: SourceDB
  Authentication type: SQL authentication
  User name: sqladmin
  Password: ComplexPassword123!
  
Advanced:
  Annotations: source, sql, employees
  
# Test connection before saving
# Should show "Connection successful"
                        </div>
                    </div>

                    <div class="substep">
                        <div class="substep-number">2</div>
                        <div class="substep-title">Create Data Lake Linked Service</div>
                        <p>Configure connection to destination data lake:</p>
                        
                        <div class="code-block">
# Create new linked service
Linked Service Type: Azure Data Lake Storage Gen2

Configuration:
  Name: EnterpriseDataLake
  Description: Data lake for processed data storage
  
Connection:
  Azure subscription: (Your subscription)
  Storage account name: enterprisedatalake[random]
  Authentication type: Account key
  # Alternative: Use Managed Identity for better security
  
Advanced:
  Annotations: destination, datalake, storage
  
# Test connection
# Verify access to containers
                        </div>
                        
                        <div class="tip">
                            <strong>💡 Security Best Practice:</strong> Use Managed Identity instead of account keys for production environments.
                        </div>
                    </div>

                    <div class="substep">
                        <div class="substep-number">3</div>
                        <div class="substep-title">Create Datasets</div>
                        <p>Define data structures for source and destination:</p>
                        
                        <div class="code-block">
# Create Source Dataset
# Click Datasets > + New > Azure SQL Database

Dataset Configuration:
  Name: EmployeesSourceDataset
  Linked service: SourceSQLDatabase
  Table name: dbo.Employees
  
Schema:
  Import schema: From connection/store
  # This will automatically detect the table structure

# Create Destination Dataset  
# Click Datasets > + New > Azure Data Lake Storage Gen2

Dataset Configuration:
  Name: EmployeesDestinationDataset
  Linked service: EnterpriseDataLake
  File path: 
    Container: processed-data
    Directory: employees/year=2024/month=01
    File name: employees.parquet
  File format: Parquet
  
# Create intermediate dataset for raw data
Dataset Configuration:
  Name: EmployeesRawDataset
  Linked service: EnterpriseDataLake
  File path:
    Container: raw-data
    Directory: employees/staging
    File name: employees_raw.csv
  File format: DelimitedText
  Column delimiter: Comma
  Row delimiter: Line feed
  First row as header: True
                        </div>
                    </div>
                </div>
            </div>

            <!-- Step 3 -->
            <div class="step-section">
                <div class="step-header">
                    <div class="step-title">Step 3: Building ETL Pipeline with Data Transformations</div>
                    <div class="step-description">Create comprehensive data pipeline with multiple transformation activities</div>
                </div>
                <div class="step-content">
                    <div class="substep">
                        <div class="substep-number">1</div>
                        <div class="substep-title">Create Main ETL Pipeline</div>
                        <p>Build pipeline with copy and transformation activities:</p>
                        
                        <div class="code-block">
# Click Pipelines > + New Pipeline

Pipeline Configuration:
  Name: Employee-ETL-Pipeline
  Description: Extract employees from SQL, transform, and load to data lake
  
# Add Activities (drag from Activities pane):

1. Copy Data Activity:
   Name: CopyEmployeesToRaw
   Source: EmployeesSourceDataset
   Sink: EmployeesRawDataset
   
   Source Settings:
     Use query: Query
     Query: 
       SELECT EmployeeID, FirstName, LastName, Email, 
              Department, Salary, HireDate, IsActive,
              FORMAT(CreatedDate, 'yyyy-MM-dd') as CreatedDate
       FROM dbo.Employees 
       WHERE IsActive = 1
   
   Sink Settings:
     Copy behavior: None
     Max concurrent connections: 5
   
   Settings:
     Enable staging: No
     Data integration units: Auto
     Degree of copy parallelism: Auto

2. Data Flow Activity:
   Name: TransformEmployeeData
   Data flow: Create new data flow
                        </div>
                    </div>

                    <div class="substep">
                        <div class="substep-number">2</div>
                        <div class="substep-title">Create Data Flow for Transformations</div>
                        <p>Design data transformation logic:</p>
                        
                        <div class="code-block">
# Create new Data Flow
Name: EmployeeTransformationFlow

# Add Transformations:

1. Source:
   Name: SourceEmployees
   Dataset: EmployeesRawDataset
   
2. Derived Column:
   Name: AddCalculatedFields
   Columns to add:
   - FullName: FirstName + " " + LastName
   - SalaryGrade: case(Salary < 60000, "Junior", 
                      Salary < 75000, "Mid", 
                      "Senior")
   - YearsOfService: datediff(HireDate, currentDate(), "year")
   - ProcessedDate: currentTimestamp()

3. Filter:
   Name: FilterActivePast6Months
   Filter condition: 
     IsActive == true && 
     datediff(HireDate, currentDate(), "month") >= 6

4. Aggregate:
   Name: DepartmentSummary
   Group by: Department
   Aggregates:
   - EmployeeCount: count()
   - AvgSalary: avg(Salary)
   - MaxSalary: max(Salary)
   - MinSalary: min(Salary)

5. Sink (for detailed data):
   Name: DetailedEmployeeSink
   Dataset: EmployeesDestinationDataset
   Settings:
     File name option: Output to single file
     Output to single file: employees_detailed.parquet

6. Sink (for aggregated data):
   Name: DepartmentSummarySink
   Dataset: Create new dataset
   File path: processed-data/aggregated/department_summary.parquet
                        </div>
                        
                        <div class="tip">
                            <strong>💡 Performance Tip:</strong> Use appropriate partition strategies and optimize data flow compute for large datasets.
                        </div>
                    </div>

                    <div class="substep">
                        <div class="substep-number">3</div>
                        <div class="substep-title">Add Error Handling and Notifications</div>
                        <p>Implement robust error handling and monitoring:</p>
                        
                        <div class="code-block">
# Add activities to pipeline:

3. Email Notification (on success):
   Activity Type: Web Activity
   Name: SendSuccessNotification
   
   Settings:
     URL: https://prod-[region].westus.logic.azure.com/workflows/[id]/triggers/manual/paths/invoke
     Method: POST
     Headers:
       Content-Type: application/json
     Body:
     {
       "subject": "ETL Pipeline Success",
       "message": "Employee ETL pipeline completed successfully",
       "pipelineName": "@pipeline().Pipeline",
       "runId": "@pipeline().RunId",
       "status": "Success"
     }

4. Failure Notification:
   Activity Type: Web Activity  
   Name: SendFailureNotification
   
   # Configure as failure path from main activities
   
# Add Variables:
Variables:
  - RowCount (Integer): Store processed record count
  - ProcessingDate (String): Store processing timestamp
  - ErrorMessage (String): Store error details if any

# Set Variable Activities:
1. Set RowCount:
   Variable name: RowCount
   Value: @activity('CopyEmployeesToRaw').output.rowsCopied

2. Set ProcessingDate:
   Variable name: ProcessingDate  
   Value: @utcnow()

# Configure Activity Dependencies:
CopyEmployeesToRaw → TransformEmployeeData (On Success)
CopyEmployeesToRaw → SendFailureNotification (On Failure)
TransformEmployeeData → SendSuccessNotification (On Success)
TransformEmployeeData → SendFailureNotification (On Failure)
                        </div>
                    </div>
                </div>
            </div>

            <!-- Step 4 -->
            <div class="step-section">
                <div class="step-header">
                    <div class="step-title">Step 4: Implementing Scheduling and Monitoring</div>
                    <div class="step-description">Set up automated scheduling, monitoring, and alerting for the ETL pipeline</div>
                </div>
                <div class="step-content">
                    <div class="substep">
                        <div class="substep-number">1</div>
                        <div class="substep-title">Create Pipeline Trigger</div>
                        <p>Set up automated scheduling for the ETL pipeline:</p>
                        
                        <div class="code-block">
# In Pipeline view, click "Add trigger" > New/Edit

Schedule Trigger Configuration:
  Name: DailyETLSchedule
  Description: Run employee ETL daily at 2 AM
  Type: Schedule
  
Schedule:
  Start date: Today's date
  Time zone: (UTC) Coordinated Universal Time
  Recurrence: Daily
  Start time: 02:00:00
  
Advanced scheduling:
  Specify an end date: No (continuous)
  
# Alternative: Tumbling Window Trigger
Name: HourlyDataRefresh
Type: Tumbling window
Window size: 1 hour
Start time: Current time
Max concurrency: 1
Retry policy: 3 retries with 15-minute intervals

# Event-based Trigger (for real-time processing):
Name: FileArrivalTrigger
Type: Storage event
Storage account: enterprisedatalake[random]
Container name: raw-data
Folder path: employees/staging/
Events: Blob created
                        </div>
                    </div>

                    <div class="substep">
                        <div class="substep-number">2</div>
                        <div class="substep-title">Configure Monitoring and Alerts</div>
                        <p>Set up comprehensive monitoring for pipeline health:</p>
                        
                        <div class="code-block">
# Navigate to Monitor tab in Data Factory

Pipeline Monitoring Setup:
1. Go to Azure Portal > Data Factory > Monitoring
2. Set up alerts:
   
Alert Rules:
  Alert 1: Pipeline Failure
    Signal: Pipeline runs
    Condition: Status equals Failed
    Action group: Email data team
    
  Alert 2: Long Running Pipeline  
    Signal: Pipeline runs
    Condition: Duration greater than 30 minutes
    Action group: Email + SMS
    
  Alert 3: High Resource Usage
    Signal: Integration Runtime CPU
    Condition: Average CPU > 80%
    Time window: 15 minutes

# Custom Metrics Dashboard:
Create Log Analytics Workspace:
  Name: DataFactoryLogs
  Resource Group: DataProcessing-RG
  
Configure Diagnostic Settings:
  Pipeline runs: Enable
  Activity runs: Enable  
  Trigger runs: Enable
  Send to Log Analytics workspace: DataFactoryLogs

# Sample KQL Queries for monitoring:
// Failed pipeline runs
ADFPipelineRun
| where Status == "Failed" 
| where TimeGenerated > ago(24h)
| project TimeGenerated, PipelineName, RunId, ErrorMessage

// Long running activities  
ADFActivityRun
| where Status == "Succeeded"
| where DurationMs > 300000  // 5 minutes
| project TimeGenerated, ActivityName, PipelineName, DurationMs
                        </div>
                        
                        <div class="warning">
                            <strong>⚠️ Cost Monitoring:</strong> Set up budget alerts to monitor Data Factory costs, especially for frequent pipeline runs.
                        </div>
                    </div>

                    <div class="substep">
                        <div class="substep-number">3</div>
                        <div class="substep-title">Test Pipeline Execution</div>
                        <p>Validate end-to-end pipeline functionality:</p>
                        
                        <div class="code-block">
# Manual Pipeline Testing:

1. Debug Run:
   - Click "Debug" in pipeline designer
   - Monitor execution in real-time
   - Check activity outputs and duration
   - Verify data quality in destination

2. Trigger Run:
   - Click "Add trigger" > "Trigger now"
   - Provide parameters if configured
   - Monitor in pipeline runs view

3. Validation Checks:
   # Check source data count
   SELECT COUNT(*) FROM dbo.Employees WHERE IsActive = 1;
   
   # Verify files in data lake
   # Navigate to Storage Explorer:
   # - raw-data/employees/staging/ (should have CSV)
   # - processed-data/employees/ (should have detailed parquet)
   # - processed-data/aggregated/ (should have summary parquet)

4. Data Quality Validation:
   # Download and check parquet files
   # Verify transformations:
   - FullName = FirstName + LastName
   - SalaryGrade calculated correctly
   - YearsOfService calculated from HireDate
   - Department aggregations accurate

5. Performance Testing:
   # Monitor metrics:
   - Data Integration Units (DIU) usage
   - Pipeline duration
   - Copy throughput
   - Data flow cluster startup time

# Troubleshooting Common Issues:
# 1. Connection failures: Check firewall rules, credentials
# 2. Transformation errors: Validate data types, null handling
# 3. Performance issues: Optimize DIU, parallel activities
# 4. Permission errors: Check RBAC roles, storage access
                        </div>
                        
                        <div class="success">
                            <strong>✅ Success Indicators:</strong> Pipeline completes successfully, data appears in destination with correct transformations, and monitoring shows healthy execution metrics.
                        </div>
                    </div>
                </div>
            </div>

            <!-- Validation Section -->
            <div class="validation-section">
                <h4>🔍 Validation & Testing</h4>
                <ol>
                    <li><strong>Data Factory Created:</strong> Verify ADF instance is created and accessible via Data Factory Studio</li>
                    <li><strong>Linked Services Connected:</strong> Test connections to SQL Database and Data Lake Storage</li>
                    <li><strong>Datasets Defined:</strong> Confirm source and destination datasets with correct schemas</li>
                    <li><strong>Pipeline Execution:</strong> Successfully run pipeline and verify data movement</li>
                    <li><strong>Data Transformations:</strong> Validate calculated fields and aggregations are correct</li>
                    <li><strong>Scheduling Active:</strong> Confirm triggers are created and pipeline runs on schedule</li>
                    <li><strong>Monitoring Configured:</strong> Verify alerts and monitoring dashboards are working</li>
                    <li><strong>Error Handling:</strong> Test failure scenarios and notification systems</li>
                    <li><strong>Performance Optimized:</strong> Check pipeline performance meets requirements</li>
                </ol>
            </div>

            <div class="troubleshooting">
                <h4>🛠️ Common Issues & Solutions</h4>
                <ul>
                    <li><strong>Connection timeouts:</strong> Increase timeout settings, check network connectivity, verify firewall rules</li>
                    <li><strong>Authentication failures:</strong> Verify credentials, check managed identity permissions, update connection strings</li>
                    <li><strong>Data type mismatches:</strong> Review schema mapping, add explicit data type conversions in data flow</li>
                    <li><strong>Performance bottlenecks:</strong> Optimize DIU settings, use parallel processing, partition large datasets</li>
                    <li><strong>Memory errors in data flow:</strong> Increase cluster size, optimize transformations, process data in chunks</li>
                    <li><strong>Trigger not firing:</strong> Check trigger configuration, verify schedule time zones, ensure triggers are published</li>
                    <li><strong>Incomplete data transfers:</strong> Check source query conditions, verify copy activity settings, monitor for throttling</li>
                    <li><strong>High costs:</strong> Monitor DIU usage, optimize pipeline frequency, use auto-scale integration runtime</li>
                </ul>
            </div>

            <div class="resource-links">
                <h4>📚 Additional Resources</h4>
                <ul>
                    <li><a href="https://docs.microsoft.com/en-us/azure/data-factory/" target="_blank">Azure Data Factory Documentation</a></li>
                    <li><a href="https://docs.microsoft.com/en-us/azure/data-factory/concepts-pipelines-activities" target="_blank">Pipelines and Activities Guide</a></li>
                    <li><a href="https://docs.microsoft.com/en-us/azure/data-factory/concepts-data-flow-overview" target="_blank">Mapping Data Flows Overview</a></li>
                    <li><a href="https://docs.microsoft.com/en-us/azure/data-factory/monitor-visually" target="_blank">Visual Monitoring Guide</a></li>
                    <li><a href="https://docs.microsoft.com/en-us/azure/data-factory/concepts-integration-runtime" target="_blank">Integration Runtime Configuration</a></li>
                    <li><a href="https://docs.microsoft.com/en-us/azure/data-factory/concepts-linked-services" target="_blank">Linked Services and Datasets</a></li>
                    <li><a href="https://docs.microsoft.com/en-us/azure/data-factory/how-to-create-schedule-trigger" target="_blank">Scheduling and Triggers</a></li>
                </ul>
            </div>
        </div>

        <div id="solution21" class="solution-card">
            <div class="solution-header">
                <div class="solution-title">Assignment 21: Azure Synapse Analytics</div>
                <div class="solution-meta">
                    <span class="meta-tag">📊 Analytics</span>
                    <span class="meta-tag">⏱️ 110 minutes</span>
                    <span class="meta-tag">📊 Expert</span>
                </div>
            </div>
            
            <div class="time-estimate">
                <strong>⏱️ Estimated Time:</strong> 1 hour 50 minutes<br>
                <strong>🎯 Learning Objective:</strong> Build comprehensive big data analytics platform with unified data integration, warehousing, and advanced analytics capabilities
            </div>

            <div class="prerequisites">
                <h4>📋 Prerequisites</h4>
                <ul>
                    <li>Understanding of data warehousing concepts and OLAP vs OLTP</li>
                    <li>Familiarity with SQL, Apache Spark, and big data processing concepts</li>
                    <li>Completed Assignment 2 (Storage Account) for data lake integration</li>
                    <li>Basic knowledge of ETL/ELT processes and data modeling</li>
                    <li>Understanding of analytical workloads and query optimization</li>
                </ul>
            </div>

            <!-- Step 1 -->
            <div class="step-section">
                <div class="step-header">
                    <div class="step-title">Step 1: Creating Synapse Workspace and Data Lake Integration</div>
                    <div class="step-description">Set up unified analytics platform with integrated data lake storage</div>
                </div>
                <div class="step-content">
                    <div class="substep">
                        <div class="substep-number">1</div>
                        <div class="substep-title">Create Synapse Workspace</div>
                        <p>Set up the core analytics workspace:</p>
                        
                        <div class="code-block">
# Navigate to Azure Portal > Create a resource > Azure Synapse Analytics

Basics:
  Subscription: (Your subscription)
  Resource Group: Analytics-RG (create new)
  Workspace name: enterprise-synapse-[random]
  Region: East US
  Select Data Lake Storage Gen2:
    Account name: synapsestore[random] (create new)
    File system name: synapse-data
  
SQL administrator credentials:
  SQL admin username: sqladmin
  Password: ComplexP@ssw0rd123!
  
Security:
  Allow connections from all IP addresses: Yes (for development)
  # In production, configure specific IP ranges or private endpoints
  
Advanced:
  Managed Virtual Network: Enable
  Data exfiltration protection: Disable (for demo)
  
Tags:
  Environment: Development
  Project: DataAnalytics
  Owner: DataTeam
                        </div>
                        
                        <div class="tip">
                            <strong>💡 Storage Integration:</strong> Synapse automatically creates and integrates with Data Lake Storage Gen2 for seamless data access.
                        </div>
                    </div>

                    <div class="substep">
                        <div class="substep-number">2</div>
                        <div class="substep-title">Configure Workspace Security</div>
                        <p>Set up proper access controls and security:</p>
                        
                        <div class="code-block">
# After workspace creation, configure security settings:

# 1. Access Control (IAM):
# Navigate to Synapse Workspace > Access control (IAM)
# Add role assignments:

Role Assignments:
  - Synapse Administrator: Your user account
  - Synapse Contributor: Data team members
  - Synapse SQL Administrator: SQL admin team
  - Storage Blob Data Contributor: Synapse workspace (for data lake access)

# 2. Synapse Studio RBAC:
# Go to Synapse Studio > Manage > Security > Access control
# Assign workspace roles:

Workspace Roles:
  - Synapse Administrator: Full workspace access
  - Synapse Contributor: Can run code, read data
  - Synapse Artifact Publisher: Can publish code artifacts
  - Synapse Artifact User: Can use published artifacts

# 3. Configure Managed Identity:
# Synapse workspace automatically gets a managed identity
# Grant this identity access to other Azure resources:
# - Storage Account: Storage Blob Data Contributor
# - Key Vault: Get/List permissions for secrets
                        </div>
                    </div>

                    <div class="substep">
                        <div class="substep-number">3</div>
                        <div class="substep-title">Upload Sample Data</div>
                        <p>Prepare sample datasets for analytics:</p>
                        
                        <div class="code-block">
# Create folder structure in Data Lake:
# Navigate to Storage Account > Containers > synapse-data

Folder Structure:
/synapse-data/
  /raw/
    /sales/
      /2024/
        /01/
    /customers/
    /products/
  /processed/
    /aggregated/
    /cleansed/
  /external/
    /reference-data/

# Upload sample CSV files:
# 1. Sales data (sales.csv):
OrderID,CustomerID,ProductID,Quantity,UnitPrice,OrderDate,Region
1001,C001,P001,2,25.99,2024-01-15,East
1002,C002,P002,1,15.50,2024-01-15,West
1003,C001,P003,3,35.00,2024-01-16,East
1004,C003,P001,1,25.99,2024-01-16,South
1005,C004,P004,2,45.00,2024-01-17,North

# 2. Customer data (customers.csv):
CustomerID,FirstName,LastName,Email,City,State,Country
C001,John,Smith,john.smith@email.com,New York,NY,USA
C002,Sarah,Johnson,sarah.j@email.com,Los Angeles,CA,USA
C003,Mike,Brown,mike.brown@email.com,Miami,FL,USA
C004,Lisa,Davis,lisa.davis@email.com,Chicago,IL,USA

# 3. Product data (products.csv):
ProductID,ProductName,Category,Price,Supplier
P001,Laptop,Electronics,25.99,TechCorp
P002,Mouse,Electronics,15.50,TechCorp
P003,Keyboard,Electronics,35.00,InputDevices
P004,Monitor,Electronics,45.00,DisplayTech

# Upload files to respective folders:
# - Upload to /raw/sales/2024/01/sales.csv
# - Upload to /raw/customers/customers.csv
# - Upload to /raw/products/products.csv
                        </div>
                        
                        <div class="success">
                            <strong>✅ Success Indicator:</strong> Workspace is created, security configured, and sample data uploaded to data lake.
                        </div>
                    </div>
                </div>
            </div>

            <!-- Step 2 -->
            <div class="step-section">
                <div class="step-header">
                    <div class="step-title">Step 2: Setting Up Dedicated SQL Pool and Serverless SQL</div>
                    <div class="step-description">Configure data warehousing capabilities with both dedicated and serverless compute</div>
                </div>
                <div class="step-content">
                    <div class="substep">
                        <div class="substep-number">1</div>
                        <div class="substep-title">Create Dedicated SQL Pool</div>
                        <p>Set up data warehouse for structured analytics:</p>
                        
                        <div class="code-block">
# In Synapse Studio > Manage > SQL pools > + New

Dedicated SQL Pool Configuration:
  SQL pool name: EnterpriseDW
  Performance level: DW100c (100 compute units)
  # Start small and scale up as needed
  
Advanced Settings:
  Collation: SQL_Latin1_General_CP1_CI_AS
  
Auto-pause Settings:
  Enable auto-pause: Yes
  Pause after: 60 minutes of inactivity
  # Saves costs when not in use
  
# After creation, the pool will take 5-10 minutes to provision

# Scale configuration (can be changed later):
# Navigate to SQL pool > Scale
# Performance levels available:
# - DW100c: 1 compute node, 60 GB memory
# - DW200c: 1 compute node, 120 GB memory
# - DW300c: 2 compute nodes, 180 GB memory
# - Up to DW30000c for large workloads
                        </div>
                    </div>

                    <div class="substep">
                        <div class="substep-number">2</div>
                        <div class="substep-title">Create Data Warehouse Schema</div>
                        <p>Design dimensional model for analytics:</p>
                        
                        <div class="code-block">
# Connect to Dedicated SQL Pool in Synapse Studio
# Go to Data > SQL pools > EnterpriseDW > New SQL script

-- Create dimensional model for sales analytics

-- 1. Create staging schema
CREATE SCHEMA staging;

-- 2. Create dimension tables
CREATE TABLE dbo.DimCustomer (
    CustomerKey INT IDENTITY(1,1) PRIMARY KEY,
    CustomerID NVARCHAR(10) NOT NULL,
    FirstName NVARCHAR(50),
    LastName NVARCHAR(50),
    Email NVARCHAR(100),
    City NVARCHAR(50),
    State NVARCHAR(10),
    Country NVARCHAR(50),
    EffectiveDate DATETIME2 DEFAULT GETDATE(),
    IsActive BIT DEFAULT 1
)
WITH (
    CLUSTERED COLUMNSTORE INDEX,
    DISTRIBUTION = REPLICATE
);

CREATE TABLE dbo.DimProduct (
    ProductKey INT IDENTITY(1,1) PRIMARY KEY,
    ProductID NVARCHAR(10) NOT NULL,
    ProductName NVARCHAR(100),
    Category NVARCHAR(50),
    Price DECIMAL(10,2),
    Supplier NVARCHAR(100),
    EffectiveDate DATETIME2 DEFAULT GETDATE(),
    IsActive BIT DEFAULT 1
)
WITH (
    CLUSTERED COLUMNSTORE INDEX,
    DISTRIBUTION = REPLICATE
);

CREATE TABLE dbo.DimDate (
    DateKey INT PRIMARY KEY,
    Date DATE,
    Year INT,
    Quarter INT,
    Month INT,
    MonthName NVARCHAR(10),
    Day INT,
    DayOfWeek INT,
    DayName NVARCHAR(10),
    IsWeekend BIT
)
WITH (
    CLUSTERED COLUMNSTORE INDEX,
    DISTRIBUTION = REPLICATE
);

-- 3. Create fact table
CREATE TABLE dbo.FactSales (
    SalesKey BIGINT IDENTITY(1,1) PRIMARY KEY,
    OrderID NVARCHAR(20),
    CustomerKey INT,
    ProductKey INT,
    DateKey INT,
    Quantity INT,
    UnitPrice DECIMAL(10,2),
    SalesAmount DECIMAL(12,2),
    Region NVARCHAR(20),
    FOREIGN KEY (CustomerKey) REFERENCES DimCustomer(CustomerKey),
    FOREIGN KEY (ProductKey) REFERENCES DimProduct(ProductKey),
    FOREIGN KEY (DateKey) REFERENCES DimDate(DateKey)
)
WITH (
    CLUSTERED COLUMNSTORE INDEX,
    DISTRIBUTION = HASH(CustomerKey)
);
                        </div>
                        
                        <div class="tip">
                            <strong>💡 Distribution Strategy:</strong> Use REPLICATE for small dimension tables and HASH distribution for large fact tables based on a high-cardinality column.
                        </div>
                    </div>

                    <div class="substep">
                        <div class="substep-number">3</div>
                        <div class="substep-title">Query Data Lake with Serverless SQL</div>
                        <p>Use serverless SQL to query data lake files directly:</p>
                        
                        <div class="code-block">
# In Synapse Studio > Data > Linked > Azure Data Lake Storage Gen2
# Navigate to your data files and right-click > New SQL script > Select TOP 100 rows

-- Query CSV files directly from data lake using OPENROWSET

-- 1. Query sales data
SELECT TOP 100 *
FROM OPENROWSET(
    BULK 'https://synapsestore[random].dfs.core.windows.net/synapse-data/raw/sales/2024/01/sales.csv',
    FORMAT = 'CSV',
    PARSER_VERSION = '2.0',
    HEADER_ROW = TRUE
) AS sales;

-- 2. Create external data source for easier querying
CREATE EXTERNAL DATA SOURCE SynapseDataLake
WITH (
    LOCATION = 'https://synapsestore[random].dfs.core.windows.net/synapse-data/'
);

-- 3. Create external file format
CREATE EXTERNAL FILE FORMAT CsvFormat
WITH (
    FORMAT_TYPE = DELIMITEDTEXT,
    FORMAT_OPTIONS (
        FIELD_TERMINATOR = ',',
        STRING_DELIMITER = '"',
        FIRST_ROW = 2,
        USE_TYPE_DEFAULT = FALSE
    )
);

-- 4. Query using external data source
SELECT 
    OrderID,
    CustomerID,
    ProductID,
    Quantity,
    UnitPrice,
    CAST(OrderDate AS DATE) AS OrderDate,
    Region,
    (Quantity * UnitPrice) AS SalesAmount
FROM OPENROWSET(
    BULK 'raw/sales/2024/01/sales.csv',
    DATA_SOURCE = 'SynapseDataLake',
    FORMAT = 'CSV',
    PARSER_VERSION = '2.0',
    HEADER_ROW = TRUE
) AS sales;

-- 5. Create view for easier access
CREATE VIEW vw_SalesData AS
SELECT 
    OrderID,
    CustomerID,
    ProductID,
    Quantity,
    UnitPrice,
    CAST(OrderDate AS DATE) AS OrderDate,
    Region,
    (Quantity * UnitPrice) AS SalesAmount
FROM OPENROWSET(
    BULK 'raw/sales/2024/01/sales.csv',
    DATA_SOURCE = 'SynapseDataLake',
    FORMAT = 'CSV',
    PARSER_VERSION = '2.0',
    HEADER_ROW = TRUE
) AS sales;
                        </div>
                    </div>
                </div>
            </div>

            <!-- Step 3 -->
            <div class="step-section">
                <div class="step-header">
                    <div class="step-title">Step 3: Creating and Configuring Apache Spark Pools</div>
                    <div class="step-description">Set up big data processing with Apache Spark for advanced analytics and machine learning</div>
                </div>
                <div class="step-content">
                    <div class="substep">
                        <div class="substep-number">1</div>
                        <div class="substep-title">Create Apache Spark Pool</div>
                        <p>Configure Spark cluster for big data processing:</p>
                        
                        <div class="code-block">
# In Synapse Studio > Manage > Apache Spark pools > + New

Spark Pool Configuration:
  Apache Spark pool name: sparkpool
  Node size family: Memory Optimized
  Node size: Small (4 vCPUs, 32 GB memory)
  
Autoscale:
  Enable autoscale: Yes
  Number of nodes: 3-10 nodes
  # Automatically scales based on workload
  
Auto-pause:
  Enable auto-pause: Yes
  Number of minutes idle: 15 minutes
  # Saves costs when not processing

Additional settings:
  Apache Spark version: 3.3
  Session level packages: Yes (allows custom packages)
  
Environment Configuration (optional):
  # Can configure custom environments with specific packages
  # For now, use default environment
                        </div>
                    </div>

                    <div class="substep">
                        <div class="substep-number">2</div>
                        <div class="substep-title">Create Spark Notebook for Data Processing</div>
                        <p>Build data processing pipeline using PySpark:</p>
                        
                        <div class="code-block">
# In Synapse Studio > Develop > + > Notebook
# Attach to: sparkpool

# Cell 1: Import libraries and read data
from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.types import *

# Read CSV files from data lake
sales_df = spark.read \
    .option("header", "true") \
    .option("inferSchema", "true") \
    .csv("abfss://synapse-data@synapsestore[random].dfs.core.windows.net/raw/sales/2024/01/sales.csv")

customers_df = spark.read \
    .option("header", "true") \
    .option("inferSchema", "true") \
    .csv("abfss://synapse-data@synapsestore[random].dfs.core.windows.net/raw/customers/customers.csv")

products_df = spark.read \
    .option("header", "true") \
    .option("inferSchema", "true") \
    .csv("abfss://synapse-data@synapsestore[random].dfs.core.windows.net/raw/products/products.csv")

# Display data
display(sales_df.limit(10))

# Cell 2: Data cleaning and transformation
# Clean sales data
sales_clean = sales_df \
    .withColumn("SalesAmount", col("Quantity") * col("UnitPrice")) \
    .withColumn("OrderDate", to_date(col("OrderDate"), "yyyy-MM-dd")) \
    .withColumn("Year", year(col("OrderDate"))) \
    .withColumn("Month", month(col("OrderDate"))) \
    .withColumn("Quarter", quarter(col("OrderDate")))

# Clean customer data
customers_clean = customers_df \
    .withColumn("FullName", concat(col("FirstName"), lit(" "), col("LastName"))) \
    .filter(col("Email").isNotNull())

# Display schema
sales_clean.printSchema()

# Cell 3: Data aggregation and analytics
# Sales by region
sales_by_region = sales_clean \
    .groupBy("Region") \
    .agg(
        sum("SalesAmount").alias("TotalSales"),
        avg("SalesAmount").alias("AvgOrderValue"),
        count("OrderID").alias("OrderCount"),
        countDistinct("CustomerID").alias("UniqueCustomers")
    ) \
    .orderBy(desc("TotalSales"))

display(sales_by_region)

# Monthly sales trend
monthly_sales = sales_clean \
    .groupBy("Year", "Month") \
    .agg(
        sum("SalesAmount").alias("TotalSales"),
        count("OrderID").alias("OrderCount")
    ) \
    .orderBy("Year", "Month")

display(monthly_sales)

# Cell 4: Join data for comprehensive analysis
# Join sales with customers and products
comprehensive_sales = sales_clean \
    .join(customers_clean, "CustomerID", "inner") \
    .join(products_df, "ProductID", "inner") \
    .select(
        "OrderID", "CustomerID", "FullName", "Email", 
        "ProductID", "ProductName", "Category",
        "Quantity", "UnitPrice", "SalesAmount",
        "OrderDate", "Region", "State", "Country"
    )

display(comprehensive_sales.limit(20))

# Cell 5: Save processed data
# Write processed data back to data lake in parquet format
comprehensive_sales.write \
    .mode("overwrite") \
    .option("path", "abfss://synapse-data@synapsestore[random].dfs.core.windows.net/processed/sales_comprehensive/") \
    .saveAsTable("processed_sales")

# Write aggregated data
sales_by_region.write \
    .mode("overwrite") \
    .parquet("abfss://synapse-data@synapsestore[random].dfs.core.windows.net/processed/aggregated/sales_by_region/")

monthly_sales.write \
    .mode("overwrite") \
    .parquet("abfss://synapse-data@synapsestore[random].dfs.core.windows.net/processed/aggregated/monthly_sales/")

print("Data processing completed successfully!")
                        </div>
                        
                        <div class="tip">
                            <strong>💡 Performance Tip:</strong> Use Parquet format for better compression and query performance. Partition large datasets by date or other relevant columns.
                        </div>
                    </div>

                    <div class="substep">
                        <div class="substep-number">3</div>
                        <div class="substep-title">Create Spark SQL Database</div>
                        <p>Create database in Spark for structured queries:</p>
                        
                        <div class="code-block">
# In Synapse Studio > Data > Databases > + New > Spark database

Database Configuration:
  Database name: SalesAnalytics
  Description: Sales analytics database for Spark workloads

# Create tables from processed data
# Cell in Spark notebook:

# Create database
spark.sql("CREATE DATABASE IF NOT EXISTS SalesAnalytics")
spark.sql("USE SalesAnalytics")

# Create managed table from processed data
comprehensive_sales.write \
    .mode("overwrite") \
    .option("path", "abfss://synapse-data@synapsestore[random].dfs.core.windows.net/processed/tables/sales_fact/") \
    .saveAsTable("SalesAnalytics.SalesFact")

# Create external table for raw data
spark.sql("""
CREATE TABLE IF NOT EXISTS SalesAnalytics.SalesRaw (
    OrderID STRING,
    CustomerID STRING,
    ProductID STRING,
    Quantity INT,
    UnitPrice DOUBLE,
    OrderDate STRING,
    Region STRING
)
USING CSV
OPTIONS (
    path 'abfss://synapse-data@synapsestore[random].dfs.core.windows.net/raw/sales/2024/01/sales.csv',
    header 'true'
)
""")

# Verify tables
spark.sql("SHOW TABLES IN SalesAnalytics").show()

# Test queries
spark.sql("""
SELECT Region, 
       SUM(SalesAmount) as TotalSales,
       COUNT(*) as OrderCount
FROM SalesAnalytics.SalesFact 
GROUP BY Region 
ORDER BY TotalSales DESC
""").show()
                        </div>
                        
                        <div class="success">
                            <strong>✅ Success Indicator:</strong> Spark pool is running, notebooks execute successfully, and data is processed and stored in optimized format.
                        </div>
                    </div>
                </div>
            </div>

            <!-- Step 4 -->
            <div class="step-section">
                <div class="step-header">
                    <div class="step-title">Step 4: Building Analytics Pipelines and Monitoring</div>
                    <div class="step-description">Create automated data pipelines and set up comprehensive monitoring for the analytics platform</div>
                </div>
                <div class="step-content">
                    <div class="substep">
                        <div class="substep-number">1</div>
                        <div class="substep-title">Create Data Integration Pipeline</div>
                        <p>Build automated pipeline for data processing:</p>
                        
                        <div class="code-block">
# In Synapse Studio > Integrate > + > Pipeline

Pipeline Configuration:
  Name: Sales-Data-Processing-Pipeline
  Description: End-to-end sales data processing
  
Activities:
1. Copy Data Activity:
   Name: LoadRawData
   Source: Raw CSV files from data lake
   Sink: Staging tables in SQL pool
   
2. Spark Notebook Activity:
   Name: ProcessAndTransform
   Notebook: Sales data processing notebook created earlier
   Spark pool: sparkpool
   
3. SQL Script Activity:
   Name: LoadDimensionsAndFacts
   SQL pool: EnterpriseDW
   Script: Load processed data into dimensional model
   
4. Data Flow Activity (optional):
   Name: DataQualityChecks
   Validate data quality and completeness

# Configure activity dependencies:
LoadRawData → ProcessAndTransform → LoadDimensionsAndFacts

# Add parameters for dynamic execution:
Parameters:
  - ProcessingDate: @formatDateTime(utcnow(), 'yyyy-MM-dd')
  - SourcePath: @concat('raw/sales/', formatDateTime(utcnow(), 'yyyy/MM'))
  - TargetPath: @concat('processed/sales/', formatDateTime(utcnow(), 'yyyy/MM'))

# SQL Script for loading dimensional model:
-- Load dimension tables first
MERGE dbo.DimCustomer AS target
USING (
    SELECT DISTINCT CustomerID, FirstName, LastName, Email, City, State, Country
    FROM staging.customers_staging
) AS source ON target.CustomerID = source.CustomerID
WHEN NOT MATCHED THEN
    INSERT (CustomerID, FirstName, LastName, Email, City, State, Country)
    VALUES (source.CustomerID, source.FirstName, source.LastName, 
            source.Email, source.City, source.State, source.Country);

-- Similar MERGE for DimProduct
-- Load fact table
INSERT INTO dbo.FactSales (OrderID, CustomerKey, ProductKey, DateKey, Quantity, UnitPrice, SalesAmount, Region)
SELECT 
    s.OrderID,
    c.CustomerKey,
    p.ProductKey,
    CAST(FORMAT(CAST(s.OrderDate AS DATE), 'yyyyMMdd') AS INT) as DateKey,
    s.Quantity,
    s.UnitPrice,
    s.SalesAmount,
    s.Region
FROM staging.sales_staging s
JOIN dbo.DimCustomer c ON s.CustomerID = c.CustomerID
JOIN dbo.DimProduct p ON s.ProductID = p.ProductID;
                        </div>
                    </div>

                    <div class="substep">
                        <div class="substep-number">2</div>
                        <div class="substep-title">Set Up Pipeline Scheduling and Triggers</div>
                        <p>Configure automated execution and monitoring:</p>
                        
                        <div class="code-block">
# Create Schedule Trigger
# In Pipeline view > Add trigger > New/Edit

Trigger Configuration:
  Name: DailySalesProcessing
  Type: Schedule
  Start date: Current date
  Time zone: (UTC) Coordinated Universal Time
  Recurrence: Daily
  At: 02:00 AM (after business hours)
  
Advanced scheduling:
  Specify an end date: No
  
# Create Tumbling Window Trigger for incremental processing
Trigger Configuration:
  Name: HourlySalesIncremental
  Type: Tumbling window
  Start date: Current date  
  Recurrence: Every 1 hour
  Delay: 0 minutes
  Max concurrency: 1
  Retry: 3 times

# Monitor Pipeline Execution:
# Navigate to Monitor > Pipeline runs
# Check execution status, duration, and activity details
# Set up alerts for failed pipeline runs

# Create Pipeline Monitoring Dashboard:
Pipeline Metrics to Track:
  - Execution duration
  - Success/failure rates
  - Data volume processed
  - Resource utilization
  - Cost per execution

# Custom Log Analytics Queries:
// Pipeline performance over time
SynapseDXCommand
| where OperationName == "PipelineRuns"
| summarize 
    AvgDuration = avg(DurationMs),
    SuccessRate = countif(Status == "Succeeded") * 100.0 / count()
    by bin(TimeGenerated, 1d)
| render timechart

// Resource utilization by Spark pool
SynapseDXCommand
| where ResourceClass == "SparkPool"
| summarize 
    AvgCpuUsage = avg(CpuUsage),
    AvgMemoryUsage = avg(MemoryUsage)
    by bin(TimeGenerated, 1h), ResourceName
| render timechart
                        </div>
                    </div>

                    <div class="substep">
                        <div class="substep-number">3</div>
                        <div class="substep-title">Performance Optimization and Cost Management</div>
                        <p>Optimize performance and manage costs effectively:</p>
                        
                        <div class="code-block">
# SQL Pool Optimization:

-- 1. Create statistics for better query performance
CREATE STATISTICS stat_factsales_customerkey 
ON dbo.FactSales (CustomerKey);

CREATE STATISTICS stat_factsales_datekey 
ON dbo.FactSales (DateKey);

-- 2. Create materialized views for common queries
CREATE MATERIALIZED VIEW mv_SalesByRegion
WITH (DISTRIBUTION = HASH(Region))
AS
SELECT 
    Region,
    YEAR(d.Date) as SalesYear,
    MONTH(d.Date) as SalesMonth,
    SUM(f.SalesAmount) as TotalSales,
    COUNT(*) as OrderCount
FROM dbo.FactSales f
JOIN dbo.DimDate d ON f.DateKey = d.DateKey
GROUP BY Region, YEAR(d.Date), MONTH(d.Date);

-- 3. Partition tables for better performance
CREATE TABLE dbo.FactSales_Partitioned (
    SalesKey BIGINT IDENTITY(1,1),
    OrderID NVARCHAR(20),
    CustomerKey INT,
    ProductKey INT,
    DateKey INT,
    Quantity INT,
    UnitPrice DECIMAL(10,2),
    SalesAmount DECIMAL(12,2),
    Region NVARCHAR(20)
)
WITH (
    CLUSTERED COLUMNSTORE INDEX,
    DISTRIBUTION = HASH(CustomerKey),
    PARTITION (DateKey RANGE RIGHT FOR VALUES 
        (20240101, 20240201, 20240301, 20240401))
);

# Spark Pool Optimization:

# Configure session-level settings for better performance
spark.conf.set("spark.sql.adaptive.enabled", "true")
spark.conf.set("spark.sql.adaptive.coalescePartitions.enabled", "true")
spark.conf.set("spark.sql.adaptive.skewJoin.enabled", "true")

# Use appropriate storage formats
# Write data in Delta format for ACID transactions
comprehensive_sales.write \
    .format("delta") \
    .mode("overwrite") \
    .option("overwriteSchema", "true") \
    .save("abfss://synapse-data@synapsestore[random].dfs.core.windows.net/processed/delta/sales/")

# Cost Management Settings:

# 1. SQL Pool auto-pause
# Already configured - pauses after 60 minutes

# 2. Spark Pool auto-pause  
# Already configured - pauses after 15 minutes

# 3. Set up cost alerts
# Navigate to Azure Portal > Cost Management + Billing
# Create budget alerts for Synapse workspace

# 4. Monitor resource usage
# Use built-in monitoring to track:
# - SQL DW compute usage
# - Spark cluster utilization
# - Storage costs
# - Network egress charges

# 5. Right-size resources based on usage patterns
# Scale SQL pool up/down based on workload
# Adjust Spark pool node counts based on processing needs
                        </div>
                        
                        <div class="warning">
                            <strong>⚠️ Cost Optimization:</strong> Always configure auto-pause for development environments and monitor resource usage to avoid unexpected costs.
                        </div>
                    </div>
                </div>
            </div>

            <!-- Validation Section -->
            <div class="validation-section">
                <h4>🔍 Validation & Testing</h4>
                <ol>
                    <li><strong>Workspace Operational:</strong> Verify Synapse workspace is accessible and all services are running</li>
                    <li><strong>Data Lake Integration:</strong> Confirm data files are accessible from both SQL and Spark</li>
                    <li><strong>SQL Pool Functionality:</strong> Test dimensional model queries and performance</li>
                    <li><strong>Spark Processing:</strong> Verify notebooks execute successfully and data transformations work</li>
                    <li><strong>Pipeline Execution:</strong> Confirm end-to-end pipeline runs without errors</li>
                    <li><strong>Scheduled Triggers:</strong> Verify automated pipeline execution works as scheduled</li>
                    <li><strong>Performance Metrics:</strong> Check query performance and resource utilization</li>
                    <li><strong>Cost Management:</strong> Verify auto-pause settings and monitor costs</li>
                    <li><strong>Security Configuration:</strong> Confirm proper access controls and permissions</li>
                </ol>
            </div>

            <div class="troubleshooting">
                <h4>🛠️ Common Issues & Solutions</h4>
                <ul>
                    <li><strong>SQL Pool connection timeouts:</strong> Check firewall settings, verify credentials, ensure pool is not paused</li>
                    <li><strong>Spark job failures:</strong> Review notebook logs, check memory settings, verify data lake permissions</li>
                    <li><strong>Data access permissions:</strong> Ensure managed identity has proper RBAC roles on storage account</li>
                    <li><strong>Poor query performance:</strong> Create statistics, use materialized views, optimize distributions</li>
                    <li><strong>High costs:</strong> Enable auto-pause, right-size compute resources, monitor usage patterns</li>
                    <li><strong>Pipeline failures:</strong> Check activity logs, verify linked service connections, validate data schemas</li>
                    <li><strong>Notebook kernel issues:</strong> Restart Spark session, check package dependencies, verify cluster status</li>
                    <li><strong>Data skew in Spark:</strong> Repartition data, use salting techniques, optimize join strategies</li>
                </ul>
            </div>

            <div class="resource-links">
                <h4>📚 Additional Resources</h4>
                <ul>
                    <li><a href="https://docs.microsoft.com/en-us/azure/synapse-analytics/" target="_blank">Azure Synapse Analytics Documentation</a></li>
                    <li><a href="https://docs.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/" target="_blank">Dedicated SQL Pool Guide</a></li>
                    <li><a href="https://docs.microsoft.com/en-us/azure/synapse-analytics/spark/" target="_blank">Apache Spark in Synapse</a></li>
                    <li><a href="https://docs.microsoft.com/en-us/azure/synapse-analytics/sql/on-demand-workspace-overview" target="_blank">Serverless SQL Pool Overview</a></li>
                    <li><a href="https://docs.microsoft.com/en-us/azure/synapse-analytics/monitoring/" target="_blank">Monitoring and Performance Tuning</a></li>
                    <li><a href="https://docs.microsoft.com/en-us/azure/synapse-analytics/security/" target="_blank">Security and Access Control</a></li>
                </ul>
            </div>
        </div>

        <div id="solution22" class="solution-card">
            <div class="solution-header">
                <div class="solution-title">Assignment 22: Azure Stream Analytics</div>
                <div class="solution-meta">
                    <span class="meta-tag">⚡ Real-time</span>
                    <span class="meta-tag">⏱️ 85 minutes</span>
                    <span class="meta-tag">📊 Advanced</span>
                </div>
            </div>
            
            <div class="time-estimate">
                <strong>⏱️ Estimated Time:</strong> 1 hour 25 minutes<br>
                <strong>🎯 Learning Objective:</strong> Build real-time stream processing pipelines with complex event processing, analytics, and multi-output scenarios
            </div>

            <div class="prerequisites">
                <h4>📋 Prerequisites</h4>
                <ul>
                    <li>Understanding of stream processing concepts and real-time analytics</li>
                    <li>Familiarity with SQL query language and window functions</li>
                    <li>Completed Assignment 10 (Event Hubs) for event ingestion</li>
                    <li>Basic knowledge of IoT data patterns and time-series analysis</li>
                    <li>Understanding of JSON data formats and message schemas</li>
                </ul>
            </div>

            <!-- Step 1 -->
            <div class="step-section">
                <div class="step-header">
                    <div class="step-title">Step 1: Setting Up Event Sources and Stream Analytics Job</div>
                    <div class="step-description">Create event ingestion infrastructure and initialize stream processing service</div>
                </div>
                <div class="step-content">
                    <div class="substep">
                        <div class="substep-number">1</div>
                        <div class="substep-title">Create Event Hub for Data Ingestion</div>
                        <p>Set up event streaming infrastructure:</p>
                        
                        <div class="code-block">
# Create Event Hubs Namespace
# Navigate to Azure Portal > Create a resource > Event Hubs

Event Hubs Namespace:
  Subscription: (Your subscription)
  Resource Group: Analytics-RG
  Namespace name: telemetry-events-[random]
  Location: East US
  Pricing tier: Standard
  Throughput units: 1
  Enable auto-inflate: Yes
  Maximum throughput units: 2

# Create Event Hub
# In Event Hubs Namespace > + Event Hub

Event Hub Configuration:
  Name: device-telemetry
  Partition count: 4
  Message retention: 1 day
  
# Create consumer groups for different processing scenarios
Consumer Groups:
  1. stream-analytics (default already exists)
  2. archive-processing
  3. real-time-dashboard
  4. machine-learning

# Configure authorization rules
# Go to Settings > Shared access policies > + Add

Shared Access Policy:
  Policy name: StreamAnalyticsAccess
  Permissions: Listen, Send
  # Note: Send is for testing, Stream Analytics only needs Listen
                        </div>
                    </div>

                    <div class="substep">
                        <div class="substep-number">2</div>
                        <div class="substep-title">Create Stream Analytics Job</div>
                        <p>Set up real-time stream processing service:</p>
                        
                        <div class="code-block">
# Navigate to Azure Portal > Create a resource > Stream Analytics job

Stream Analytics Job Configuration:
  Job name: telemetry-processing-[random]
  Subscription: (Your subscription)
  Resource Group: Analytics-RG
  Location: East US
  Hosting environment: Cloud
  Streaming units: 3
  # 3 SUs provide good balance of cost and performance
  
Advanced settings:
  Storage account: Create new or use existing
  # Used for checkpoint/recovery data
  
Event ordering:
  Event ordering policy: Adjust
  Out of order tolerance: 20 seconds
  Late arrival tolerance: 5 seconds
  # These settings handle event timing issues
                        </div>
                        
                        <div class="tip">
                            <strong>💡 Streaming Units:</strong> Start with 3 SUs for development. Each SU can process ~1MB/sec. Scale up based on actual throughput needs.
                        </div>
                    </div>

                    <div class="substep">
                        <div class="substep-number">3</div>
                        <div class="substep-title">Generate Sample IoT Data</div>
                        <p>Create test data generator for realistic scenarios:</p>
                        
                        <div class="code-block">
# Create simple Python script to generate IoT telemetry data
# Save as telemetry_generator.py

import json
import random
import time
from datetime import datetime, timezone
from azure.eventhub import EventHubProducerClient, EventData

# Connection string from Event Hub > Shared access policies
CONNECTION_STRING = "Endpoint=sb://telemetry-events-[random].servicebus.windows.net/;SharedAccessKeyName=StreamAnalyticsAccess;SharedAccessKey=[key]"
EVENT_HUB_NAME = "device-telemetry"

# Sample device IDs
DEVICE_IDS = [
    "sensor-001", "sensor-002", "sensor-003", "sensor-004", "sensor-005",
    "sensor-006", "sensor-007", "sensor-008", "sensor-009", "sensor-010"
]

LOCATIONS = [
    {"building": "A", "floor": 1, "room": "101"},
    {"building": "A", "floor": 2, "room": "201"},
    {"building": "B", "floor": 1, "room": "105"},
    {"building": "B", "floor": 3, "room": "301"},
    {"building": "C", "floor": 2, "room": "205"}
]

def generate_telemetry_data():
    device_id = random.choice(DEVICE_IDS)
    location = random.choice(LOCATIONS)
    
    # Simulate different sensor types
    data = {
        "deviceId": device_id,
        "timestamp": datetime.now(timezone.utc).isoformat(),
        "location": location,
        "temperature": round(random.uniform(18.0, 28.0), 2),
        "humidity": round(random.uniform(30.0, 70.0), 2),
        "pressure": round(random.uniform(990.0, 1020.0), 2),
        "battery": round(random.uniform(20.0, 100.0), 1),
        "signal_strength": random.randint(-90, -30),
        "event_type": random.choice(["telemetry", "alert", "diagnostic"]),
        "alert_level": random.choice(["normal", "warning", "critical"]) if random.random() < 0.1 else "normal"
    }
    
    # Occasionally generate anomalies
    if random.random() < 0.05:  # 5% chance
        data["temperature"] = round(random.uniform(35.0, 45.0), 2)  # High temp
        data["alert_level"] = "critical"
        data["event_type"] = "alert"
    
    return data

def send_telemetry():
    producer = EventHubProducerClient.from_connection_string(
        conn_str=CONNECTION_STRING,
        eventhub_name=EVENT_HUB_NAME
    )
    
    print("Starting telemetry data generation...")
    
    try:
        while True:
            # Generate batch of events
            event_data_batch = producer.create_batch()
            
            for _ in range(10):  # Send 10 events per batch
                telemetry = generate_telemetry_data()
                event_data = EventData(json.dumps(telemetry))
                event_data_batch.add(event_data)
            
            producer.send_batch(event_data_batch)
            print(f"Sent batch at {datetime.now()}")
            
            time.sleep(5)  # Send batch every 5 seconds
            
    except KeyboardInterrupt:
        print("Stopping telemetry generation...")
    finally:
        producer.close()

if __name__ == "__main__":
    send_telemetry()

# To run: pip install azure-eventhub
# Then: python telemetry_generator.py
                        </div>
                    </div>
                </div>
            </div>

            <!-- Step 2 -->
            <div class="step-section">
                <div class="step-header">
                    <div class="step-title">Step 2: Configuring Inputs and Outputs</div>
                    <div class="step-description">Set up data sources and destinations for stream processing</div>
                </div>
                <div class="step-content">
                    <div class="substep">
                        <div class="substep-number">1</div>
                        <div class="substep-title">Configure Event Hub Input</div>
                        <p>Connect Stream Analytics to event source:</p>
                        
                        <div class="code-block">
# In Stream Analytics job > Inputs > + Add stream input > Event Hub

Input Configuration:
  Input alias: telemetry-input
  # This is how you'll reference it in queries
  
Event Hub settings:
  Select Event Hub from your subscriptions: Yes
  Subscription: (Your subscription)
  Event Hub namespace: telemetry-events-[random]
  Event Hub name: device-telemetry
  Event Hub policy name: StreamAnalyticsAccess
  Event Hub consumer group: stream-analytics
  
Serialization:
  Event serialization format: JSON
  Encoding: UTF-8
  Event compression type: None
  
Advanced options:
  Partition key: deviceId
  # Helps with parallel processing
  
# Test the connection
# Click "Test" to verify connectivity
# Should show "Connection successful"
                        </div>
                    </div>

                    <div class="substep">
                        <div class="substep-number">2</div>
                        <div class="substep-title">Create SQL Database Output</div>
                        <p>Set up destination for processed analytics:</p>
                        
                        <div class="code-block">
# First create Azure SQL Database
# Navigate to Create a resource > SQL Database

SQL Database Configuration:
  Database name: TelemetryAnalytics
  Server: Create new
    Server name: telemetry-sql-[random]
    Admin login: sqladmin
    Password: ComplexP@ssw0rd123!
    Location: East US
  
Compute + storage: Basic (5 DTU)
Backup storage redundancy: Locally-redundant

# After database creation, create tables for output
# Connect via Query editor or SQL Server Management Studio

-- Create table for aggregated telemetry
CREATE TABLE dbo.TelemetryAggregates (
    WindowStart DATETIME2 NOT NULL,
    WindowEnd DATETIME2 NOT NULL,
    DeviceId NVARCHAR(50) NOT NULL,
    Building NVARCHAR(10),
    Floor INT,
    Room NVARCHAR(10),
    AvgTemperature FLOAT,
    MaxTemperature FLOAT,
    MinTemperature FLOAT,
    AvgHumidity FLOAT,
    AvgPressure FLOAT,
    EventCount INT,
    AlertCount INT,
    CreatedAt DATETIME2 DEFAULT GETDATE(),
    PRIMARY KEY (WindowStart, DeviceId)
);

-- Create table for alerts
CREATE TABLE dbo.TelemetryAlerts (
    AlertId UNIQUEIDENTIFIER DEFAULT NEWID() PRIMARY KEY,
    DeviceId NVARCHAR(50) NOT NULL,
    AlertLevel NVARCHAR(20) NOT NULL,
    Temperature FLOAT,
    Humidity FLOAT,
    Pressure FLOAT,
    Building NVARCHAR(10),
    Floor INT,
    Room NVARCHAR(10),
    EventTime DATETIME2 NOT NULL,
    ProcessedTime DATETIME2 DEFAULT GETDATE()
);

-- Create table for real-time metrics
CREATE TABLE dbo.RealTimeMetrics (
    MetricTime DATETIME2 NOT NULL,
    TotalDevices INT,
    ActiveDevices INT,
    AvgTemperature FLOAT,
    AlertsPerMinute INT,
    PRIMARY KEY (MetricTime)
);

# Configure SQL Database output in Stream Analytics
# In Stream Analytics job > Outputs > + Add > SQL Database

Output Configuration:
  Output alias: sql-aggregates
  Database: TelemetryAnalytics
  Server name: telemetry-sql-[random].database.windows.net
  Username: sqladmin
  Password: ComplexP@ssw0rd123!
  Table: TelemetryAggregates
  
Authentication mode: SQL Server authentication
Max batch count: 100
Max batch size: 10MB
                        </div>
                    </div>

                    <div class="substep">
                        <div class="substep-number">3</div>
                        <div class="substep-title">Configure Additional Outputs</div>
                        <p>Set up multiple output destinations:</p>
                        
                        <div class="code-block">
# 1. Create Blob Storage output for archival
# In Outputs > + Add > Blob storage/ADLS Gen2

Blob Storage Output:
  Output alias: archive-storage
  Storage account: Create new or use existing
    Account name: telemetryarchive[random]
    Container: telemetry-archive
  Path pattern: 
    year={datetime:yyyy}/month={datetime:MM}/day={datetime:dd}/hour={datetime:HH}/
  Event serialization format: JSON
  Encoding: UTF-8
  Format: Array

# 2. Create Event Hub output for downstream processing
# In Outputs > + Add > Event Hub

Event Hub Output:
  Output alias: processed-events
  Event Hub namespace: telemetry-events-[random]
  Event Hub name: processed-telemetry (create new event hub)
  Event Hub policy name: RootManageSharedAccessKey
  Partition key column: deviceId
  Event serialization format: JSON

# 3. Create Power BI output for real-time dashboards
# In Outputs > + Add > Power BI

Power BI Output:
  Output alias: powerbi-dashboard
  Authorize connection to Power BI
  Group workspace: My workspace
  Dataset name: TelemetryRealTime
  Table name: LiveMetrics
  Authentication mode: User token

# 4. Create additional SQL outputs for alerts
# In Outputs > + Add > SQL Database

Alert Output Configuration:
  Output alias: sql-alerts
  Database: TelemetryAnalytics (same as above)
  Table: TelemetryAlerts
  
Real-time Metrics Output:
  Output alias: sql-metrics
  Database: TelemetryAnalytics
  Table: RealTimeMetrics
                        </div>
                        
                        <div name="warning">
                            <strong>⚠️ Output Configuration:</strong> Each output adds to processing cost. Start with essential outputs and add more as needed.
                        </div>
                    </div>
                </div>
            </div>

            <!-- Step 3 -->
            <div class="step-section">
                <div class="step-header">
                    <div class="step-title">Step 3: Building Complex Stream Analytics Queries</div>
                    <div class="step-description">Create sophisticated real-time analytics with windowing, filtering, and aggregations</div>
                </div>
                <div class="step-content">
                    <div class="substep">
                        <div class="substep-number">1</div>
                        <div class="substep-title">Create Aggregation Query</div>
                        <p>Build time-window based analytics:</p>
                        
                        <div class="code-block">
-- Main aggregation query for telemetry data
-- In Stream Analytics job > Query editor

-- Query 1: 5-minute tumbling window aggregations
WITH AggregatedTelemetry AS (
    SELECT 
        System.Timestamp() AS WindowEnd,
        System.Timestamp() - CAST('00:05:00' AS TIME) AS WindowStart,
        deviceId,
        location.building AS Building,
        location.floor AS Floor,
        location.room AS Room,
        AVG(temperature) AS AvgTemperature,
        MAX(temperature) AS MaxTemperature,
        MIN(temperature) AS MinTemperature,
        AVG(humidity) AS AvgHumidity,
        AVG(pressure) AS AvgPressure,
        COUNT(*) AS EventCount,
        SUM(CASE WHEN alert_level != 'normal' THEN 1 ELSE 0 END) AS AlertCount
    FROM [telemetry-input]
    WHERE event_type = 'telemetry'
    GROUP BY 
        deviceId,
        location.building,
        location.floor,
        location.room,
        TumblingWindow(minute, 5)
)

-- Output aggregated data to SQL
SELECT 
    WindowStart,
    WindowEnd,
    deviceId,
    Building,
    Floor,
    Room,
    AvgTemperature,
    MaxTemperature,
    MinTemperature,
    AvgHumidity,
    AvgPressure,
    EventCount,
    AlertCount
INTO [sql-aggregates]
FROM AggregatedTelemetry;

-- Query 2: Real-time alerts for anomalies
SELECT 
    deviceId,
    alert_level AS AlertLevel,
    temperature,
    humidity,
    pressure,
    location.building AS Building,
    location.floor AS Floor,
    location.room AS Room,
    System.Timestamp() AS EventTime
INTO [sql-alerts]
FROM [telemetry-input]
WHERE alert_level IN ('warning', 'critical')
   OR temperature > 30.0
   OR humidity > 80.0
   OR battery < 20.0;

-- Query 3: System-wide metrics every minute
WITH SystemMetrics AS (
    SELECT 
        System.Timestamp() AS MetricTime,
        COUNT(DISTINCT deviceId) AS TotalDevices,
        COUNT(DISTINCT CASE WHEN DATEDIFF(minute, CAST(timestamp AS datetime), System.Timestamp()) <= 5 
                            THEN deviceId END) AS ActiveDevices,
        AVG(temperature) AS AvgTemperature,
        COUNT(CASE WHEN alert_level != 'normal' THEN 1 END) AS AlertsPerMinute
    FROM [telemetry-input]
    GROUP BY TumblingWindow(minute, 1)
)

SELECT *
INTO [sql-metrics]
FROM SystemMetrics;
                        </div>
                    </div>

                    <div class="substep">
                        <div class="substep-number">2</div>
                        <div class="substep-title">Implement Advanced Analytics</div>
                        <p>Add complex event processing and pattern detection:</p>
                        
                        <div class="code-block">
-- Advanced analytics queries

-- Query 4: Sliding window for trend detection
WITH TemperatureTrend AS (
    SELECT 
        deviceId,
        location.building,
        location.room,
        AVG(temperature) AS AvgTemp,
        STDEV(temperature) AS StdDevTemp,
        System.Timestamp() AS WindowEnd
    FROM [telemetry-input]
    WHERE event_type = 'telemetry'
    GROUP BY 
        deviceId,
        location.building,
        location.room,
        SlidingWindow(minute, 10)
),

-- Detect devices with increasing temperature trend
TrendAnalysis AS (
    SELECT 
        deviceId,
        building,
        room,
        AvgTemp,
        StdDevTemp,
        WindowEnd,
        LAG(AvgTemp, 1) OVER (PARTITION BY deviceId ORDER BY WindowEnd) AS PrevAvgTemp,
        LAG(AvgTemp, 2) OVER (PARTITION BY deviceId ORDER BY WindowEnd) AS Prev2AvgTemp
    FROM TemperatureTrend
)

-- Alert on consistent temperature increase
SELECT 
    deviceId,
    'temperature_trend' AS AlertType,
    AvgTemp AS CurrentTemp,
    PrevAvgTemp,
    Prev2AvgTemp,
    building,
    room,
    WindowEnd
INTO [processed-events]
FROM TrendAnalysis
WHERE AvgTemp > PrevAvgTemp 
  AND PrevAvgTemp > Prev2AvgTemp
  AND AvgTemp - Prev2AvgTemp > 2.0;

-- Query 5: Device heartbeat monitoring
WITH DeviceHeartbeat AS (
    SELECT 
        deviceId,
        MAX(CAST(timestamp AS datetime)) AS LastSeen,
        System.Timestamp() AS CheckTime
    FROM [telemetry-input]
    GROUP BY deviceId, TumblingWindow(minute, 2)
)

-- Detect offline devices
SELECT 
    deviceId,
    'device_offline' AS AlertType,
    LastSeen,
    CheckTime,
    DATEDIFF(minute, LastSeen, CheckTime) AS MinutesSinceLastSeen
INTO [processed-events]
FROM DeviceHeartbeat
WHERE DATEDIFF(minute, LastSeen, CheckTime) > 5;

-- Query 6: Pattern matching for maintenance alerts
WITH BatteryPattern AS (
    SELECT 
        deviceId,
        battery,
        LAG(battery, 1) OVER (PARTITION BY deviceId ORDER BY CAST(timestamp AS datetime)) AS PrevBattery,
        LAG(battery, 2) OVER (PARTITION BY deviceId ORDER BY CAST(timestamp AS datetime)) AS Prev2Battery,
        System.Timestamp() AS EventTime
    FROM [telemetry-input]
    WHERE event_type = 'telemetry'
)

-- Alert on rapid battery drain
SELECT 
    deviceId,
    'maintenance_required' AS AlertType,
    battery AS CurrentBattery,
    PrevBattery,
    Prev2Battery,
    EventTime
INTO [processed-events]
FROM BatteryPattern
WHERE battery < PrevBattery 
  AND PrevBattery < Prev2Battery
  AND Prev2Battery - battery > 10.0  -- Lost more than 10% in 3 readings
  AND battery < 30.0;  -- Below 30%

-- Query 7: Geospatial correlation (building/floor analysis)
WITH LocationAnalysis AS (
    SELECT 
        location.building,
        location.floor,
        AVG(temperature) AS AvgBuildingTemp,
        COUNT(DISTINCT deviceId) AS DeviceCount,
        SUM(CASE WHEN alert_level != 'normal' THEN 1 ELSE 0 END) AS AlertCount,
        System.Timestamp() AS WindowEnd
    FROM [telemetry-input]
    WHERE event_type = 'telemetry'
    GROUP BY 
        location.building,
        location.floor,
        TumblingWindow(minute, 5)
)

-- Archive location-based analytics
SELECT 
    building,
    floor,
    AvgBuildingTemp,
    DeviceCount,
    AlertCount,
    WindowEnd
INTO [archive-storage]
FROM LocationAnalysis;
                        </div>
                        
                        <div class="tip">
                            <strong>💡 Query Optimization:</strong> Use appropriate window types: Tumbling for discrete periods, Sliding for overlapping analysis, Hopping for regular intervals with gaps.
                        </div>
                    </div>

                    <div class="substep">
                        <div class="substep-number">3</div>
                        <div class="substep-title">Configure Machine Learning Integration</div>
                        <p>Add anomaly detection with built-in ML functions:</p>
                        
                        <div class="code-block">
-- Query 8: Anomaly detection using ML functions
WITH AnomalyDetection AS (
    SELECT 
        deviceId,
        temperature,
        humidity,
        pressure,
        AnomalyDetection_SpikeAndDip(temperature, 80, 120, 'spikesanddips') 
            OVER (PARTITION BY deviceId LIMIT DURATION(minute, 10)) AS TempAnomalyScore,
        AnomalyDetection_ChangePoint(temperature, 80, 1440) 
            OVER (PARTITION BY deviceId LIMIT DURATION(day, 1)) AS TempChangePoint,
        System.Timestamp() AS EventTime
    FROM [telemetry-input]
    WHERE event_type = 'telemetry'
)

-- Output anomalies for further investigation
SELECT 
    deviceId,
    temperature,
    humidity,
    pressure,
    TempAnomalyScore,
    TempChangePoint,
    EventTime,
    'ml_anomaly' AS AlertType
INTO [sql-alerts]
FROM AnomalyDetection
WHERE TempAnomalyScore > 0.8  -- High anomaly score
   OR TempChangePoint > 0.8;  -- Significant change point

-- Query 9: Reference data join for device metadata
-- First, upload reference data (device metadata) to blob storage
-- Create reference data input in Stream Analytics

WITH EnrichedTelemetry AS (
    SELECT 
        t.deviceId,
        t.temperature,
        t.humidity,
        t.pressure,
        t.location,
        r.deviceType,
        r.manufacturer,
        r.installDate,
        r.maintenanceSchedule,
        System.Timestamp() AS ProcessTime
    FROM [telemetry-input] t
    JOIN [device-reference] r ON t.deviceId = r.deviceId
    WHERE t.event_type = 'telemetry'
)

-- Enhanced analytics with device context
SELECT 
    deviceId,
    deviceType,
    manufacturer,
    location.building,
    location.floor,
    AVG(temperature) AS AvgTemp,
    COUNT(*) AS ReadingCount,
    DATEDIFF(day, CAST(installDate AS datetime), System.Timestamp()) AS DaysSinceInstall,
    ProcessTime
INTO [archive-storage]
FROM EnrichedTelemetry
GROUP BY 
    deviceId,
    deviceType,
    manufacturer,
    location.building,
    location.floor,
    installDate,
    TumblingWindow(hour, 1);
                        </div>
                    </div>
                </div>
            </div>

            <!-- Step 4 -->
            <div class="step-section">
                <div class="step-header">
                    <div class="step-title">Step 4: Testing, Monitoring, and Optimization</div>
                    <div class="step-description">Validate stream processing, set up monitoring, and optimize performance</div>
                </div>
                <div class="step-content">
                    <div class="substep">
                        <div class="substep-number">1</div>
                        <div class="substep-title">Test Stream Analytics Job</div>
                        <p>Validate queries and data flow:</p>
                        
                        <div class="code-block">
# Test with sample data
# In Stream Analytics job > Query editor

# 1. Use sample data for testing
# Click "Test query" with sample input data
# Upload sample JSON file with test telemetry:

[
  {
    "deviceId": "sensor-001",
    "timestamp": "2024-01-15T10:30:00Z",
    "location": {
      "building": "A",
      "floor": 1,
      "room": "101"
    },
    "temperature": 22.5,
    "humidity": 45.2,
    "pressure": 1013.2,
    "battery": 85.5,
    "signal_strength": -45,
    "event_type": "telemetry",
    "alert_level": "normal"
  },
  {
    "deviceId": "sensor-002",
    "timestamp": "2024-01-15T10:31:00Z",
    "location": {
      "building": "A",
      "floor": 2,
      "room": "201"
    },
    "temperature": 35.5,
    "humidity": 60.0,
    "pressure": 1012.8,
    "battery": 25.0,
    "signal_strength": -50,
    "event_type": "alert",
    "alert_level": "critical"
  }
]

# 2. Test individual query components
# Test aggregation logic:
SELECT 
    deviceId,
    AVG(temperature) AS AvgTemp,
    COUNT(*) AS EventCount
FROM [telemetry-input]
GROUP BY deviceId, TumblingWindow(minute, 5);

# 3. Start the job and monitor
# Click "Start" to begin processing
# Select job output start time:
# - Now (for real-time processing)
# - Custom (for historical data)
# - When last stopped (to resume)

# 4. Generate test data
# Run the Python telemetry generator script
# Monitor incoming events in Event Hub metrics
# Verify data appears in output destinations

# 5. Validate outputs
# Check SQL database tables:
SELECT TOP 10 * FROM dbo.TelemetryAggregates ORDER BY WindowEnd DESC;
SELECT TOP 10 * FROM dbo.TelemetryAlerts ORDER BY ProcessedTime DESC;
SELECT TOP 10 * FROM dbo.RealTimeMetrics ORDER BY MetricTime DESC;

# Check blob storage for archived data
# Navigate to storage account > telemetry-archive container
# Verify files are created with proper partitioning

# Check Power BI dataset (if configured)
# Verify real-time data is flowing to dashboard
                        </div>
                    </div>

                    <div class="substep">
                        <div class="substep-number">2</div>
                        <div class="substep-title">Set Up Monitoring and Alerts</div>
                        <p>Configure comprehensive monitoring for stream processing:</p>
                        
                        <div class="code-block">
# Configure Azure Monitor alerts
# Navigate to Stream Analytics job > Monitoring > Alerts

Alert Rules:
1. Input Events Alert:
   Signal: Input Events
   Condition: Less than 100 events in 5 minutes
   Action: Email notification
   
2. Output Events Alert:
   Signal: Output Events  
   Condition: 0 events for 10 minutes
   Action: Email + SMS notification
   
3. Runtime Errors Alert:
   Signal: Runtime Errors
   Condition: Greater than 0 errors in 5 minutes
   Action: Email notification
   
4. Data Conversion Errors Alert:
   Signal: Data Conversion Errors
   Condition: Greater than 5 errors in 15 minutes
   Action: Email notification

5. Watermark Delay Alert:
   Signal: Watermark Delay
   Condition: Greater than 60 seconds
   Action: Email notification

# Create Log Analytics Workspace for detailed monitoring
Workspace Configuration:
  Name: StreamAnalyticsLogs
  Resource Group: Analytics-RG
  
# Configure Diagnostic Settings
# In Stream Analytics job > Diagnostic settings > + Add setting

Diagnostic Setting:
  Name: StreamAnalyticsMonitoring
  Send to Log Analytics workspace: StreamAnalyticsLogs
  
Log Categories:
  ✓ Execution
  ✓ Authoring
  ✓ AllMetrics

# Sample KQL queries for monitoring:

// Query performance over time
AzureDiagnostics
| where ResourceProvider == "MICROSOFT.STREAMANALYTICS"
| where Category == "Execution"
| summarize 
    AvgInputEvents = avg(inputEvents_d),
    AvgOutputEvents = avg(outputEvents_d),
    AvgWatermarkDelay = avg(watermarkDelaySeconds_d)
    by bin(TimeGenerated, 5m)
| render timechart

// Error analysis
AzureDiagnostics
| where ResourceProvider == "MICROSOFT.STREAMANALYTICS"
| where Level == "Error"
| summarize ErrorCount = count() by OperationName, bin(TimeGenerated, 1h)
| render columnchart

// Resource utilization
AzureDiagnostics
| where ResourceProvider == "MICROSOFT.STREAMANALYTICS"
| where Category == "Execution"
| project TimeGenerated, resourceUtilization_d, cpuPercent_d, memoryUtilization_d
| render timechart
                        </div>
                    </div>

                    <div class="substep">
                        <div class="substep-number">3</div>
                        <div class="substep-title">Performance Optimization</div>
                        <p>Optimize stream processing performance and costs:</p>
                        
                        <div class="code-block">
# Performance Optimization Strategies:

# 1. Query Optimization
-- Use temporal filters early in queries
SELECT deviceId, temperature, System.Timestamp() AS EventTime
FROM [telemetry-input] TIMESTAMP BY CAST(timestamp AS datetime)
WHERE DATEDIFF(hour, System.Timestamp(), CAST(timestamp AS datetime)) BETWEEN -1 AND 1
  AND event_type = 'telemetry'  -- Filter early

-- Minimize data shuffling with appropriate partitioning
-- Use PARTITION BY in window functions when possible
SELECT 
    deviceId,
    AVG(temperature) OVER (PARTITION BY deviceId ORDER BY timestamp ROWS 10 PRECEDING) AS MovingAvg
FROM [telemetry-input];

-- Use efficient aggregations
-- Instead of multiple SELECT statements, use CTEs

# 2. Scaling Configuration
# Monitor SU% utilization in portal
# If consistently >80%, scale up
# If consistently <20%, scale down

Scaling Guidelines:
- 1 SU: ~1 MB/sec throughput
- 3 SUs: ~3 MB/sec throughput  
- 6 SUs: ~6 MB/sec throughput
- Max: 192 SUs for very high throughput

# Scale up via CLI:
az stream-analytics job scale \
  --resource-group Analytics-RG \
  --name telemetry-processing-[random] \
  --streaming-units 6

# 3. Optimize Event Ordering
# Configure event ordering policy based on use case:

Event Ordering Options:
- Adjust: Handle out-of-order events (default)
- Drop: Drop out-of-order events (better performance)

# For high-volume scenarios with tolerance for data loss:
Out of order tolerance: 0 seconds
Late arrival tolerance: 0 seconds
Event ordering policy: Drop

# 4. Output Optimization
# Batch output writes for better performance

SQL Output Optimization:
- Max batch count: 100-1000 (balance latency vs throughput)
- Max batch size: 10MB (default is usually optimal)

Blob Output Optimization:
- Path pattern with appropriate partitioning
- Use Parquet format for better compression
- Configure appropriate file size

# 5. Reference Data Optimization
# Keep reference data small (<100MB)
# Update frequency: only when necessary
# Use appropriate snapshot format

# 6. Query Pattern Optimization
-- Use CASE WHEN instead of multiple WHERE clauses
SELECT 
    deviceId,
    temperature,
    CASE 
        WHEN temperature > 30 THEN 'high'
        WHEN temperature < 15 THEN 'low'
        ELSE 'normal'
    END AS TempCategory
FROM [telemetry-input];

-- Use appropriate window sizes
-- Smaller windows = lower latency, higher resource usage
-- Larger windows = higher latency, better resource efficiency

# 7. Cost Optimization
# Stop jobs during non-business hours if appropriate
# Use reserved capacity for predictable workloads
# Monitor and set budget alerts

# Auto-start/stop script (Azure Automation):
# Start job at 8 AM weekdays
# Stop job at 6 PM weekdays
                        </div>
                        
                        <div class="success">
                            <strong>✅ Success Indicators:</strong> Query execution completes without errors, data flows to all outputs, monitoring shows healthy metrics, and performance meets requirements.
                        </div>
                    </div>
                </div>
            </div>

            <!-- Validation Section -->
            <div class="validation-section">
                <h4>🔍 Validation & Testing</h4>
                <ol>
                    <li><strong>Event Hub Connectivity:</strong> Verify events are being ingested from Event Hub</li>
                    <li><strong>Query Execution:</strong> Confirm all queries execute without syntax errors</li>
                    <li><strong>Output Data Flow:</strong> Verify data appears in all configured outputs (SQL, Blob, Power BI)</li>
                    <li><strong>Real-time Processing:</strong> Check end-to-end latency meets requirements</li>
                    <li><strong>Alert Generation:</strong> Verify alerts are triggered for anomalous data</li>
                    <li><strong>Data Quality:</strong> Confirm aggregations and calculations are accurate</li>
                    <li><strong>Monitoring Active:</strong> Verify Azure Monitor alerts and Log Analytics are capturing metrics</li>
                    <li><strong>Performance Metrics:</strong> Check SU utilization and throughput are optimal</li>
                    <li><strong>Error Handling:</strong> Verify system handles malformed data gracefully</li>
                </ol>
            </div>

            <div class="troubleshooting">
                <h4>🛠️ Common Issues & Solutions</h4>
                <ul>
                    <li><strong>No input events:</strong> Check Event Hub connection, consumer group, and authentication keys</li>
                    <li><strong>Query syntax errors:</strong> Validate SQL syntax, check input aliases and field names</li>
                    <li><strong>Output connection failures:</strong> Verify destination credentials, network connectivity, and permissions</li>
                    <li><strong>High watermark delay:</strong> Optimize queries, increase streaming units, check for data skew</li>
                    <li><strong>Data conversion errors:</strong> Validate input JSON schema, handle null values, check data types</li>
                    <li><strong>Missing output data:</strong> Check output configuration, verify table schema matches query output</li>
                    <li><strong>Performance issues:</strong> Scale up streaming units, optimize queries, reduce window sizes</li>
                    <li><strong>Cost overruns:</strong> Monitor SU usage, optimize queries, consider auto-start/stop for dev environments</li>
                </ul>
            </div>

            <div class="resource-links">
                <h4>📚 Additional Resources</h4>
                <ul>
                    <li><a href="https://docs.microsoft.com/en-us/azure/stream-analytics/" target="_blank">Azure Stream Analytics Documentation</a></li>
                    <li><a href="https://docs.microsoft.com/en-us/stream-analytics-query/" target="_blank">Stream Analytics Query Language Reference</a></li>
                    <li><a href="https://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-window-functions" target="_blank">Window Functions Guide</a></li>
                    <li><a href="https://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-machine-learning-anomaly-detection" target="_blank">Anomaly Detection Functions</a></li>
                    <li><a href="https://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-monitoring" target="_blank">Monitoring and Troubleshooting Guide</a></li>
                    <li><a href="https://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-scale-jobs" target="_blank">Scaling and Performance Optimization</a></li>
                </ul>
            </div>
        </div>

        <div id="solution23" class="solution-card">
            <div class="solution-header">
                <div class="solution-title">Assignment 23: Azure Machine Learning</div>
                <div class="solution-meta">
                    <span class="meta-tag">🤖 ML</span>
                    <span class="meta-tag">⏱️ 120 minutes</span>
                    <span class="meta-tag">📊 Expert</span>
                </div>
            </div>
            
            <div class="time-estimate">
                <strong>⏱️ Estimated Time:</strong> 2 hours<br>
                <strong>🎯 Learning Objective:</strong> Complete machine learning lifecycle from data preparation to model deployment with MLOps practices
            </div>

            <div class="prerequisites">
                <h4>📋 Prerequisites</h4>
                <ul>
                    <li>Understanding of machine learning concepts (supervised/unsupervised learning, model evaluation)</li>
                    <li>Familiarity with Python programming and data science libraries (pandas, scikit-learn)</li>
                    <li>Basic knowledge of statistics and data analysis</li>
                    <li>Completed Assignment 2 (Storage Account) for data storage</li>
                    <li>Understanding of model lifecycle and deployment concepts</li>
                </ul>
            </div>

            <!-- Step 1 -->
            <div class="step-section">
                <div class="step-header">
                    <div class="step-title">Step 1: Setting Up Azure Machine Learning Workspace</div>
                    <div class="step-description">Create ML workspace with compute resources and data preparation</div>
                </div>
                <div class="step-content">
                    <div class="substep">
                        <div class="substep-number">1</div>
                        <div class="substep-title">Create ML Workspace</div>
                        <p>Set up the machine learning environment:</p>
                        
                        <div class="code-block">
# Navigate to Azure Portal > Create a resource > Machine Learning

Machine Learning Workspace:
  Subscription: (Your subscription)
  Resource Group: ML-RG (create new)
  Workspace name: enterprise-ml-[random]
  Region: East US
  
Associated Resources:
  Storage account: Create new
    Account name: enterprisemlstore[random]
    Account type: Standard_LRS
  
  Key vault: Create new
    Vault name: enterprise-ml-kv-[random]
    
  Application insights: Create new
    Application name: enterprise-ml-insights
    
  Container registry: Create new
    Registry name: enterprisemlacr[random]
    SKU: Basic

Advanced Settings:
  High business impact workspace: No (for development)
  Allow public access: All networks (for development)
  # In production, use private endpoints
  
Identity:
  System assigned managed identity: Enable
  
Tags:
  Environment: Development
  Project: MachineLearning
  Owner: DataScienceTeam
                        </div>
                        
                        <div class="tip">
                            <strong>💡 Resource Organization:</strong> The workspace automatically creates supporting services (Storage, Key Vault, Application Insights, Container Registry) that work together seamlessly.
                        </div>
                    </div>

                    <div class="substep">
                        <div class="substep-number">2</div>
                        <div class="substep-title">Create Compute Resources</div>
                        <p>Set up compute instances for development and training:</p>
                        
                        <div class="code-block">
# Launch Azure ML Studio: https://ml.azure.com
# Navigate to Compute > Compute instances > + New

Compute Instance Configuration:
  Compute name: ml-compute-instance-[yourname]
  Virtual machine type: CPU
  Virtual machine size: Standard_DS3_v2
    # 4 cores, 14 GB RAM - good for development
  
Advanced settings:
  Enable SSH access: Yes (for debugging)
  Enable terminal access: Yes
  Assign managed identity: System-assigned
  
Security:
  Enable virtual network: No (for simplicity)
  # In production, use VNet integration
  
Startup script: (Optional)
  # Create script to install custom packages
  
# Create Compute Cluster for training jobs
# Navigate to Compute > Compute clusters > + New

Compute Cluster Configuration:
  Compute name: ml-training-cluster
  Virtual machine type: CPU
  Virtual machine priority: Dedicated
  Virtual machine size: Standard_DS3_v2
  
Scale settings:
  Minimum number of nodes: 0
  Maximum number of nodes: 4
  Idle seconds before scale down: 120
  # Auto-scales based on workload
  
Advanced settings:
  Enable SSH access: Yes
  Admin username: azureuser
  SSH public key: (Generate or upload)
  Enable virtual network: No
  
# For GPU workloads (optional)
GPU Compute Cluster:
  Compute name: ml-gpu-cluster
  Virtual machine type: GPU  
  Virtual machine size: Standard_NC6s_v3
  # 6 cores, 112 GB RAM, 1 V100 GPU
  Scale settings:
    Minimum: 0, Maximum: 2
                        </div>
                    </div>

                    <div class="substep">
                        <div class="substep-number">3</div>
                        <div class="substep-title">Prepare Sample Dataset</div>
                        <p>Upload and register datasets for machine learning:</p>
                        
                        <div class="code-block">
# Create sample dataset for customer churn prediction
# In ML Studio > Data > Datasets > + Create dataset > From web files

Dataset Creation:
  Name: customer-churn-data
  Description: Customer data for churn prediction model
  Dataset type: Tabular
  
Web URL: https://raw.githubusercontent.com/IBM/telco-customer-churn-on-icp4d/master/data/Telco-Customer-Churn.csv

Settings:
  File format: Delimited
  Delimiter: Comma
  Encoding: UTF-8
  Column headers: All files have same headers
  Skip rows: None
  
Schema:
  Include all columns: Yes
  # Review and validate data types

# Alternative: Upload local CSV file
# Sample customer churn data structure:
CustomerID,Gender,SeniorCitizen,Partner,Dependents,Tenure,PhoneService,MultipleLines,InternetService,OnlineSecurity,OnlineBackup,DeviceProtection,TechSupport,StreamingTV,StreamingMovies,Contract,PaperlessBilling,PaymentMethod,MonthlyCharges,TotalCharges,Churn
7590-VHVEG,Female,0,Yes,No,1,No,No phone service,DSL,No,Yes,No,No,No,No,Month-to-month,Yes,Electronic check,29.85,29.85,No
5575-GNVDE,Male,0,No,No,34,Yes,No,DSL,Yes,No,Yes,No,No,No,One year,No,Mailed check,56.95,1889.5,No

# Create additional datasets
# 1. Upload to Blob Storage first
# Navigate to Storage Account > Containers > Create container: ml-datasets

# 2. Register dataset from Blob Storage
# In ML Studio > Data > Datasets > + Create dataset > From datastore

Dataset from Datastore:
  Name: sales-forecast-data
  Description: Historical sales data for forecasting
  Dataset type: Tabular
  Datastore: Default blob store
  Path: ml-datasets/sales_data.csv
  
# 3. Create File dataset for unstructured data
File Dataset:
  Name: product-images
  Description: Product images for classification
  Dataset type: File
  Path: ml-datasets/images/
                        </div>
                        
                        <div name="success">
                            <strong>✅ Success Indicator:</strong> Workspace is created, compute resources are running, and datasets are registered and accessible.
                        </div>
                    </div>
                </div>
            </div>

            <!-- Step 2 -->
            <div class="step-section">
                <div class="step-header">
                    <div class="step-title">Step 2: Building and Training Machine Learning Models</div>
                    <div class="step-description">Develop, train, and evaluate ML models using AutoML and custom training</div>
                </div>
                <div class="step-content">
                    <div class="substep">
                        <div class="substep-number">1</div>
                        <div class="substep-title">Create AutoML Experiment</div>
                        <p>Use automated machine learning for model development:</p>
                        
                        <div class="code-block">
# In ML Studio > Automated ML > + New Automated ML job

AutoML Experiment Configuration:
  Experiment name: customer-churn-automl
  Description: Automated ML for customer churn prediction
  
Dataset Selection:
  Dataset: customer-churn-data
  Target column: Churn
  
Task type: Classification
  Primary metric: AUC_weighted
  # Other options: Accuracy, Precision, Recall
  
Additional configuration:
  Training job time (hours): 1
  Metric score threshold: 0.85
  Max concurrent iterations: 4
  
Validation:
  Validation type: Train-validation split
  Validation size: 20%
  
Compute:
  Select compute type: Compute cluster
  Select compute cluster: ml-training-cluster
  
# Advanced settings (expand)
Exit criteria:
  Training job time (hours): 1
  Metric score threshold: 0.85
  
Concurrency:
  Max concurrent iterations: 4
  Max cores per iteration: -1 (all available)
  
Featurization:
  Automatic featurization: Enable
  # Handles missing values, encoding, scaling automatically
  
Allowed algorithms:
  ✓ LightGBM
  ✓ XGBoost  
  ✓ Random Forest
  ✓ Logistic Regression
  ✓ Naive Bayes
  # Uncheck algorithms you don't want to try

# Start the experiment
# Monitor progress in Experiments section
# AutoML will try multiple algorithms and hyperparameters
                        </div>
                    </div>

                    <div class="substep">
                        <div class="substep-number">2</div>
                        <div class="substep-title">Custom Model Training with Notebooks</div>
                        <p>Build custom models using Jupyter notebooks:</p>
                        
                        <div class="code-block">
# In ML Studio > Notebooks > + Create new file
# File name: customer_churn_custom_model.ipynb
# Compute: ml-compute-instance-[yourname]

# Cell 1: Import libraries and setup
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score
from azureml.core import Workspace, Dataset, Experiment, Run
from azureml.core.model import Model
import joblib

# Connect to workspace
ws = Workspace.from_config()
print(f"Workspace name: {ws.name}")
print(f"Resource group: {ws.resource_group}")

# Cell 2: Load and explore data
# Get dataset from workspace
dataset = Dataset.get_by_name(ws, name='customer-churn-data')
df = dataset.to_pandas_dataframe()

print(f"Dataset shape: {df.shape}")
print("\nColumn info:")
print(df.info())
print("\nChurn distribution:")
print(df['Churn'].value_counts(normalize=True))

# Data visualization
plt.figure(figsize=(12, 8))

# Subplot 1: Churn distribution
plt.subplot(2, 3, 1)
df['Churn'].value_counts().plot(kind='bar')
plt.title('Churn Distribution')
plt.xticks(rotation=0)

# Subplot 2: Tenure vs Churn
plt.subplot(2, 3, 2)
sns.boxplot(x='Churn', y='tenure', data=df)
plt.title('Tenure vs Churn')

# Subplot 3: Monthly charges vs Churn
plt.subplot(2, 3, 3)
sns.boxplot(x='Churn', y='MonthlyCharges', data=df)
plt.title('Monthly Charges vs Churn')

# Subplot 4: Contract type vs Churn
plt.subplot(2, 3, 4)
contract_churn = df.groupby(['Contract', 'Churn']).size().unstack()
contract_churn.plot(kind='bar', stacked=True)
plt.title('Contract Type vs Churn')
plt.xticks(rotation=45)

# Subplot 5: Internet Service vs Churn
plt.subplot(2, 3, 5)
internet_churn = df.groupby(['InternetService', 'Churn']).size().unstack()
internet_churn.plot(kind='bar', stacked=True)
plt.title('Internet Service vs Churn')
plt.xticks(rotation=45)

plt.tight_layout()
plt.show()

# Cell 3: Data preprocessing
# Handle missing values
df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')
df['TotalCharges'].fillna(df['TotalCharges'].median(), inplace=True)

# Encode categorical variables
label_encoders = {}
categorical_columns = ['gender', 'Partner', 'Dependents', 'PhoneService', 'MultipleLines',
                      'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection',
                      'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract',
                      'PaperlessBilling', 'PaymentMethod', 'Churn']

for column in categorical_columns:
    le = LabelEncoder()
    df[column] = le.fit_transform(df[column])
    label_encoders[column] = le

# Feature engineering
df['tenure_months'] = df['tenure']
df['avg_monthly_charges'] = df['TotalCharges'] / (df['tenure'] + 1)  # +1 to avoid division by zero
df['charges_per_service'] = df['MonthlyCharges'] / (df[['PhoneService', 'InternetService']].sum(axis=1) + 1)

# Prepare features and target
features_to_drop = ['customerID']  # Non-predictive features
X = df.drop(['Churn'] + features_to_drop, axis=1)
y = df['Churn']

print(f"Features shape: {X.shape}")
print(f"Target shape: {y.shape}")

# Cell 4: Model training and evaluation
# Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Scale features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Train multiple models
models = {
    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),
    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42)
}

model_results = {}

for name, model in models.items():
    print(f"\nTraining {name}...")
    
    if name == 'Logistic Regression':
        model.fit(X_train_scaled, y_train)
        y_pred = model.predict(X_test_scaled)
        y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]
    else:
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
        y_pred_proba = model.predict_proba(X_test)[:, 1]
    
    # Evaluate model
    accuracy = accuracy_score(y_test, y_pred)
    auc_score = roc_auc_score(y_test, y_pred_proba)
    
    model_results[name] = {
        'model': model,
        'accuracy': accuracy,
        'auc_score': auc_score,
        'predictions': y_pred,
        'probabilities': y_pred_proba
    }
    
    print(f"Accuracy: {accuracy:.4f}")
    print(f"AUC Score: {auc_score:.4f}")
    print("\nClassification Report:")
    print(classification_report(y_test, y_pred))

# Select best model
best_model_name = max(model_results.keys(), key=lambda k: model_results[k]['auc_score'])
best_model = model_results[best_model_name]['model']

print(f"\nBest model: {best_model_name}")
print(f"Best AUC Score: {model_results[best_model_name]['auc_score']:.4f}")

# Cell 5: Feature importance analysis
if best_model_name == 'Random Forest':
    feature_importance = pd.DataFrame({
        'feature': X.columns,
        'importance': best_model.feature_importances_
    }).sort_values('importance', ascending=False)
    
    plt.figure(figsize=(10, 8))
    plt.barh(feature_importance['feature'][:15], feature_importance['importance'][:15])
    plt.title('Top 15 Feature Importances')
    plt.xlabel('Importance')
    plt.gca().invert_yaxis()
    plt.tight_layout()
    plt.show()
    
    print("Top 10 Important Features:")
    print(feature_importance.head(10))
</div>
                    </div>

                    <div class="substep">
                        <div class="substep-number">3</div>
                        <div class="substep-title">Model Experiment Tracking</div>
                        <p>Use Azure ML experiment tracking for model management:</p>
                        
                        <div class="code-block">
# Cell 6: Azure ML experiment tracking
# Create experiment
experiment = Experiment(workspace=ws, name='customer-churn-custom')

# Start run
with experiment.start_logging() as run:
    # Log parameters
    run.log('algorithm', best_model_name)
    run.log('train_size', len(X_train))
    run.log('test_size', len(X_test))
    
    # Log metrics
    run.log('accuracy', model_results[best_model_name]['accuracy'])
    run.log('auc_score', model_results[best_model_name]['auc_score'])
    
    # Log confusion matrix
    cm = confusion_matrix(y_test, model_results[best_model_name]['predictions'])
    run.log('true_negatives', int(cm[0][0]))
    run.log('false_positives', int(cm[0][1]))
    run.log('false_negatives', int(cm[1][0]))
    run.log('true_positives', int(cm[1][1]))
    
    # Log feature importance (if Random Forest)
    if best_model_name == 'Random Forest':
        for feature, importance in zip(X.columns, best_model.feature_importances_):
            run.log(f'feature_importance_{feature}', importance)
    
    # Save model artifacts
    model_filename = 'churn_prediction_model.pkl'
    scaler_filename = 'feature_scaler.pkl'
    
    joblib.dump(best_model, model_filename)
    joblib.dump(scaler, scaler_filename)
    joblib.dump(label_encoders, 'label_encoders.pkl')
    
    # Upload model files
    run.upload_file('outputs/' + model_filename, model_filename)
    run.upload_file('outputs/' + scaler_filename, scaler_filename)
    run.upload_file('outputs/label_encoders.pkl', 'label_encoders.pkl')
    
    # Create and upload model visualization
    plt.figure(figsize=(8, 6))
    cm_plot = sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
    plt.title('Confusion Matrix')
    plt.ylabel('Actual')
    plt.xlabel('Predicted')
    plt.savefig('confusion_matrix.png')
    run.upload_file('outputs/confusion_matrix.png', 'confusion_matrix.png')
    plt.show()
    
    # Complete run
    run.complete()
    
print(f"Experiment run completed: {run.get_portal_url()}")

# Cell 7: Register model
# Register the best performing model
model = Model.register(
    workspace=ws,
    model_name='customer-churn-predictor',
    model_path='outputs/' + model_filename,
    description='Customer churn prediction model using Random Forest',
    tags={
        'algorithm': best_model_name,
        'accuracy': str(model_results[best_model_name]['accuracy']),
        'auc_score': str(model_results[best_model_name]['auc_score']),
        'framework': 'scikit-learn'
    },
    run=run
)

print(f"Model registered: {model.name}, Version: {model.version}")
                        </div>
                        
                        <div class="tip">
                            <strong>💡 Experiment Tracking:</strong> Azure ML automatically tracks metrics, parameters, and artifacts, making it easy to compare different model runs and reproduce results.
                        </div>
                    </div>
                </div>
            </div>

            <!-- Step 3 -->
            <div class="step-section">
                <div class="step-header">
                    <div class="step-title">Step 3: Model Deployment and Real-time Inference</div>
                    <div class="step-description">Deploy trained models as web services for real-time predictions</div>
                </div>
                <div class="step-content">
                    <div class="substep">
                        <div class="substep-number">1</div>
                        <div class="substep-title">Create Inference Configuration</div>
                        <p>Set up scoring script and environment for deployment:</p>
                        
                        <div class="code-block">
# Cell 8: Create scoring script
# Create score.py file for model inference

scoring_script = '''
import json
import joblib
import numpy as np
import pandas as pd
from azureml.core.model import Model

def init():
    global model, scaler, label_encoders
    
    # Load model and preprocessing objects
    model_path = Model.get_model_path('customer-churn-predictor')
    model = joblib.load(model_path)
    
    # Load scaler and encoders (these should be uploaded with model)
    try:
        scaler = joblib.load('feature_scaler.pkl')
        label_encoders = joblib.load('label_encoders.pkl')
    except:
        # If not available, create dummy objects
        scaler = None
        label_encoders = None

def run(raw_data):
    try:
        # Parse input data
        data = json.loads(raw_data)
        
        # Convert to DataFrame
        df = pd.DataFrame(data)
        
        # Preprocess data (same as training)
        if 'customerID' in df.columns:
            df = df.drop(['customerID'], axis=1)
        
        # Handle TotalCharges conversion
        if 'TotalCharges' in df.columns:
            df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')
            df['TotalCharges'].fillna(df['TotalCharges'].median(), inplace=True)
        
        # Apply label encoding if available
        if label_encoders:
            categorical_columns = ['gender', 'Partner', 'Dependents', 'PhoneService', 'MultipleLines',
                                  'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection',
                                  'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract',
                                  'PaperlessBilling', 'PaymentMethod']
            
            for column in categorical_columns:
                if column in df.columns and column in label_encoders:
                    try:
                        df[column] = label_encoders[column].transform(df[column])
                    except:
                        # Handle unseen categories
                        df[column] = 0
        
        # Feature engineering
        if 'tenure' in df.columns and 'TotalCharges' in df.columns:
            df['avg_monthly_charges'] = df['TotalCharges'] / (df['tenure'] + 1)
        
        if 'MonthlyCharges' in df.columns and 'PhoneService' in df.columns and 'InternetService' in df.columns:
            df['charges_per_service'] = df['MonthlyCharges'] / (df[['PhoneService', 'InternetService']].sum(axis=1) + 1)
        
        # Apply scaling if available
        if scaler and hasattr(model, 'predict_proba'):
            # For logistic regression
            X = scaler.transform(df)
        else:
            # For tree-based models
            X = df.values
        
        # Make predictions
        predictions = model.predict(X)
        probabilities = model.predict_proba(X)[:, 1] if hasattr(model, 'predict_proba') else None
        
        # Prepare response
        result = []
        for i in range(len(predictions)):
            pred_result = {
                'prediction': int(predictions[i]),
                'churn_probability': float(probabilities[i]) if probabilities is not None else None,
                'risk_level': 'High' if (probabilities[i] > 0.7 if probabilities is not None else predictions[i] == 1) else 
                             'Medium' if (probabilities[i] > 0.4 if probabilities is not None else False) else 'Low'
            }
            result.append(pred_result)
        
        return json.dumps(result)
        
    except Exception as e:
        return json.dumps({'error': str(e)})
'''

# Save scoring script
with open('score.py', 'w') as f:
    f.write(scoring_script)

# Create environment configuration
from azureml.core import Environment
from azureml.core.conda_dependencies import CondaDependencies

# Create environment
env = Environment(name='churn-prediction-env')
env.python.conda_dependencies = CondaDependencies.create(
    conda_packages=['scikit-learn', 'pandas', 'numpy'],
    pip_packages=['azureml-defaults', 'joblib']
)

# Create inference configuration
from azureml.core.model import InferenceConfig

inference_config = InferenceConfig(
    entry_script='score.py',
    environment=env
)

print("Inference configuration created successfully")
                        </div>
                    </div>

                    <div class="substep">
                        <div class="substep-number">2</div>
                        <div class="substep-title">Deploy to Azure Container Instances</div>
                        <p>Create real-time endpoint for model inference:</p>
                        
                        <div class="code-block">
# Cell 9: Deploy model to ACI
from azureml.core.webservice import AciWebservice, Webservice

# Configure deployment
aci_config = AciWebservice.deploy_configuration(
    cpu_cores=1,
    memory_gb=1,
    auth_enabled=True,  # Enable key-based authentication
    description='Customer churn prediction service',
    tags={
        'model': 'customer-churn-predictor',
        'version': str(model.version),
        'framework': 'scikit-learn'
    }
)

# Deploy model
service_name = 'churn-prediction-service'

try:
    # Check if service already exists
    service = Webservice(workspace=ws, name=service_name)
    print(f"Service {service_name} already exists. Updating...")
    service.update(models=[model], inference_config=inference_config)
except:
    print(f"Deploying new service {service_name}...")
    service = Model.deploy(
        workspace=ws,
        name=service_name,
        models=[model],
        inference_config=inference_config,
        deployment_config=aci_config
    )

# Wait for deployment to complete
service.wait_for_deployment(show_output=True)

print(f"Service state: {service.state}")
print(f"Service URL: {service.scoring_uri}")

# Get authentication keys
keys = service.get_keys()
print(f"Primary key: {keys[0]}")

# Cell 10: Test the deployed service
import requests

# Test data
test_data = {
    "data": [
        {
            "gender": "Female",
            "SeniorCitizen": 0,
            "Partner": "Yes",
            "Dependents": "No",
            "tenure": 1,
            "PhoneService": "No",
            "MultipleLines": "No phone service",
            "InternetService": "DSL",
            "OnlineSecurity": "No",
            "OnlineBackup": "Yes",
            "DeviceProtection": "No",
            "TechSupport": "No",
            "StreamingTV": "No",
            "StreamingMovies": "No",
            "Contract": "Month-to-month",
            "PaperlessBilling": "Yes",
            "PaymentMethod": "Electronic check",
            "MonthlyCharges": 29.85,
            "TotalCharges": "29.85"
        }
    ]
}

# Make prediction request
headers = {
    'Content-Type': 'application/json',
    'Authorization': f'Bearer {keys[0]}'
}

response = requests.post(
    service.scoring_uri,
    json=test_data,
    headers=headers
)

if response.status_code == 200:
    predictions = response.json()
    print("Prediction successful:")
    print(json.dumps(predictions, indent=2))
else:
    print(f"Error: {response.status_code}")
    print(response.text)

# Test with multiple customers
batch_test_data = {
    "data": [
        # High-risk customer
        {
            "gender": "Male", "SeniorCitizen": 1, "Partner": "No", "Dependents": "No",
            "tenure": 2, "PhoneService": "Yes", "MultipleLines": "No",
            "InternetService": "Fiber optic", "OnlineSecurity": "No", "OnlineBackup": "No",
            "DeviceProtection": "No", "TechSupport": "No", "StreamingTV": "Yes", "StreamingMovies": "Yes",
            "Contract": "Month-to-month", "PaperlessBilling": "Yes", "PaymentMethod": "Electronic check",
            "MonthlyCharges": 85.75, "TotalCharges": "171.5"
        },
        # Low-risk customer
        {
            "gender": "Female", "SeniorCitizen": 0, "Partner": "Yes", "Dependents": "Yes",
            "tenure": 48, "PhoneService": "Yes", "MultipleLines": "Yes",
            "InternetService": "Fiber optic", "OnlineSecurity": "Yes", "OnlineBackup": "Yes",
            "DeviceProtection": "Yes", "TechSupport": "Yes", "StreamingTV": "No", "StreamingMovies": "No",
            "Contract": "Two year", "PaperlessBilling": "No", "PaymentMethod": "Bank transfer (automatic)",
            "MonthlyCharges": 75.65, "TotalCharges": "3632.1"
        }
    ]
}

batch_response = requests.post(
    service.scoring_uri,
    json=batch_test_data,
    headers=headers
)

if batch_response.status_code == 200:
    batch_predictions = batch_response.json()
    print("\nBatch prediction results:")
    for i, pred in enumerate(batch_predictions):
        print(f"Customer {i+1}: {pred}")
else:
    print(f"Batch prediction error: {batch_response.status_code}")
    print(batch_response.text)
                        </div>
                    </div>

                    <div class="substep">
                        <div class="substep-number">3</div>
                        <div class="substep-title">Set Up Batch Scoring Pipeline</div>
                        <p>Create pipeline for batch predictions on large datasets:</p>
                        
                        <div class="code-block">
# Cell 11: Create batch scoring pipeline
from azureml.pipeline.core import Pipeline
from azureml.pipeline.steps import PythonScriptStep
from azureml.core.runconfig import RunConfiguration

# Create batch scoring script
batch_scoring_script = '''
import argparse
import pandas as pd
import joblib
from azureml.core import Run, Model
from azureml.core.model import Model

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--input_data', type=str, help='Input data path')
    parser.add_argument('--output_data', type=str, help='Output data path')
    args = parser.parse_args()
    
    # Get current run context
    run = Run.get_context()
    
    # Load model
    model_path = Model.get_model_path('customer-churn-predictor')
    model = joblib.load(model_path)
    
    print(f"Loading data from: {args.input_data}")
    
    # Load input data
    df = pd.read_csv(args.input_data)
    original_df = df.copy()
    
    print(f"Processing {len(df)} records...")
    
    # Preprocess data (simplified version)
    if 'customerID' in df.columns:
        customer_ids = df['customerID'].copy()
        df = df.drop(['customerID'], axis=1)
    
    # Handle missing values and data types
    if 'TotalCharges' in df.columns:
        df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')
        df['TotalCharges'].fillna(df['TotalCharges'].median(), inplace=True)
    
    # Basic preprocessing for demo (in production, use proper pipeline)
    categorical_columns = ['gender', 'Partner', 'Dependents', 'PhoneService', 'MultipleLines',
                          'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection',
                          'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract',
                          'PaperlessBilling', 'PaymentMethod']
    
    for col in categorical_columns:
        if col in df.columns:
            df[col] = pd.Categorical(df[col]).codes
    
    # Make predictions
    predictions = model.predict(df)
    probabilities = model.predict_proba(df)[:, 1] if hasattr(model, 'predict_proba') else None
    
    # Create results dataframe
    results = original_df.copy()
    results['churn_prediction'] = predictions
    if probabilities is not None:
        results['churn_probability'] = probabilities
        results['risk_level'] = pd.cut(probabilities, 
                                     bins=[0, 0.3, 0.7, 1.0], 
                                     labels=['Low', 'Medium', 'High'])
    
    # Save results
    print(f"Saving results to: {args.output_data}")
    results.to_csv(args.output_data, index=False)
    
    # Log metrics
    if probabilities is not None:
        avg_probability = probabilities.mean()
        high_risk_count = (probabilities > 0.7).sum()
        run.log('avg_churn_probability', avg_probability)
        run.log('high_risk_customers', int(high_risk_count))
        run.log('total_customers_scored', len(df))
    
    print("Batch scoring completed successfully!")

if __name__ == '__main__':
    main()
'''

# Save batch scoring script
with open('batch_score.py', 'w') as f:
    f.write(batch_scoring_script)

# Create run configuration
run_config = RunConfiguration()
run_config.environment = env

# Create pipeline step
from azureml.data import OutputFileDatasetConfig

# Define output location
output_dir = OutputFileDatasetConfig(name="batch_predictions")

batch_step = PythonScriptStep(
    script_name='batch_score.py',
    arguments=[
        '--input_data', dataset.as_named_input('input_data').as_mount(),
        '--output_data', output_dir.as_upload()
    ],
    inputs=[dataset.as_named_input('input_data')],
    outputs=[output_dir],
    compute_target='ml-training-cluster',
    runconfig=run_config,
    name="batch_churn_scoring"
)

# Create and run pipeline
batch_pipeline = Pipeline(workspace=ws, steps=[batch_step])
batch_experiment = Experiment(ws, 'batch-churn-scoring')

pipeline_run = batch_experiment.submit(batch_pipeline)
print(f"Pipeline submitted: {pipeline_run.get_portal_url()}")

# Monitor pipeline execution
pipeline_run.wait_for_completion(show_output=True)
                        </div>
                        
                        <div class="success">
                            <strong>✅ Success Indicator:</strong> Model is successfully deployed, real-time endpoint responds correctly, and batch scoring pipeline processes data without errors.
                        </div>
                    </div>
                </div>
            </div>

            <!-- Step 4 -->
            <div class="step-section">
                <div class="step-header">
                    <div class="step-title">Step 4: MLOps and Model Monitoring</div>
                    <div class="step-description">Implement model monitoring, retraining workflows, and MLOps practices</div>
                </div>
                <div class="step-content">
                    <div class="substep">
                        <div class="substep-number">1</div>
                        <div class="substep-title">Set Up Model Monitoring</div>
                        <p>Configure model performance and data drift monitoring:</p>
                        
                        <div class="code-block">
# Cell 12: Enable model monitoring
from azureml.monitoring import ModelDataCollector

# Enable data collection on deployed service
# This requires updating the scoring script to collect data

monitoring_scoring_script = '''
import json
import joblib
import numpy as np
import pandas as pd
from azureml.core.model import Model
from azureml.monitoring import ModelDataCollector

# Global variables
model = None
scaler = None
label_encoders = None
inputs_collector = None
predictions_collector = None

def init():
    global model, scaler, label_encoders, inputs_collector, predictions_collector
    
    # Load model
    model_path = Model.get_model_path('customer-churn-predictor')
    model = joblib.load(model_path)
    
    # Initialize data collectors
    inputs_collector = ModelDataCollector(
        model_name='customer-churn-predictor',
        designation='inputs',
        feature_names=[
            'gender', 'SeniorCitizen', 'Partner', 'Dependents', 'tenure',
            'PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity',
            'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV',
            'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod',
            'MonthlyCharges', 'TotalCharges'
        ]
    )
    
    predictions_collector = ModelDataCollector(
        model_name='customer-churn-predictor',
        designation='predictions',
        feature_names=['churn_prediction', 'churn_probability']
    )

def run(raw_data):
    global model, inputs_collector, predictions_collector
    
    try:
        data = json.loads(raw_data)
        df = pd.DataFrame(data)
        
        # Collect input data for monitoring
        if inputs_collector:
            inputs_collector.collect(df.values)
        
        # [Previous preprocessing code here...]
        # ... (same as before)
        
        # Make predictions
        predictions = model.predict(X)
        probabilities = model.predict_proba(X)[:, 1] if hasattr(model, 'predict_proba') else None
        
        # Collect predictions for monitoring
        if predictions_collector and probabilities is not None:
            pred_data = np.column_stack([predictions, probabilities])
            predictions_collector.collect(pred_data)
        
        # Prepare response
        result = []
        for i in range(len(predictions)):
            pred_result = {
                'prediction': int(predictions[i]),
                'churn_probability': float(probabilities[i]) if probabilities is not None else None,
                'risk_level': 'High' if (probabilities[i] > 0.7 if probabilities is not None else predictions[i] == 1) else 
                             'Medium' if (probabilities[i] > 0.4 if probabilities is not None else False) else 'Low'
            }
            result.append(pred_result)
        
        return json.dumps(result)
        
    except Exception as e:
        return json.dumps({'error': str(e)})
'''

# Save monitoring-enabled scoring script
with open('score_with_monitoring.py', 'w') as f:
    f.write(monitoring_scoring_script)

print("Monitoring-enabled scoring script created")

# Set up Application Insights for service monitoring
from azureml.core import Workspace
from azureml.monitoring import ModelDataCollector

# Enable Application Insights
service.update(enable_app_insights=True)
print("Application Insights enabled for the service")

# Create data drift monitor
from azureml.datadrift import DataDriftDetector

# Configure data drift detection
drift_detector = DataDriftDetector.create_from_datasets(
    ws, 
    name='churn-data-drift-detector',
    baseline_dataset=dataset,
    target_dataset=dataset,  # In production, this would be incoming data
    compute_target='ml-training-cluster',
    frequency='Week',
    feature_list=None,  # Monitor all features
    drift_threshold=0.3
)

print(f"Data drift detector created: {drift_detector.name}")
                        </div>
                    </div>

                    <div class="substep">
                        <div class="substep-number">2</div>
                        <div class="substep-title">Create Model Retraining Pipeline</div>
                        <p>Set up automated retraining workflow:</p>
                        
                        <div class="code-block">
# Cell 13: Create retraining pipeline
from azureml.pipeline.core import Pipeline
from azureml.pipeline.steps import PythonScriptStep
from azureml.data import OutputFileDatasetConfig

# Create retraining script
retraining_script = '''
import argparse
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, roc_auc_score
from azureml.core import Run, Dataset, Model
import joblib
import os

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--training_data', type=str, help='Training data path')
    parser.add_argument('--model_name', type=str, default='customer-churn-predictor')
    parser.add_argument('--accuracy_threshold', type=float, default=0.8)
    args = parser.parse_args()
    
    # Get run context
    run = Run.get_context()
    ws = run.experiment.workspace
    
    print("Starting model retraining...")
    
    # Load training data
    dataset = Dataset.get_by_name(ws, args.training_data)
    df = dataset.to_pandas_dataframe()
    
    print(f"Training data shape: {df.shape}")
    
    # Preprocess data (simplified)
    df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')
    df['TotalCharges'].fillna(df['TotalCharges'].median(), inplace=True)
    
    # Encode categorical variables
    categorical_columns = ['gender', 'Partner', 'Dependents', 'PhoneService', 'MultipleLines',
                          'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection',
                          'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract',
                          'PaperlessBilling', 'PaymentMethod', 'Churn']
    
    for column in categorical_columns:
        if column in df.columns:
            df[column] = pd.Categorical(df[column]).codes
    
    # Prepare features and target
    X = df.drop(['Churn', 'customerID'], axis=1, errors='ignore')
    y = df['Churn']
    
    # Split data
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    
    # Train new model
    model = RandomForestClassifier(n_estimators=100, random_state=42)
    model.fit(X_train, y_train)
    
    # Evaluate model
    y_pred = model.predict(X_test)
    y_pred_proba = model.predict_proba(X_test)[:, 1]
    
    accuracy = accuracy_score(y_test, y_pred)
    auc_score = roc_auc_score(y_test, y_pred_proba)
    
    print(f"New model accuracy: {accuracy:.4f}")
    print(f"New model AUC: {auc_score:.4f}")
    
    # Log metrics
    run.log('accuracy', accuracy)
    run.log('auc_score', auc_score)
    run.log('training_samples', len(X_train))
    run.log('test_samples', len(X_test))
    
    # Compare with existing model
    try:
        existing_model = Model(ws, args.model_name)
        existing_accuracy = float(existing_model.tags.get('accuracy', 0))
        
        print(f"Existing model accuracy: {existing_accuracy:.4f}")
        run.log('existing_model_accuracy', existing_accuracy)
        
        # Register new model if it's better
        if accuracy > existing_accuracy and accuracy >= args.accuracy_threshold:
            print("New model performs better. Registering...")
            
            # Save model
            model_filename = 'retrained_model.pkl'
            joblib.dump(model, model_filename)
            
            # Register model
            new_model = Model.register(
                workspace=ws,
                model_name=args.model_name,
                model_path=model_filename,
                description=f'Retrained customer churn model - Accuracy: {accuracy:.4f}',
                tags={
                    'accuracy': str(accuracy),
                    'auc_score': str(auc_score),
                    'framework': 'scikit-learn',
                    'retrained': 'true'
                }
            )
            
            run.log('model_registered', True)
            print(f"New model registered: {new_model.name}, Version: {new_model.version}")
        else:
            run.log('model_registered', False)
            print("New model doesn't improve performance. Keeping existing model.")
            
    except Exception as e:
        print(f"Error comparing with existing model: {e}")
        # Register as first model
        model_filename = 'retrained_model.pkl'
        joblib.dump(model, model_filename)
        
        new_model = Model.register(
            workspace=ws,
            model_name=args.model_name,
            model_path=model_filename,
            description=f'Initial customer churn model - Accuracy: {accuracy:.4f}',
            tags={
                'accuracy': str(accuracy),
                'auc_score': str(auc_score),
                'framework': 'scikit-learn'
            }
        )
        run.log('model_registered', True)
        print(f"Model registered: {new_model.name}, Version: {new_model.version}")

if __name__ == '__main__':
    main()
'''

# Save retraining script
with open('retrain_model.py', 'w') as f:
    f.write(retraining_script)

# Create retraining pipeline
retrain_step = PythonScriptStep(
    script_name='retrain_model.py',
    arguments=[
        '--training_data', 'customer-churn-data',
        '--model_name', 'customer-churn-predictor',
        '--accuracy_threshold', 0.8
    ],
    compute_target='ml-training-cluster',
    runconfig=run_config,
    name="retrain_churn_model"
)

retrain_pipeline = Pipeline(workspace=ws, steps=[retrain_step])

# Create schedule for retraining (weekly)
from azureml.pipeline.core.schedule import ScheduleRecurrence, Schedule

weekly_recurrence = ScheduleRecurrence(frequency="Week", interval=1, week_days=["Monday"], hours=[2])

retrain_schedule = Schedule.create(
    workspace=ws,
    name="weekly-churn-retraining",
    description="Weekly retraining of customer churn model",
    pipeline_id=retrain_pipeline.id,
    experiment_name="model-retraining",
    recurrence=weekly_recurrence
)

print(f"Retraining schedule created: {retrain_schedule.name}")
                        </div>
                    </div>

                    <div class="substep">
                        <div class="substep-number">3</div>
                        <div class="substep-title">Set Up Alerts and Notifications</div>
                        <p>Configure monitoring alerts for model performance:</p>
                        
                        <div class="code-block">
# Cell 14: Configure Azure Monitor alerts
# This requires Azure CLI or REST API calls

# Create alert rules for model performance
alert_config = {
    'model_accuracy_drop': {
        'description': 'Alert when model accuracy drops below threshold',
        'metric': 'accuracy',
        'threshold': 0.75,
        'comparison': 'LessThan'
    },
    'data_drift_detected': {
        'description': 'Alert when data drift is detected',
        'metric': 'data_drift_magnitude',
        'threshold': 0.3,
        'comparison': 'GreaterThan'
    },
    'prediction_volume_anomaly': {
        'description': 'Alert on unusual prediction volume',
        'metric': 'prediction_requests_per_minute',
        'threshold': 100,
        'comparison': 'GreaterThan'
    }
}

# Create notification webhook for Slack/Teams (example)
webhook_config = '''
{
    "notification_webhook": "https://hooks.slack.com/services/YOUR/SLACK/WEBHOOK",
    "alert_template": {
        "text": "🚨 Azure ML Alert: {{alert_name}}",
        "attachments": [
            {
                "color": "danger",
                "fields": [
                    {
                        "title": "Model",
                        "value": "{{model_name}}",
                        "short": true
                    },
                    {
                        "title": "Metric",
                        "value": "{{metric_name}}: {{metric_value}}",
                        "short": true
                    },
                    {
                        "title": "Timestamp",
                        "value": "{{timestamp}}",
                        "short": true
                    }
                ]
            }
        ]
    }
}
'''

print("Alert configuration template created")

# Create custom monitoring dashboard
dashboard_config = '''
Azure ML Monitoring Dashboard Components:

1. Model Performance Metrics:
   - Accuracy over time
   - AUC score trends
   - Prediction latency
   - Request volume

2. Data Quality Metrics:
   - Data drift score
   - Missing value rates
   - Feature distribution changes
   - Input data volume

3. Infrastructure Metrics:
   - Endpoint availability
   - Response times
   - Error rates
   - Resource utilization

4. Business Metrics:
   - Prediction distribution
   - High-risk customer count
   - Model confidence scores
   - Business impact metrics

5. Alerts and Actions:
   - Model retraining triggers
   - Performance degradation alerts
   - Data quality issues
   - Infrastructure problems
'''

print("Monitoring dashboard configuration:")
print(dashboard_config)

# Create model governance checklist
governance_checklist = '''
MLOps Governance Checklist:

✓ Model versioning and lineage tracking
✓ Experiment tracking and reproducibility
✓ Automated model validation
✓ A/B testing framework
✓ Model performance monitoring
✓ Data drift detection
✓ Automated retraining pipelines
✓ Model approval workflows
✓ Security and access controls
✓ Compliance and audit trails
✓ Model documentation
✓ Incident response procedures
'''

print("MLOps governance checklist:")
print(governance_checklist)

# Log completion
run_context = Run.get_context()
if hasattr(run_context, 'log'):
    run_context.log('mlops_setup_complete', True)
    run_context.log('monitoring_enabled', True)
    run_context.log('retraining_scheduled', True)

print("MLOps setup completed successfully!")
                        </div>
                        
                        <div class="warning">
                            <strong>⚠️ Production Considerations:</strong> Implement proper security, compliance, and governance practices for production ML systems.
                        </div>
                    </div>
                </div>
            </div>

            <!-- Validation Section -->
            <div class="validation-section">
                <h4>🔍 Validation & Testing</h4>
                <ol>
                    <li><strong>Workspace Functional:</strong> Verify ML workspace and compute resources are operational</li>
                    <li><strong>Data Access:</strong> Confirm datasets are registered and accessible from notebooks</li>
                    <li><strong>Model Training:</strong> Validate both AutoML and custom training complete successfully</li>
                    <li><strong>Model Registration:</strong> Verify trained models are registered with proper metadata</li>
                    <li><strong>Deployment Success:</strong> Confirm model deploys and serves predictions correctly</li>
                    <li><strong>Real-time Inference:</strong> Test endpoint with sample data and verify response format</li>
                    <li><strong>Batch Processing:</strong> Verify batch scoring pipeline processes data correctly</li>
                    <li><strong>Monitoring Active:</strong> Confirm data collection and monitoring are functional</li>
                    <li><strong>MLOps Pipeline:</strong> Validate retraining pipeline and scheduling work properly</li>
                </ol>
            </div>

            <div class="troubleshooting">
                <h4>🛠️ Common Issues & Solutions</h4>
                <ul>
                    <li><strong>Compute instance startup failures:</strong> Check quota limits, verify region availability, review network settings</li>
                    <li><strong>Model training errors:</strong> Validate data quality, check memory requirements, verify package dependencies</li>
                    <li><strong>Deployment failures:</strong> Review scoring script syntax, check environment configuration, verify resource limits</li>
                    <li><strong>Endpoint timeout issues:</strong> Optimize model size, increase memory allocation, implement caching</li>
                    <li><strong>AutoML job failures:</strong> Verify data quality, check target column, review compute resources</li>
                    <li><strong>Pipeline execution errors:</strong> Check step dependencies, verify data paths, review permissions</li>
                    <li><strong>Monitoring data collection issues:</strong> Verify collector initialization, check data formats, review service logs</li>
                    <li><strong>High compute costs:</strong> Enable auto-shutdown, optimize resource sizes, monitor usage patterns</li>
                </ul>
            </div>

            <div class="resource-links">
                <h4>📚 Additional Resources</h4>
                <ul>
                    <li><a href="https://docs.microsoft.com/en-us/azure/machine-learning/" target="_blank">Azure Machine Learning Documentation</a></li>
                    <li><a href="https://docs.microsoft.com/en-us/azure/machine-learning/how-to-auto-train-models" target="_blank">Automated Machine Learning Guide</a></li>
                    <li><a href="https://docs.microsoft.com/en-us/azure/machine-learning/how-to-deploy-and-where" target="_blank">Model Deployment Guide</a></li>
                    <li><a href="https://docs.microsoft.com/en-us/azure/machine-learning/how-to-monitor-models" target="_blank">Model Monitoring and Data Drift</a></li>
                    <li><a href="https://docs.microsoft.com/en-us/azure/machine-learning/concept-ml-pipelines" target="_blank">Machine Learning Pipelines</a></li>
                    <li><a href="https://docs.microsoft.com/en-us/azure/machine-learning/concept-mlops" target="_blank">MLOps: DevOps for Machine Learning</a></li>
                </ul>
            </div>
        </div>

        <div id="solution24" class="solution-card">
            <div class="solution-header">
                <div class="solution-title">Assignment 24: Azure IoT Hub</div>
                <div class="solution-meta">
                    <span class="meta-tag">📡 IoT</span>
                    <span class="meta-tag">⏱️ 95 minutes</span>
                    <span class="meta-tag">📊 Advanced</span>
                </div>
            </div>
            
            <div class="time-estimate">
                <strong>⏱️ Estimated Time:</strong> 95 minutes<br>
                <strong>🎯 Learning Objective:</strong> Build a complete IoT solution with device connectivity, telemetry processing, and real-time monitoring
                <br><strong>🎓 Skills Gained:</strong> IoT device management, telemetry routing, device twins, direct methods, and IoT analytics
            </div>

            <div class="prerequisites">
                <h4>📋 Prerequisites</h4>
                <ul>
                    <li>Azure subscription with Owner or Contributor access</li>
                    <li>Basic understanding of IoT concepts and JSON</li>
                    <li>Familiarity with Azure portal navigation</li>
                    <li>Understanding of messaging and event-driven architecture</li>
                </ul>
            </div>

            <div class="step-section">
                <div class="step-header">
                    <div class="step-title">Step 1: Create IoT Hub and Setup Environment</div>
                    <div class="step-description">Establish the central IoT communication hub</div>
                </div>
                <div class="step-content">
                    <div class="substep">
                        <div class="substep-number">1</div>
                        <div class="substep-title">Create Resource Group and IoT Hub</div>
                        <p>Start by creating a dedicated resource group for all IoT resources:</p>
                        
                        <div class="code-block">
# Login to Azure Portal (portal.azure.com)
# Click "Create a resource" → Search "IoT Hub"

Resource Group: IoT-Enterprise-RG
Location: East US
IoT Hub Name: enterprise-iothub-[random]
Pricing and scale tier: S1 - Standard
Number of S1 IoT Hub units: 1
Device-to-cloud partitions: 4
                        </div>
                        
                        <div class="info">
                            <strong>💡 IoT Hub Tiers:</strong> S1 Standard supports up to 400,000 messages/day and is ideal for production workloads with device management features.
                        </div>
                    </div>

                    <div class="substep">
                        <div class="substep-number">2</div>
                        <div class="substep-title">Configure IoT Hub Settings</div>
                        <p>Set up security and networking configurations:</p>
                        
                        <div class="code-block">
# In IoT Hub → Settings → Shared access policies
# Note down the connection strings for:
- iothubowner (full permissions)
- service (service connect)
- device (device connect)

# Configure IP filter (optional for security)
# Settings → Networking
Action: Allow
IP Filter Name: Office-Network
IP Range: Your-Public-IP/32
                        </div>
                    </div>

                    <div class="substep">
                        <div class="substep-number">3</div>
                        <div class="substep-title">Install Azure IoT Tools</div>
                        <p>Install necessary tools for device simulation and management:</p>
                        
                        <div class="code-block">
# Option 1: Azure CLI with IoT extension
az extension add --name azure-iot

# Option 2: Azure IoT Explorer (GUI tool)
# Download from: https://github.com/Azure/azure-iot-explorer/releases

# Option 3: VS Code Azure IoT Tools extension
# Install from VS Code marketplace
                        </div>
                    </div>
                </div>
            </div>

            <div class="step-section">
                <div class="step-header">
                    <div class="step-title">Step 2: Device Registration and Connectivity</div>
                    <div class="step-description">Register IoT devices and establish secure connections</div>
                </div>
                <div class="step-content">
                    <div class="substep">
                        <div class="substep-number">1</div>
                        <div class="substep-title">Register IoT Devices</div>
                        <p>Create multiple device identities with different authentication methods:</p>
                        
                        <div class="code-block">
# Azure CLI commands for device registration
az iot hub device-identity create \
    --device-id temperature-sensor-001 \
    --hub-name enterprise-iothub-[random] \
    --auth-method shared_private_key

az iot hub device-identity create \
    --device-id humidity-sensor-002 \
    --hub-name enterprise-iothub-[random] \
    --auth-method shared_private_key

az iot hub device-identity create \
    --device-id pressure-sensor-003 \
    --hub-name enterprise-iothub-[random] \
    --auth-method shared_private_key

# Get device connection strings
az iot hub device-identity connection-string show \
    --device-id temperature-sensor-001 \
    --hub-name enterprise-iothub-[random]
                        </div>
                        
                        <div class="info">
                            <strong>💡 Device Authentication:</strong> Symmetric keys are easiest for testing, but X.509 certificates are recommended for production environments.
                        </div>
                    </div>

                    <div class="substep">
                        <div class="substep-number">2</div>
                        <div class="substep-title">Configure Device Twins</div>
                        <p>Set up device twins for device metadata and configuration:</p>
                        
                        <div class="code-block">
# Configure device twin for temperature sensor
az iot hub device-twin update \
    --device-id temperature-sensor-001 \
    --hub-name enterprise-iothub-[random] \
    --set tags='{"location": "warehouse-a", "type": "temperature", "critical": true}' \
          properties.desired='{"telemetryInterval": 30, "temperatureThreshold": 75}'

# View device twin
az iot hub device-twin show \
    --device-id temperature-sensor-001 \
    --hub-name enterprise-iothub-[random]
                        </div>
                    </div>

                    <div class="substep">
                        <div class="substep-number">3</div>
                        <div class="substep-title">Create Device Simulator</div>
                        <p>Create a Python script to simulate IoT device telemetry:</p>
                        
                        <div class="code-block">
# device_simulator.py
import asyncio
import json
import random
import time
from azure.iot.device.aio import IoTHubDeviceClient
from azure.iot.device import Message

# Device connection string (replace with actual)
CONNECTION_STRING = "HostName=enterprise-iothub-[random].azure-devices.net;DeviceId=temperature-sensor-001;SharedAccessKey=YOUR_KEY"

async def main():
    # Initialize the device client
    device_client = IoTHubDeviceClient.create_from_connection_string(CONNECTION_STRING)
    
    # Connect to IoT Hub
    await device_client.connect()
    print("Device connected to IoT Hub")
    
    try:
        while True:
            # Generate telemetry data
            temperature = round(random.uniform(20.0, 30.0), 2)
            humidity = round(random.uniform(40.0, 80.0), 2)
            pressure = round(random.uniform(990.0, 1020.0), 2)
            
            telemetry_data = {
                "deviceId": "temperature-sensor-001",
                "timestamp": time.time(),
                "temperature": temperature,
                "humidity": humidity,
                "pressure": pressure,
                "location": "warehouse-a",
                "alert": temperature > 25
            }
            
            # Create message
            message = Message(json.dumps(telemetry_data))
            message.content_encoding = "utf-8"
            message.content_type = "application/json"
            
            # Add custom properties
            message.custom_properties["temperatureAlert"] = "true" if temperature > 25 else "false"
            message.custom_properties["deviceType"] = "sensor"
            
            # Send message
            await device_client.send_message(message)
            print(f"Sent: {telemetry_data}")
            
            # Wait before next reading
            await asyncio.sleep(30)
            
    except KeyboardInterrupt:
        print("Simulation stopped")
    finally:
        await device_client.disconnect()

if __name__ == "__main__":
    asyncio.run(main())
</div>
                    </div>
                </div>
            </div>

            <div class="step-section">
                <div class="step-header">
                    <div class="step-title">Step 3: Message Routing and Processing</div>
                    <div class="step-description">Configure intelligent message routing and processing</div>
                </div>
                <div class="step-content">
                    <div class="substep">
                        <div class="substep-number">1</div>
                        <div class="substep-title">Create Storage Account for Routing</div>
                        <p>Set up storage account for cold path analytics:</p>
                        
                        <div class="code-block">
# Create storage account
Storage Account Name: iottelemetrydata[random]
Resource Group: IoT-Enterprise-RG
Location: East US
Performance: Standard
Replication: LRS
Access tier: Hot

# Create containers
Container 1: telemetry-data
Container 2: alerts
Container 3: device-logs
                        </div>
                    </div>

                    <div class="substep">
                        <div class="substep-number">2</div>
                        <div class="substep-title">Create Event Hub for Hot Path</div>
                        <p>Set up Event Hub for real-time processing:</p>
                        
                        <div class="code-block">
# Create Event Hubs Namespace
Namespace: enterprise-eventhubs-[random]
Resource Group: IoT-Enterprise-RG
Location: East US
Pricing tier: Standard
Throughput units: 1

# Create Event Hub
Event Hub Name: iot-telemetry
Partition count: 4
Message retention: 1 day
                        </div>
                    </div>

                    <div class="substep">
                        <div class="substep-number">3</div>
                        <div class="substep-title">Configure Message Routing</div>
                        <p>Set up intelligent routing based on message properties:</p>
                        
                        <div class="code-block">
# In IoT Hub → Message routing

# Route 1: Critical Alerts to Event Hub
Route name: CriticalAlerts
Data source: Device Telemetry Messages
Routing query: temperatureAlert = 'true'
Endpoint: iot-telemetry (Event Hub)
Enabled: Yes

# Route 2: All Telemetry to Storage
Route name: AllTelemetry
Data source: Device Telemetry Messages
Routing query: true
Endpoint: telemetry-data (Storage)
Enabled: Yes

# Route 3: Device Lifecycle to Storage
Route name: DeviceLifecycle
Data source: Device Lifecycle Events
Routing query: true
Endpoint: device-logs (Storage)
Enabled: Yes
                        </div>
                        
                        <div class="warning">
                            <strong>⚠️ Route Order:</strong> Routes are evaluated in order. More specific routes should come before general ones.
                        </div>
                    </div>

                    <div class="substep">
                        <div class="substep-number">4</div>
                        <div class="substep-title">Configure Message Enrichments</div>
                        <p>Add metadata to messages before routing:</p>
                        
                        <div class="code-block">
# In IoT Hub → Message routing → Message enrichments

Enrichment 1:
Key: iotHubName
Value: $iothubname
Endpoints: All

Enrichment 2:
Key: deviceLocation
Value: $twin.tags.location
Endpoints: All

Enrichment 3:
Key: processingTime
Value: $utctimestamp
Endpoints: All
                        </div>
                    </div>
                </div>
            </div>

            <div class="step-section">
                <div class="step-header">
                    <div class="step-title">Step 4: Device Management and Monitoring</div>
                    <div class="step-description">Implement comprehensive device management capabilities</div>
                </div>
                <div class="step-content">
                    <div class="substep">
                        <div class="substep-number">1</div>
                        <div class="substep-title">Implement Direct Methods</div>
                        <p>Create device methods for remote management:</p>
                        
                        <div class="code-block">
# Enhanced device simulator with direct methods
import asyncio
import json
from azure.iot.device.aio import IoTHubDeviceClient
from azure.iot.device import MethodResponse

async def method_handler(method_request):
    """Handle direct method calls"""
    method_name = method_request.name
    
    if method_name == "reboot":
        print("Received reboot command")
        payload = {"result": "Rebooting device..."}
        status = 200
    
    elif method_name == "update_config":
        print(f"Received config update: {method_request.payload}")
        payload = {"result": "Configuration updated"}
        status = 200
    
    elif method_name == "get_status":
        payload = {
            "status": "online",
            "temperature": 23.5,
            "last_telemetry": "2024-01-15T10:30:00Z"
        }
        status = 200
    
    else:
        payload = {"error": f"Unknown method: {method_name}"}
        status = 404
    
    # Create method response
    method_response = MethodResponse.create_from_method_request(
        method_request, status, payload
    )
    
    return method_response
</div>

                        <p>Test direct methods using Azure CLI:</p>
                        
                        <div class="code-block">
az iot hub invoke-device-method \
    --device-id temperature-sensor-001 \
    --hub-name enterprise-iothub-[random] \
    --method-name reboot
</div>

                        <div class="code-block">
az iot hub invoke-device-method \
    --device-id temperature-sensor-001 \
    --hub-name enterprise-iothub-[random] \
    --method-name get_status
</div>
                    </div>

                    <div class="substep">
                        <div class="substep-number">2</div>
                        <div class="substep-title">Configure Device Monitoring</div>
                        <p>Set up comprehensive monitoring and alerting:</p>
                        
                        <div class="code-block">
# Create Action Group for alerts
Action Group Name: IoT-Alerts
Resource Group: IoT-Enterprise-RG
Short name: iotalerts

# Add email notification
Email: admin@company.com
Name: IoT Administrator

# Create Alert Rules in IoT Hub → Monitoring → Alerts

Alert 1: High Message Volume
Metric: Telemetry messages sent
Condition: Greater than 1000 messages in 5 minutes
Action Group: IoT-Alerts

Alert 2: Device Disconnection
Metric: Connected devices
Condition: Less than expected device count
Action Group: IoT-Alerts

Alert 3: Routing Failures
Metric: Routing delivery failures
Condition: Greater than 0 failures
Action Group: IoT-Alerts
                        </div>
                    </div>

                    <div class="substep">
                        <div class="substep-number">3</div>
                        <div class="substep-title">Create Device Management Dashboard</div>
                        <p>Build a monitoring dashboard for IoT operations:</p>
                        
                        <div class="code-block">
# Create Azure Dashboard
Dashboard Name: IoT Hub Operations
Resource Group: IoT-Enterprise-RG

# Add tiles:
1. IoT Hub Overview
2. Device telemetry metrics
3. Message routing success rate
4. Connected devices count
5. Error logs
6. Device twin queries results

# Pin important metrics:
- Messages sent (last 24 hours)
- Active devices
- Message routing failures
- Average message latency
                        </div>
                    </div>
                </div>
            </div>

            <div class="step-section">
                <div class="step-header">
                    <div class="step-title">Step 5: Analytics and Insights</div>
                    <div class="step-description">Extract insights from IoT telemetry data</div>
                </div>
                <div class="step-content">
                    <div class="substep">
                        <div class="substep-number">1</div>
                        <div class="substep-title">Create Stream Analytics Job</div>
                        <p>Process real-time telemetry for insights and alerts:</p>
                        
                        <div class="code-block">
# Create Stream Analytics Job
Job Name: iot-stream-analytics
Resource Group: IoT-Enterprise-RG
Location: East US
Streaming units: 1

# Configure Input
Input alias: telemetry
Source type: Event Hub
Event Hub: iot-telemetry
Serialization: JSON

# Configure Output 1: SQL Database for aggregated data
Output alias: aggregated-data
Sink type: SQL Database
Database: iot-analytics-db
Table: telemetry_summary

# Configure Output 2: Power BI for real-time dashboard
Output alias: powerbi
Sink type: Power BI
Dataset: IoT Telemetry
Table: real_time_data
                        </div>
                    </div>

                    <div class="substep">
                        <div class="substep-number">2</div>
                        <div class="substep-title">Write Stream Analytics Queries</div>
                        <p>Create queries for real-time analytics and alerting:</p>
                        
                        <div class="code-block">
-- Query 1: Real-time aggregation
SELECT 
    deviceId,
    location,
    AVG(temperature) as avg_temperature,
    MAX(temperature) as max_temperature,
    MIN(temperature) as min_temperature,
    COUNT(*) as message_count,
    System.Timestamp as window_end
INTO [aggregated-data]
FROM [telemetry]
GROUP BY 
    deviceId, 
    location,
    TumblingWindow(minute, 5)

-- Query 2: Temperature alerts
SELECT 
    deviceId,
    temperature,
    humidity,
    pressure,
    EventProcessedUtcTime,
    'HIGH_TEMPERATURE' as alert_type
INTO [powerbi]
FROM [telemetry]
WHERE temperature > 28

-- Query 3: Device health monitoring
SELECT 
    deviceId,
    COUNT(*) as message_count,
    System.Timestamp as window_end
INTO [device-health]
FROM [telemetry]
GROUP BY 
    deviceId,
    TumblingWindow(hour, 1)
HAVING COUNT(*) < 10
                        </div>
                    </div>

                    <div class="substep">
                        <div class="substep-number">3</div>
                        <div class="substep-title">Create Time Series Insights</div>
                        <p>Set up Time Series Insights for historical analysis:</p>
                        
                        <div class="code-block">
# Create Time Series Insights Environment
Environment name: iot-tsi-environment
Resource Group: IoT-Enterprise-RG
Location: East US
Pricing tier: PAYG
Time series ID: deviceId
Storage configuration: Warm store (7 days)

# Add Event Source
Event source: iot-telemetry (Event Hub)
Consumer group: tsi-consumer-group
Timestamp property: timestamp

# Configure Data Access Policies
Policy name: IoT-Analysts
Roles: Reader, Contributor
Principal: IoT team members
                        </div>
                    </div>
                </div>
            </div>

            <div class="validation-section">
                <h4>🔍 Validation Checklist</h4>
                <div class="validation-grid">
                    <div class="validation-item">
                        <span class="validation-step">✅ IoT Hub Created</span>
                        <span class="validation-desc">Hub operational with proper configuration</span>
                    </div>
                    <div class="validation-item">
                        <span class="validation-step">✅ Devices Registered</span>
                        <span class="validation-desc">Multiple devices with proper authentication</span>
                    </div>
                    <div class="validation-item">
                        <span class="validation-step">✅ Telemetry Flowing</span>
                        <span class="validation-desc">Messages being sent and received</span>
                    </div>
                    <div class="validation-item">
                        <span class="validation-step">✅ Routing Configured</span>
                        <span class="validation-desc">Messages properly routed to endpoints</span>
                    </div>
                    <div class="validation-item">
                        <span class="validation-step">✅ Device Management</span>
                        <span class="validation-desc">Direct methods and twins working</span>
                    </div>
                    <div class="validation-item">
                        <span class="validation-step">✅ Analytics Setup</span>
                        <span class="validation-desc">Stream processing and insights available</span>
                    </div>
                </div>
            </div>

            <div class="troubleshooting">
                <h4>🔧 Common Issues & Solutions</h4>
                <div class="issue-item">
                    <div class="issue-title">Device Connection Failed</div>
                    <div class="issue-solution">
                        • Verify connection string format<br>
                        • Check device registration in IoT Hub<br>
                        • Ensure IoT Hub is in correct region<br>
                        • Validate shared access key
                    </div>
                </div>
                <div class="issue-item">
                    <div class="issue-title">Messages Not Routing</div>
                    <div class="issue-solution">
                        • Check routing query syntax<br>
                        • Verify endpoint configuration<br>
                        • Test with fallback route<br>
                        • Review message properties
                    </div>
                </div>
                <div class="issue-item">
                    <div class="issue-title">High Latency</div>
                    <div class="issue-solution">
                        • Monitor IoT Hub metrics<br>
                        • Check throughput units<br>
                        • Optimize message size<br>
                        • Review partition distribution
                    </div>
                </div>
            </div>
        </div>

        <div id="solution25" class="solution-card">
            <div class="solution-header">
                <div class="solution-title">Assignment 25: Azure Kubernetes Service</div>
                <div class="solution-meta">
                    <span class="meta-tag">☸️ Kubernetes</span>
                    <span class="meta-tag">⏱️ 130 minutes</span>
                    <span class="meta-tag">📊 Expert</span>
                </div>
            </div>
            
            <div class="time-estimate">
                <strong>⏱️ Estimated Time:</strong> 130 minutes<br>
                <strong>🎯 Learning Objective:</strong> Container orchestration at scale
            </div>

            <div class="step-section">
                <div class="step-header">
                    <div class="step-title">Step 1: AKS Cluster Setup</div>
                    <div class="step-description">Deploy managed Kubernetes cluster</div>
                </div>
                <div class="step-content">
                    <div class="substep">
                        <div class="substep-number">1</div>
                        <div class="substep-title">Create AKS Cluster</div>
                        <div class="code-block">
Resource Group: Kubernetes-RG
Cluster name: enterprise-aks
Kubernetes version: Latest stable
Node count: 3
Node size: Standard_DS2_v2
Network configuration: Azure CNI
                        </div>
                    </div>
                    <div class="substep">
                        <div class="substep-number">2</div>
                        <div class="substep-title">Configure kubectl</div>
                        <div class="code-block">
az aks get-credentials \
    --resource-group Kubernetes-RG \
    --name enterprise-aks

kubectl get nodes
kubectl create namespace production
                        </div>
                    </div>
                    <div class="substep">
                        <div class="substep-number">3</div>
                        <div class="substep-title">Deploy Application</div>
                        <div class="code-block">
apiVersion: apps/v1
kind: Deployment
metadata:
    name: web-app
spec:
    replicas: 3
    selector:
        matchLabels:
            app: web-app
    template:
        spec:
            containers:
            - name: web-app
              image: nginx:latest
              ports:
              - containerPort: 80
                        </div>
                    </div>
                </div>
            </div>

            <div class="validation-section">
                <h4>🔍 Validation</h4>
                <p>✅ Cluster operational, pods running, services accessible, monitoring enabled</p>
            </div>
        </div>

        <div id="solution26" class="solution-card">
            <div class="solution-header">
                <div class="solution-title">Assignment 26: Azure Security Center</div>
                <div class="solution-meta">
                    <span class="meta-tag">🛡️ Security</span>
                    <span class="meta-tag">⏱️ 85 minutes</span>
                    <span class="meta-tag">📊 Advanced</span>
                </div>
            </div>
            
            <div class="time-estimate">
                <strong>⏱️ Estimated Time:</strong> 85 minutes<br>
                <strong>🎯 Learning Objective:</strong> Comprehensive security management
            </div>

            <div class="step-section">
                <div class="step-header">
                    <div class="step-title">Step 1: Security Configuration</div>
                    <div class="step-description">Enable advanced security features</div>
                </div>
                <div class="step-content">
                    <div class="substep">
                        <div class="substep-number">1</div>
                        <div class="substep-title">Enable Defender</div>
                        <div class="code-block">
Microsoft Defender for Cloud
Enable for all subscriptions
Enhanced security features
Pricing tier: Standard
Auto-provisioning: Enable
                        </div>
                    </div>
                    <div class="substep">
                        <div class="substep-number">2</div>
                        <div class="substep-title">Security Policies</div>
                        <div class="code-block">
Regulatory compliance
Initiative assignments
Custom policies
Policy parameters
Exemptions management
                        </div>
                    </div>
                    <div class="substep">
                        <div class="substep-number">3</div>
                        <div class="substep-title">Threat Protection</div>
                        <div class="code-block">
Just-in-time VM access
Adaptive application controls
File integrity monitoring
Security alerts review
                        </div>
                    </div>
                </div>
            </div>

            <div class="validation-section">
                <h4>🔍 Validation</h4>
                <p>✅ Security posture improved, threats detected, recommendations applied, compliance tracked</p>
            </div>
        </div>

        <div id="solution27" class="solution-card">
            <div class="solution-header">
                <div class="solution-title">Assignment 27: Azure Site Recovery</div>
                <div class="solution-meta">
                    <span class="meta-tag">🔄 DR</span>
                    <span class="meta-tag">⏱️ 100 minutes</span>
                    <span class="meta-tag">📊 Advanced</span>
                </div>
            </div>
            
            <div class="time-estimate">
                <strong>⏱️ Estimated Time:</strong> 100 minutes<br>
                <strong>🎯 Learning Objective:</strong> Disaster recovery and business continuity
            </div>

            <div class="step-section">
                <div class="step-header">
                    <div class="step-title">Step 1: Site Recovery Setup</div>
                    <div class="step-description">Configure disaster recovery solution</div>
                </div>
                <div class="step-content">
                    <div class="substep">
                        <div class="substep-number">1</div>
                        <div class="substep-title">Recovery Vault</div>
                        <div class="code-block">
Resource Group: DR-RG
Vault: enterprise-recovery
Region: West US (different from primary)
Replication policy: 24-hour RPO
Retention: 7 days
                        </div>
                    </div>
                    <div class="substep">
                        <div class="substep-number">2</div>
                        <div class="substep-title">VM Replication</div>
                        <div class="code-block">
Source: East US VMs
Target: West US
Replication settings
Compute and network mapping
Test failover execution
                        </div>
                    </div>
                    <div class="substep">
                        <div class="substep-number">3</div>
                        <div class="substep-title">Recovery Plans</div>
                        <div class="code-block">
Multi-tier application recovery
Recovery groups
Manual actions
Automation runbooks
Failover testing
                        </div>
                    </div>
                </div>
            </div>

            <div class="validation-section">
                <h4>🔍 Validation</h4>
                <p>✅ Replication active, test failover successful, recovery plan validated, RTO/RPO met</p>
            </div>
        </div>

        <div id="solution28" class="solution-card">
            <div class="solution-header">
                <div class="solution-title">Assignment 28: Azure Front Door</div>
                <div class="solution-meta">
                    <span class="meta-tag">🌐 Global LB</span>
                    <span class="meta-tag">⏱️ 80 minutes</span>
                    <span class="meta-tag">📊 Advanced</span>
                </div>
            </div>
            
            <div class="time-estimate">
                <strong>⏱️ Estimated Time:</strong> 80 minutes<br>
                <strong>🎯 Learning Objective:</strong> Global load balancing and acceleration
            </div>

            <div class="step-section">
                <div class="step-header">
                    <div class="step-title">Step 1: Front Door Configuration</div>
                    <div class="step-description">Set up global application acceleration</div>
                </div>
                <div class="step-content">
                    <div class="substep">
                        <div class="substep-number">1</div>
                        <div class="substep-title">Create Front Door</div>
                        <div class="code-block">
Resource Group: Global-RG
Front Door: enterprise-frontdoor
Frontend hosts: Custom domain
Backend pools: Multi-region web apps
Health probes: HTTP/HTTPS
                        </div>
                    </div>
                    <div class="substep">
                        <div class="substep-number">2</div>
                        <div class="substep-title">Routing Rules</div>
                        <div class="code-block">
Route patterns: /*
Forwarding protocol: HTTPS only
Backend selection: Priority-based
Session affinity: Disabled
Caching: Enabled
                        </div>
                    </div>
                    <div class="substep">
                        <div class="substep-number">3</div>
                        <div class="substep-title">WAF Protection</div>
                        <div class="code-block">
Web Application Firewall
Custom rules
Managed rule sets
Rate limiting
Geo-filtering
                        </div>
                    </div>
                </div>
            </div>

            <div class="validation-section">
                <h4>🔍 Validation</h4>
                <p>✅ Global routing working, latency reduced, WAF protecting, health probes functional</p>
            </div>
        </div>

        <div id="solution29" class="solution-card">
            <div class="solution-header">
                <div class="solution-title">Assignment 29: Azure Power BI Embedded</div>
                <div class="solution-meta">
                    <span class="meta-tag">📊 BI</span>
                    <span class="meta-tag">⏱️ 90 minutes</span>
                    <span class="meta-tag">📊 Intermediate</span>
                </div>
            </div>
            
            <div class="time-estimate">
                <strong>⏱️ Estimated Time:</strong> 90 minutes<br>
                <strong>🎯 Learning Objective:</strong> Embedded analytics and reporting
            </div>

            <div class="step-section">
                <div class="step-header">
                    <div class="step-title">Step 1: Power BI Setup</div>
                    <div class="step-description">Configure embedded analytics service</div>
                </div>
                <div class="step-content">
                    <div class="substep">
                        <div class="substep-number">1</div>
                        <div class="substep-title">Create Power BI Embedded</div>
                        <div class="code-block">
Resource Group: Analytics-RG
Power BI Embedded: enterprise-powerbi
Region: East US
Administrator: Service principal
Pricing tier: A1
Mode: Gen2
                        </div>
                    </div>
                    <div class="substep">
                        <div class="substep-number">2</div>
                        <div class="substep-title">Workspace Setup</div>
                        <div class="code-block">
Power BI workspace
Dataset creation
Report development
Dashboard configuration
Row-level security
                        </div>
                    </div>
                    <div class="substep">
                        <div class="substep-number">3</div>
                        <div class="substep-title">Application Integration</div>
                        <div class="code-block">
Embed tokens generation
JavaScript SDK integration
Authentication flow
Report embedding
Interactive features
                        </div>
                    </div>
                </div>
            </div>

            <div class="validation-section">
                <h4>🔍 Validation</h4>
                <p>✅ Reports embedded, data refreshing, security working, interactive features functional</p>
            </div>
        </div>

        <div id="solution30" class="solution-card">
            <div class="solution-header">
                <div class="solution-title">Assignment 30: Azure Arc</div>
                <div class="solution-meta">
                    <span class="meta-tag">🔗 Hybrid</span>
                    <span class="meta-tag">⏱️ 110 minutes</span>
                    <span class="meta-tag">📊 Expert</span>
                </div>
            </div>
            
            <div class="time-estimate">
                <strong>⏱️ Estimated Time:</strong> 110 minutes<br>
                <strong>🎯 Learning Objective:</strong> Hybrid and multi-cloud management
            </div>

            <div class="step-section">
                <div class="step-header">
                    <div class="step-title">Step 1: Arc Configuration</div>
                    <div class="step-description">Extend Azure services to any infrastructure</div>
                </div>
                <div class="step-content">
                    <div class="substep">
                        <div class="substep-number">1</div>
                        <div class="substep-title">Server Registration</div>
                        <div class="code-block">
Resource Group: Arc-RG
Connected machine agent installation
Server registration to Azure
Resource manager representation
Hybrid connectivity
                        </div>
                    </div>
                    <div class="substep">
                        <div class="substep-number">2</div>
                        <div class="substep-title">Policy Management</div>
                        <div class="code-block">
Guest configuration
Compliance assessment
Azure Policy for Arc servers
Security baseline
Update management
                        </div>
                    </div>
                    <div class="substep">
                        <div class="substep-number">3</div>
                        <div class="substep-title">Monitoring Integration</div>
                        <div class="code-block">
Log Analytics agent
Azure Monitor integration
Performance monitoring
Security monitoring
Hybrid operations
                        </div>
                    </div>
                </div>
            </div>

            <div class="validation-section">
                <h4>🔍 Validation</h4>
                <p>✅ Servers connected, policies applied, monitoring active, hybrid operations functional</p>
            </div>
        </div>

        <div id="solution31" class="solution-card">
            <div class="solution-header">
                <div class="solution-title">Assignment 31: Azure Lighthouse</div>
                <div class="solution-meta">
                    <span class="meta-tag">🏢 Multi-tenant</span>
                    <span class="meta-tag">⏱️ 95 minutes</span>
                    <span class="meta-tag">📊 Advanced</span>
                </div>
            </div>
            
            <div class="time-estimate">
                <strong>⏱️ Estimated Time:</strong> 95 minutes<br>
                <strong>🎯 Learning Objective:</strong> Cross-tenant management and delegation
            </div>

            <div class="step-section">
                <div class="step-header">
                    <div class="step-title">Step 1: Lighthouse Setup</div>
                    <div class="step-description">Configure delegated resource management</div>
                </div>
                <div class="step-content">
                    <div class="substep">
                        <div class="substep-number">1</div>
                        <div class="substep-title">Service Provider Setup</div>
                        <div class="code-block">
Azure AD tenant configuration
Service provider registration
Offer creation in partner center
Authorization definitions
Principal assignments
                        </div>
                    </div>
                    <div class="substep">
                        <div class="substep-number">2</div>
                        <div class="substep-title">Customer Onboarding</div>
                        <div class="code-block">
ARM template deployment
Delegated subscription access
Resource group delegation
Role-based access control
Customer consent process
                        </div>
                    </div>
                    <div class="substep">
                        <div class="substep-number">3</div>
                        <div class="substep-title">Management Operations</div>
                        <div class="code-block">
Cross-tenant resource management
Policy deployment at scale
Monitoring and compliance
Security management
Billing and cost optimization
                        </div>
                    </div>
                </div>
            </div>

            <div class="validation-section">
                <h4>🔍 Validation</h4>
                <p>✅ Delegation active, cross-tenant access working, policies applied, monitoring functional</p>
            </div>
        </div>

        <div id="solution32" class="solution-card">
            <div class="solution-header">
                <div class="solution-title">Assignment 32: Azure Cost Management</div>
                <div class="solution-meta">
                    <span class="meta-tag">💰 FinOps</span>
                    <span class="meta-tag">⏱️ 75 minutes</span>
                    <span class="meta-tag">📊 Intermediate</span>
                </div>
            </div>
            
            <div class="time-estimate">
                <strong>⏱️ Estimated Time:</strong> 75 minutes<br>
                <strong>🎯 Learning Objective:</strong> Cost optimization and financial management
            </div>

            <div class="step-section">
                <div class="step-header">
                    <div class="step-title">Step 1: Cost Management Setup</div>
                    <div class="step-description">Configure cost monitoring and optimization</div>
                </div>
                <div class="step-content">
                    <div class="substep">
                        <div class="substep-number">1</div>
                        <div class="substep-title">Budget Configuration</div>
                        <div class="code-block">
Budget scope: Subscription
Budget amount: $1000/month
Budget period: Monthly
Alert thresholds: 50%, 80%, 100%
Action groups: Email notifications
                        </div>
                    </div>
                    <div class="substep">
                        <div class="substep-number">2</div>
                        <div class="substep-title">Cost Analysis</div>
                        <div class="code-block">
Cost breakdown by service
Resource group analysis
Tag-based cost allocation
Trend analysis
Forecast projections
                        </div>
                    </div>
                    <div class="substep">
                        <div class="substep-number">3</div>
                        <div class="substep-title">Optimization Recommendations</div>
                        <div class="code-block">
Advisor cost recommendations
Right-sizing suggestions
Reserved instance opportunities
Unused resource identification
Automation for cost optimization
                        </div>
                    </div>
                </div>
            </div>

            <div class="validation-section">
                <h4>🔍 Validation</h4>
                <p>✅ Budgets configured, alerts working, cost analysis available, optimization recommendations reviewed</p>
            </div>
        </div>

        <div id="solution33" class="solution-card">
            <div class="solution-header">
                <div class="solution-title">Assignment 33: Azure Governance Framework</div>
                <div class="solution-meta">
                    <span class="meta-tag">🏛️ Governance</span>
                    <span class="meta-tag">⏱️ 120 minutes</span>
                    <span class="meta-tag">📊 Expert</span>
                </div>
            </div>
            
            <div class="time-estimate">
                <strong>⏱️ Estimated Time:</strong> 120 minutes<br>
                <strong>🎯 Learning Objective:</strong> Complete enterprise governance implementation
            </div>

            <div class="step-section">
                <div class="step-header">
                    <div class="step-title">Step 1: Management Groups and Policies</div>
                    <div class="step-description">Establish governance hierarchy and controls</div>
                </div>
                <div class="step-content">
                    <div class="substep">
                        <div class="substep-number">1</div>
                        <div class="substep-title">Management Group Structure</div>
                        <div class="code-block">
Root management group
├── Production
│   ├── Prod-Apps
│   └── Prod-Data
├── Non-Production
│   ├── Dev
│   └── Test
└── Sandbox
                        </div>
                    </div>
                    <div class="substep">
                        <div class="substep-number">2</div>
                        <div class="substep-title">Azure Policy Implementation</div>
                        <div class="code-block">
Allowed resource types
Naming conventions
Required tags enforcement
Geographic restrictions
Security baselines
Compliance frameworks
                        </div>
                    </div>
                    <div class="substep">
                        <div class="substep-number">3</div>
                        <div class="substep-title">Blueprints and Landing Zones</div>
                        <div class="code-block">
Enterprise blueprint creation
Resource templates
Policy assignments
Role assignments
Subscription deployment
Landing zone automation
                        </div>
                    </div>
                </div>
            </div>

            <div class="validation-section">
                <h4>🔍 Validation</h4>
                <p>✅ Governance structure deployed, policies enforced, compliance tracked, blueprints functional</p>
            </div>
        </div>
    </div>

    <script>
        function showSolution(solutionNumber) {
            // Hide all solution cards
            document.querySelectorAll('.solution-card').forEach(card => {
                card.classList.remove('active');
            });
            
            // Remove active class from all nav pills
            document.querySelectorAll('.nav-pill').forEach(pill => {
                pill.classList.remove('active');
            });
            
            // Show selected solution
            document.getElementById(`solution${solutionNumber}`).classList.add('active');
            
            // Add active class to clicked nav pill
            event.target.classList.add('active');
            
            // Scroll to top
            window.scrollTo({ top: 0, behavior: 'smooth' });
        }

        // Add copy functionality for code blocks
        document.addEventListener('DOMContentLoaded', function() {
            document.querySelectorAll('.code-block').forEach(block => {
                block.addEventListener('click', function() {
                    navigator.clipboard.writeText(this.textContent).then(() => {
                        const originalText = this.textContent;
                        this.style.background = '#d4edda';
                        setTimeout(() => {
                            this.style.background = '#2d3748';
                        }, 1000);
                    });
                });
                
                // Add click hint
                block.style.cursor = 'pointer';
                block.title = 'Click to copy to clipboard';
            });
        });
    </script>
</body>
</html>