Question,Scenario,OptionA,OptionB,OptionC,OptionD,CorrectAnswer,Explanation,Topic
"A data scientist at TechCorp needs to predict house prices based on historical sales data with known prices. What type of machine learning approach should they use?","TechCorp Real Estate wants to build an automated valuation model using 50,000 historical home sales with features like square footage, location, bedrooms, and sale prices.","A) Unsupervised learning with clustering","B) Supervised learning with regression","C) Reinforcement learning with rewards","D) Semi-supervised learning with partial labels","B","Since we have historical data with known target values (sale prices), this is a supervised learning problem. We're predicting continuous values (prices), making it a regression task.","ML Fundamentals"
"In the house price prediction model, which evaluation metric would be MOST appropriate for measuring model performance?","The model needs to minimize prediction errors and the business cares about both small and large prediction errors equally.","A) Accuracy and Precision","B) Mean Absolute Error (MAE) and Root Mean Square Error (RMSE)","C) F1-Score and ROC-AUC","D) Confusion Matrix and Classification Report","B","For regression problems like price prediction, MAE and RMSE are appropriate metrics. MAE gives average absolute error, while RMSE penalizes larger errors more heavily.","Evaluation Metrics"
"The real estate dataset has missing values in the 'garage_size' column (20% missing) and 'year_built' column (5% missing). What's the best preprocessing strategy?","Missing garage_size values might indicate no garage, while missing year_built values appear random and could significantly impact price predictions.","A) Drop all rows with any missing values","B) Fill garage_size with 0, year_built with median value","C) Fill all missing values with column means","D) Use forward-fill for all missing values","B","Domain knowledge suggests missing garage_size likely means no garage (0), while year_built missing values should be imputed with median to avoid skewing the distribution.","Data Preprocessing"
"For the house price model with 50,000 samples, what would be an appropriate train-validation-test split strategy?","The model needs hyperparameter tuning and final performance evaluation. The data spans 10 years (2013-2023) with potential temporal patterns.","A) Random 80-10-10 split","B) Temporal split: 2013-2019 train, 2020-2021 validation, 2022-2023 test","C) 90-5-5 split to maximize training data","D) Cross-validation only, no separate test set","B","Temporal splitting prevents data leakage and better reflects real-world deployment where the model predicts future prices based on historical data.","Train-Test Split"
"After training Linear Regression, Random Forest, and XGBoost models, the validation RMSE results are: LR: $45K, RF: $32K, XGB: $30K. Training RMSE: LR: $44K, RF: $15K, XGB: $28K. Which model should you choose?","The business prioritizes model interpretability but also wants reasonable accuracy. Production environment has latency constraints.","A) Linear Regression for best interpretability","B) Random Forest shows severe overfitting","C) XGBoost for best validation performance","D) Ensemble of all three models","C","XGBoost has the best validation RMSE ($30K) with a reasonable gap from training RMSE ($28K), indicating good generalization without severe overfitting like Random Forest.","Model Selection"
"E-commerce company wants to group customers for targeted marketing without predefined categories. They have customer data: age, income, purchase history, website behavior. What ML approach?","No existing customer segments are defined. The marketing team wants to discover natural groupings in customer behavior to create personalized campaigns.","A) Supervised classification with predefined labels","B) Unsupervised clustering (K-means or hierarchical)","C) Reinforcement learning with reward functions","D) Semi-supervised learning with some labeled customers","B","Since there are no predefined customer categories and the goal is to discover natural groupings, this is an unsupervised clustering problem.","ML Fundamentals"
"For customer clustering, how would you evaluate the quality of the clustering results?","You applied K-means with K=5 and want to validate if this is optimal and whether the clusters are well-separated and meaningful.","A) Accuracy and F1-score","B) Silhouette score and elbow method for inertia","C) Precision and recall","D) Mean squared error","B","For clustering evaluation, silhouette score measures how well-separated clusters are, while the elbow method helps determine optimal number of clusters (K).","Evaluation Metrics"
"Customer data has features: age (20-80), income ($20K-$200K), purchase_count (0-500), days_since_last_purchase (0-365). What preprocessing is needed before clustering?","K-means clustering is sensitive to feature scales. Without preprocessing, income values will dominate the distance calculations.","A) No preprocessing needed","B) Normalize/standardize all features to same scale","C) Apply log transformation only","D) Convert all features to categorical","B","K-means uses Euclidean distance, so features with larger scales (income) will dominate. Standardization ensures all features contribute equally to clustering.","Data Preprocessing"
"For fraud detection in credit card transactions, you have 1M transactions with 0.1% fraud rate. What's the primary challenge and appropriate model selection strategy?","Each transaction has 30 features (amount, merchant, location, time, etc.). False positives are costly (declined legitimate transactions), but false negatives are more costly (fraudulent transactions).","A) Class imbalance; use accuracy as primary metric","B) Class imbalance; use precision-recall and adjust threshold","C) Small dataset; use simple linear models","D) Too many features; use unsupervised learning","B","With 0.1% fraud rate, class imbalance is the main challenge. Precision-recall metrics and threshold tuning are more appropriate than accuracy for imbalanced datasets.","Model Selection"
"In fraud detection, Precision=0.85, Recall=0.75, F1=0.80. What does this mean for business impact?","Current model catches 75% of actual fraud cases. Of all transactions flagged as fraud, 85% are actually fraudulent. The business wants to minimize both missed fraud and false alarms.","A) 85% of fraudulent transactions are caught","B) 75% of fraudulent transactions are caught, 15% of flagged transactions are false alarms","C) Model has 80% accuracy","D) 85% accuracy with 75% coverage","B","Recall=0.75 means 75% of actual fraud is detected. Precision=0.85 means 15% of flagged transactions are false positives (legitimate transactions incorrectly flagged).","Evaluation Metrics"
"In a neural network for image classification, what is the purpose of the activation function in hidden layers?","Building a network to classify handwritten digits (0-9). Each image is 28x28 pixels. Without activation functions, stacked linear layers would be equivalent to a single linear layer.","A) To normalize input values","B) To introduce non-linearity and enable learning complex patterns","C) To reduce overfitting","D) To speed up training","B","Activation functions introduce non-linearity, allowing neural networks to learn complex, non-linear relationships. Without them, multiple layers would collapse to a single linear transformation.","Neural Networks"
"In forward propagation for digit classification, given input X (784 features) → Hidden Layer (128 neurons) → Output Layer (10 classes), what happens at each step?","Input: flattened 28x28 image. Hidden layer uses ReLU activation. Output layer uses softmax for probability distribution over 10 digit classes.","A) X → W1*X+b1 → ReLU → W2*H+b2 → Softmax → Probabilities","B) X → Softmax → W1*X+b1 → ReLU → Output","C) X → ReLU → Linear → Linear → Softmax","D) X → Normalize → W1*X → W2*X → Argmax","A","Forward propagation: Input X → Linear transformation (W1*X+b1) → ReLU activation → Linear transformation (W2*H+b2) → Softmax for probability distribution.","Neural Networks"
"During backpropagation in the digit classifier, gradients flow backward to update weights. If the loss is high, what happens to weight updates?","Model incorrectly predicts '3' for an image of '8' with high confidence. The cross-entropy loss is large, requiring significant weight adjustments.","A) Small weight updates due to high loss","B) Large weight updates proportional to gradient magnitude","C) No weight updates until loss decreases","D) Random weight updates regardless of loss","B","Large losses produce large gradients, leading to larger weight updates (scaled by learning rate). This helps the model correct significant errors more aggressively.","Neural Networks"
"For the 10-class digit classification problem, why is cross-entropy loss preferred over mean squared error?","Output layer produces probability distribution: [0.1, 0.05, 0.6, 0.15, 0.05, 0.02, 0.01, 0.01, 0.005, 0.005] for digit '2' (true class index 2).","A) Cross-entropy is faster to compute","B) Cross-entropy works better with softmax and provides better gradients for classification","C) MSE is only for regression problems","D) Cross-entropy requires less memory","B","Cross-entropy loss is designed for classification with softmax output. It provides better gradients and naturally handles probability distributions, while MSE can cause vanishing gradients.","Loss Functions"
"Comparing SGD, Adam, and RMSprop optimizers for training the digit classifier, which statement is most accurate?","Training on 60,000 MNIST images. SGD learning rate=0.01, Adam learning rate=0.001, RMSprop learning rate=0.001. Monitoring training loss and validation accuracy.","A) SGD always converges fastest","B) Adam adapts learning rates per parameter and often converges faster than SGD","C) RMSprop is only for RNNs","D) All optimizers perform identically","B","Adam combines momentum and adaptive learning rates per parameter, often leading to faster convergence than SGD. It maintains separate learning rates for each parameter based on historical gradients.","Optimizers"
"In PyTorch, creating a neural network for digit classification. Which architecture design choice would be most appropriate?","Using PyTorch nn.Module. Need 784 input features (28x28 pixels), hidden layers, and 10 output classes. Want to prevent overfitting on 60K training samples.","A) Single linear layer: nn.Linear(784, 10)","B) Deep network: nn.Linear(784, 512) → ReLU → Dropout → nn.Linear(512, 128) → ReLU → nn.Linear(128, 10)","C) Very wide network: nn.Linear(784, 10000) → ReLU → nn.Linear(10000, 10)","D) No hidden layers needed","B","A moderately deep network with dropout regularization can learn complex patterns while preventing overfitting. The architecture progressively reduces dimensions from 784→512→128→10.","Neural Networks"
"In PyTorch autograd, when you call loss.backward(), what exactly happens in the computational graph?","Training loop: output = model(x) → loss = criterion(output, target) → loss.backward() → optimizer.step(). The model has 3 layers with thousands of parameters.","A) Only the final layer weights are updated","B) Gradients are computed for all parameters with requires_grad=True using chain rule","C) Forward pass is repeated","D) Random gradients are assigned","B","loss.backward() uses automatic differentiation to compute gradients for all parameters with requires_grad=True by applying the chain rule through the computational graph.","Neural Networks"
"After 50 epochs of training the digit classifier: Training accuracy = 99.5%, Validation accuracy = 87%. What's happening and how to fix it?","Model: 3 hidden layers with 512 neurons each. No regularization. Learning rate = 0.001. Training loss decreasing, validation loss increasing after epoch 20.","A) Underfitting; increase model complexity","B) Overfitting; add dropout, reduce model size, or early stopping","C) Perfect performance; no changes needed","D) Bad data; collect more samples","B","Large gap between training (99.5%) and validation (87%) accuracy indicates overfitting. Solutions include dropout, smaller model, early stopping, or data augmentation.","Overfitting/Underfitting"
"To evaluate the digit classifier's real-world performance, which evaluation strategy is most comprehensive?","Have 60K training, 10K validation, 10K test samples. Model shows: Train=99%, Val=91%, Test=89%. Need to assess performance across different digit classes and error patterns.","A) Only report test accuracy (89%)","B) Confusion matrix, per-class precision/recall, and error analysis on test set","C) Only training accuracy (99%)","D) Average of train, val, test accuracies","B","Comprehensive evaluation includes confusion matrix to see which digits are confused, per-class metrics to identify problematic classes, and error analysis to understand failure modes.","Model Evaluation"
"When implementing gradient descent in PyTorch, why do you need optimizer.zero_grad() before loss.backward()?","Training loop without optimizer.zero_grad(): for batch in dataloader: output=model(x); loss=criterion(output,y); loss.backward(); optimizer.step(). Gradients seem to accumulate strangely.","A) To reset the model weights","B) To clear accumulated gradients from previous iterations","C) To normalize gradients","D) To save memory","B","PyTorch accumulates gradients by default. Without zero_grad(), gradients from previous iterations add up, leading to incorrect gradient magnitudes and poor training.","Neural Networks"
"In a CNN for image classification, what is the primary purpose of convolutional layers?","Building CNN for CIFAR-10 (32x32 RGB images, 10 classes). Need to extract features like edges, shapes, textures from images before classification.","A) To reduce the number of parameters only","B) To detect local features like edges and patterns using learnable filters","C) To flatten the image into a vector","D) To apply non-linear activation","B","Convolutional layers use learnable filters to detect local features (edges, shapes, patterns) while preserving spatial relationships and reducing parameters through weight sharing.","CNN Architecture"
"In the CIFAR-10 CNN, you have Conv2d(3, 32, kernel_size=3) followed by Conv2d(32, 64, kernel_size=3). What do these numbers represent?","Input: 32x32x3 RGB images. First conv layer needs to learn 32 different 3x3 filters from RGB input. Second layer learns 64 filters from the 32 feature maps.","A) 3 input channels, 32 output channels, 3x3 filter size; then 32 input channels, 64 output channels, 3x3 filter","B) 3 layers, 32 neurons, kernel size 3","C) 3 filters, 32 images, 3x3 pooling","D) RGB values, batch size, filter count","A","Conv2d(in_channels, out_channels, kernel_size): First layer takes 3 RGB channels, outputs 32 feature maps with 3x3 filters. Second layer takes 32 input features, outputs 64 feature maps.","CNN Architecture"
"Why do CNNs typically use MaxPooling layers between convolutional layers?","CIFAR-10 CNN architecture: Conv→ReLU→MaxPool→Conv→ReLU→MaxPool→FC. After each conv block, feature maps are getting large and computation is expensive.","A) To increase the number of parameters","B) To add non-linearity","C) To reduce spatial dimensions and provide translation invariance","D) To prevent overfitting only","C","MaxPooling reduces spatial dimensions (downsampling), decreases computation, provides translation invariance, and helps the network focus on the most important features in each region.","CNN Architecture"
"In your CIFAR-10 CNN, the final feature maps are 4x4x128. Before the fully connected layer, what operation is typically applied?","After conv layers: 4x4x128 feature maps. Need to connect to fully connected layer with 512 neurons for classification. Spatial structure is no longer needed.","A) Another convolution","B) Batch normalization","C) Flatten to convert 4x4x128 to 2048-dimensional vector","D) Average pooling only","C","Flatten operation converts the 3D feature maps (4×4×128 = 2048 values) into a 1D vector that can be fed into fully connected layers for final classification.","CNN Architecture"
"During CNN training on CIFAR-10, training accuracy reaches 95% but validation accuracy plateaus at 75%. What's the most likely issue and solution?","CNN with 5 conv layers, no regularization. Training for 100 epochs. Training loss keeps decreasing while validation loss starts increasing after epoch 30.","A) Underfitting; add more layers","B) Overfitting; add dropout, data augmentation, or reduce model complexity","C) Bad optimization; change learning rate","D) Perfect performance; no changes needed","B","Large gap between training (95%) and validation (75%) accuracy indicates overfitting. Solutions include dropout, data augmentation, batch normalization, or early stopping.","Overfitting/Underfitting"
"For data augmentation in CIFAR-10 training, which transformations would be most appropriate?","Original 32x32 images of planes, cars, birds, cats, etc. Want to increase dataset diversity while preserving class labels. Some transformations might change the class.","A) Random rotation, horizontal flip, color jittering","B) Vertical flip, 180° rotation, grayscale conversion","C) Random crop, Gaussian noise, normalization","D) All possible transformations","A","For CIFAR-10, horizontal flips and small rotations preserve class identity. Vertical flips would create unrealistic images (upside-down cars), and extreme rotations might change object orientation.","Data Preprocessing"
"In batch normalization for CNNs, what problem does it primarily solve?","Training deep CNN on CIFAR-10. Without batch norm: gradients vanish in early layers, training is slow and unstable. Learning rates need careful tuning.","A) Overfitting","B) Internal covariate shift and gradient flow problems","C) Memory usage","D) Computational cost","B","Batch normalization addresses internal covariate shift (changing distributions of layer inputs during training) and improves gradient flow, enabling faster and more stable training.","CNN Architecture"
"When fine-tuning a pre-trained ResNet-50 on CIFAR-10, what's the best strategy?","ResNet-50 pre-trained on ImageNet (1000 classes, 224x224 images). CIFAR-10 has 10 classes, 32x32 images. Limited computational resources and 5K training samples per class.","A) Train all layers from scratch with random weights","B) Freeze early layers, fine-tune later layers, replace final classifier","C) Only train the final classifier layer","D) Use identical architecture without modifications","B","Early layers learn general features (edges, shapes) that transfer well. Fine-tuning later layers and replacing the classifier allows adaptation to CIFAR-10 while leveraging pre-trained features.","Transfer Learning"
"In transfer learning, why might you use different learning rates for different parts of the network?","Fine-tuning pre-trained ResNet on CIFAR-10. Early layers have good general features, later layers need adaptation. Final classifier starts with random weights.","A) All layers should use the same learning rate","B) Pre-trained layers use lower learning rates, new layers use higher learning rates","C) Random learning rates work best","D) Learning rate doesn't matter in transfer learning","B","Pre-trained layers need small updates (low learning rate) to preserve learned features. New/fine-tuned layers need larger updates (higher learning rate) to adapt to the new task.","Transfer Learning"
"For RNN text classification, what is the vanishing gradient problem and how does LSTM address it?","Training RNN to classify movie reviews (sequences of 200-500 words). Vanilla RNN struggles to learn long-term dependencies. Gradients become very small in early time steps.","A) LSTM adds more parameters to solve vanishing gradients","B) LSTM uses gating mechanisms to control information flow and maintain long-term memory","C) LSTM uses different activation functions","D) LSTM eliminates gradients completely","B","LSTM uses forget, input, and output gates to selectively remember/forget information, allowing gradients to flow through long sequences without vanishing.","RNN/LSTM"
"In LSTM for sentiment analysis, what do the forget gate, input gate, and output gate control?","Processing movie review: 'The movie started well but the ending was disappointing.' LSTM needs to remember positive sentiment early, then update with negative sentiment.","A) Forget: old memory, Input: new information, Output: current hidden state","B) All gates control the same information","C) Gates only affect training speed","D) Gates are not important for performance","A","Forget gate decides what to discard from cell state, input gate controls what new information to store, output gate determines what parts of cell state to output as hidden state.","RNN/LSTM"
"When using LSTM for sequence-to-sequence tasks, what is teacher forcing?","Training English-to-French translation model. During training, for input 'Hello world' → 'Bonjour monde', the decoder should predict each French word sequentially.","A) Using the target sequence as decoder input during training instead of predicted tokens","B) Forcing the model to learn faster","C) Using only correct predictions","D) Training without supervision","A","Teacher forcing feeds the true target tokens as decoder input during training, providing stable gradients and faster convergence compared to using the model's own predictions.","RNN/LSTM"
"In a bidirectional LSTM for named entity recognition, why process text in both directions?","Identifying person names in: 'Mr. John Smith visited Paris.' Context from both sides helps: 'Mr.' suggests a person name follows, and 'visited' confirms John Smith is a person.","A) To double the training speed","B) To capture context from both past and future tokens for better predictions","C) To reduce memory usage","D) Bidirectional is always better","B","Bidirectional processing allows the model to use both left context (past) and right context (future) when making predictions, providing richer representations for tasks like NER.","RNN/LSTM"
"For handling variable-length sequences in RNN training, what is sequence padding and masking?","Training sentiment classifier on movie reviews with lengths: 50, 150, 300, 80 words. Batch processing requires fixed-length tensors, but we don't want padding to affect learning.","A) Padding adds special tokens to make sequences equal length, masking ignores padded positions in loss calculation","B) Padding and masking are the same thing","C) Only padding is needed","D) Variable lengths don't matter","A","Padding adds special tokens (usually 0) to make sequences the same length for batching. Masking ensures padded positions don't contribute to loss or gradient calculations.","Data Preprocessing"
"In attention mechanisms for sequence-to-sequence models, what problem does attention solve?","English-French translation: 'The quick brown fox jumps over the lazy dog' → 'Le renard brun rapide saute par-dessus le chien paresseux.' Standard seq2seq compresses all input info into final hidden state.","A) Attention speeds up training","B) Attention allows the decoder to focus on relevant parts of the input sequence for each output token","C) Attention reduces memory usage","D) Attention eliminates the need for RNNs","B","Attention allows the decoder to dynamically focus on different parts of the input sequence when generating each output token, solving the information bottleneck of fixed-size context vectors.","Attention Mechanism"
"In self-attention (as used in Transformers), how are queries, keys, and values computed?","Processing sentence: 'The cat sat on the mat.' Each word needs to attend to all other words to understand relationships and context.","A) Q, K, V are predefined matrices","B) Q, K, V are learned linear projections of the input embeddings","C) Q, K, V are the same as input embeddings","D) Only queries matter","B","Self-attention computes Q=XW_Q, K=XW_K, V=XW_V where X is input embeddings and W matrices are learned parameters, allowing flexible attention patterns.","Attention Mechanism"
"What is the main advantage of Transformers over RNNs for sequence processing?","Training on large datasets (millions of sentences) for language modeling. RNNs process sequentially (word by word), while Transformers can process all positions simultaneously.","A) Transformers use less memory","B) Transformers enable parallel processing of sequences and better capture long-range dependencies","C) Transformers are simpler to implement","D) RNNs are always better","B","Transformers process all sequence positions in parallel (vs. sequential RNN processing) and use attention to directly connect distant positions, enabling better parallelization and long-range modeling.","Transformers"
"In the Transformer architecture, what is the purpose of positional encodings?","Processing sentence: 'John loves Mary' vs. 'Mary loves John.' Word order completely changes meaning, but self-attention is permutation-invariant without position information.","A) To add randomness","B) To provide position information since attention is permutation-invariant","C) To reduce overfitting","D) Positional encoding is optional","B","Self-attention doesn't inherently understand word order. Positional encodings add position information to embeddings, allowing the model to distinguish between different word orders.","Transformers"
"In multi-head attention, why use multiple attention heads instead of single attention?","Analyzing sentence: 'The bank by the river was steep.' Multiple attention heads can focus on different relationships: syntactic (bank-steep), semantic (bank-river), etc.","A) Multiple heads reduce computation","B) Multiple heads allow the model to attend to different types of relationships simultaneously","C) Single head is always sufficient","D) Multiple heads prevent overfitting","B","Multiple attention heads allow the model to capture different types of relationships (syntactic, semantic, positional) in parallel, providing richer representations than single attention.","Transformers"
"What is the difference between encoder-only, decoder-only, and encoder-decoder Transformer architectures?","Three tasks: document classification (BERT-style), text generation (GPT-style), and machine translation (T5-style). Each requires different architectural choices.","A) No significant differences","B) Encoder-only for classification, decoder-only for generation, encoder-decoder for seq2seq tasks","C) All architectures work equally for all tasks","D) Only decoder-only models are useful","B","Encoder-only (BERT): bidirectional, good for classification. Decoder-only (GPT): causal/autoregressive, good for generation. Encoder-decoder: best for translation and seq2seq tasks.","Transformers"
"In BERT training, what is the masked language modeling (MLM) objective?","Pre-training BERT on large text corpus. Input: 'The [MASK] jumped over the fence.' BERT needs to predict the masked word using bidirectional context.","A) Predicting the next word in sequence","B) Predicting masked words using bidirectional context","C) Classifying sentence sentiment","D) Translating between languages","B","MLM randomly masks 15% of input tokens and trains the model to predict them using bidirectional context, enabling BERT to learn rich bidirectional representations.","BERT/GPT"
"What is the key difference between BERT and GPT in terms of training objectives?","BERT sees full context when predicting masked words: 'The cat [MASK] on the mat.' GPT only sees left context when predicting next word: 'The cat' → predict 'sat'.","A) BERT and GPT use identical training","B) BERT uses bidirectional masked LM, GPT uses unidirectional autoregressive LM","C) BERT is only for classification","D) GPT cannot be fine-tuned","B","BERT uses masked language modeling with bidirectional context. GPT uses autoregressive language modeling, predicting next tokens using only left context.","BERT/GPT"
"When fine-tuning BERT for text classification, what modifications are typically made?","Fine-tuning BERT for movie review sentiment classification. BERT outputs contextual embeddings for each token, but need single classification decision.","A) Replace all BERT layers","B) Add a classification head on top of [CLS] token representation","C) Only use the last layer","D) Freeze all BERT parameters","B","Add a linear classification layer that takes the [CLS] token's final representation as input. The [CLS] token is designed to represent the entire sequence for classification tasks.","BERT/GPT"
"In GPT, what does the autoregressive generation process look like?","Generating text with GPT starting with prompt: 'The weather today is'. Model needs to generate continuation word by word while maintaining coherence.","A) Generate all words simultaneously","B) Generate words sequentially, using previous tokens as context for next token prediction","C) Generate random words","D) Copy from training data","B","GPT generates text autoregressively: predict next token given all previous tokens, then add predicted token to sequence and repeat. Each prediction uses full left context.","BERT/GPT"
"What is the difference between GPT-1, GPT-2, and GPT-3 in terms of scale and capabilities?","Evolution from GPT-1 (117M parameters) to GPT-3 (175B parameters). Each version shows emergent capabilities with scale increases.","A) Only architectural differences","B) Primarily scale differences: GPT-1 (117M) → GPT-2 (1.5B) → GPT-3 (175B) parameters, with emergent abilities","C) Only training data differences","D) No significant differences","B","The GPT series primarily differs in scale: GPT-1 (117M), GPT-2 (1.5B), GPT-3 (175B) parameters. Larger models show emergent capabilities like few-shot learning and reasoning.","BERT/GPT"
"In word embeddings, what does the distributional hypothesis suggest?","Training word embeddings on large corpus. Words 'car' and 'automobile' appear in similar contexts: 'drive a ___', 'park the ___', '_ mechanic'.","A) Words with similar meanings appear in similar contexts","B) Word order doesn't matter","C) All words are equally similar","D) Context is irrelevant","A","Distributional hypothesis states that words appearing in similar contexts tend to have similar meanings. This principle underlies word embedding methods like Word2Vec and GloVe.","Word Embeddings"
"What is the main difference between Word2Vec's CBOW and Skip-gram models?","Training embeddings on sentence: 'The quick brown fox jumps.' CBOW uses context words to predict target. Skip-gram uses target word to predict context.","A) CBOW and Skip-gram are identical","B) CBOW predicts target from context, Skip-gram predicts context from target","C) CBOW is only for classification","D) Skip-gram doesn't use neural networks","B","CBOW (Continuous Bag of Words) predicts the target word from context words. Skip-gram predicts context words from the target word. Skip-gram works better for rare words.","Word Embeddings"
"In GloVe embeddings, what information is captured by the co-occurrence matrix?","Building embeddings from news corpus. Words 'president' and 'election' frequently appear together. 'Cat' and 'election' rarely co-occur in the same context windows.","A) Alphabetical order of words","B) Global word co-occurrence statistics across the entire corpus","C) Word frequencies only","D) Random associations","B","GloVe constructs a global word-word co-occurrence matrix capturing how often words appear together across the entire corpus, then factorizes it to learn embeddings.","Word Embeddings"
"What are the limitations of static word embeddings like Word2Vec compared to contextual embeddings?","Word 'bank' in two sentences: 'I deposited money at the bank' vs. 'The river bank was muddy.' Word2Vec gives same embedding for 'bank' in both contexts.","A) Static embeddings are always better","B) Static embeddings assign same vector to words regardless of context, missing polysemy","C) No significant differences","D) Contextual embeddings are always worse","B","Static embeddings assign fixed vectors to words, missing context-dependent meanings (polysemy). Contextual embeddings (like BERT) provide different representations based on context.","Word Embeddings"
"In contextual embeddings (like those from BERT), how do representations differ from static embeddings?","Processing 'bank' in different contexts with BERT: 'financial bank' vs. 'river bank'. BERT generates different embeddings based on surrounding context.","A) Contextual embeddings are random","B) Contextual embeddings provide different representations for the same word in different contexts","C) Contextual embeddings are identical to static embeddings","D) Context doesn't matter","B","Contextual embeddings generate different vector representations for the same word based on its context, capturing polysemy and context-dependent meanings that static embeddings miss.","Word Embeddings"
"What is a language model and how is it typically evaluated?","Training a language model to predict the next word in sequences like: 'The capital of France is ___'. Model assigns probability distributions over vocabulary.","A) Language models only do classification","B) Language models predict probability distributions over next tokens, evaluated using perplexity","C) Language models don't need evaluation","D) Only accuracy matters","B","Language models predict probability distributions over next tokens given context. Perplexity measures how well the model predicts a test set - lower perplexity indicates better predictive performance.","Language Models"
"In n-gram language models, what is the Markov assumption and why is it limiting?","Bigram model for 'The cat sat on the' predicts next word only from 'the'. Doesn't consider that the sentence started with 'cat sat', losing important context.","A) N-gram models use infinite context","B) Markov assumption limits context to n-1 previous tokens, missing long-range dependencies","C) Markov assumption is always optimal","D) Context doesn't matter in language modeling","B","N-gram models assume independence beyond n-1 tokens (Markov assumption), missing long-range dependencies. Neural language models can capture longer contexts.","Language Models"
"How do neural language models (like RNN-based or Transformer-based) improve over n-gram models?","Comparing models on sentence completion: 'Although the weather was terrible, the concert was ___'. N-gram misses long-range 'weather terrible' → 'concert good' relationship.","A) Neural models use larger vocabularies","B) Neural models can capture longer dependencies and learn distributed representations","C) N-gram models are always better","D) No significant differences","B","Neural language models can capture longer dependencies through hidden states/attention and learn dense distributed representations, overcoming the sparse, limited-context nature of n-gram models.","Language Models"
"What is the purpose of temperature in language model sampling?","GPT generating text with different temperature values: T=0.1 produces very predictable text, T=2.0 produces more random, creative text.","A) Temperature controls model size","B) Temperature controls randomness in sampling - lower T more deterministic, higher T more random","C) Temperature doesn't affect generation","D) Temperature only affects training","B","Temperature scales the logits before softmax sampling. Low temperature (T→0) makes sampling more deterministic (greedy), high temperature makes it more random and creative.","Language Models"
"In language model evaluation, what does perplexity measure and how does it relate to bits per character?","Model A: perplexity=50, Model B: perplexity=25 on same test set. Lower perplexity indicates better predictive performance.","A) Perplexity measures model size","B) Perplexity measures how surprised the model is by the test data - lower is better, related to compression efficiency","C) Higher perplexity is always better","D) Perplexity is unrelated to model quality","B","Perplexity measures average surprise - how uncertain the model is about predicting test data. Lower perplexity = better model. Related to compression: lower perplexity = fewer bits needed per character.","Language Models"
"What is Retrieval-Augmented Generation (RAG) and why is it useful?","Question-answering system needs to answer: 'What was the GDP of Japan in 2023?' The base language model was trained on data through 2021 and lacks recent information.","A) RAG only retrieves information","B) RAG combines retrieval of relevant documents with language model generation for up-to-date, factual responses","C) RAG replaces language models entirely","D) RAG is only for classification","B","RAG retrieves relevant documents from a knowledge base and provides them as context to a language model for generation, enabling access to current information beyond training data.","RAG Systems"
"In a RAG system, what is the typical pipeline for answering a user query?","User asks: 'What are the latest developments in quantum computing?' System needs to find relevant documents and generate an informed response.","A) Generate answer directly without retrieval","B) Query → retrieve relevant documents → augment prompt with documents → generate answer","C) Only retrieve documents without generation","D) Random document selection","B","RAG pipeline: 1) Encode user query, 2) Retrieve relevant documents using similarity search, 3) Augment the prompt with retrieved documents, 4) Generate answer using the augmented context.","RAG Systems"
"What role do vector databases play in RAG systems?","RAG system has 1M documents about various topics. User query 'climate change effects' needs to quickly find the most relevant documents for context.","A) Vector databases store only text","B) Vector databases store document embeddings for fast similarity search and retrieval","C) Vector databases are not needed in RAG","D) Vector databases only store metadata","B","Vector databases store high-dimensional embeddings of documents, enabling fast similarity search. When a query comes in, its embedding is compared to find the most relevant document embeddings.","Vector Databases"
"In vector similarity search, what are the trade-offs between exact and approximate methods?","Searching through 10M document embeddings (768 dimensions each) for query 'machine learning algorithms'. Need balance between speed and accuracy.","A) Exact search is always faster","B) Exact search is accurate but slow for large datasets; approximate methods (like LSH, FAISS) trade some accuracy for speed","C) Approximate methods are always more accurate","D) No trade-offs exist","B","Exact search guarantees finding the true nearest neighbors but is computationally expensive. Approximate methods (LSH, FAISS, HNSW) sacrifice some accuracy for significant speed improvements on large datasets.","Vector Databases"
"What is the difference between dense and sparse retrieval in RAG systems?","Two retrieval approaches for query 'neural network architectures': sparse uses TF-IDF/BM25 matching 'neural', 'network', 'architectures'. Dense uses embedding similarity.","A) Dense and sparse are identical","B) Sparse uses exact term matching (TF-IDF, BM25), dense uses semantic similarity in embedding space","C) Dense is always better","D) Sparse is always better","B","Sparse retrieval uses exact term matching (TF-IDF, BM25) and works well for keyword-based queries. Dense retrieval uses semantic embeddings and captures conceptual similarity beyond exact terms.","RAG Systems"
"How can you evaluate the performance of a RAG system?","RAG system answers questions about company policies. Need to measure both retrieval quality (finding right documents) and generation quality (accurate, helpful answers).","A) Only measure generation quality","B) Evaluate both retrieval metrics (precision, recall) and generation metrics (factual accuracy, relevance)","C) Only measure retrieval quality","D) Random evaluation is sufficient","B","RAG evaluation requires: 1) Retrieval metrics (precision@k, recall, MRR) measuring if relevant documents are found, 2) Generation metrics (factual accuracy, relevance, fluency) measuring answer quality.","RAG Systems"
"What is the Model Context Protocol (MCP) and what problem does it solve?","AI assistant needs to access multiple tools: file system, databases, APIs, calculators. Each tool has different interfaces and connection methods.","A) MCP is only for training models","B) MCP provides a standardized protocol for AI models to securely connect to and interact with various tools and data sources","C) MCP replaces all AI models","D) MCP is only for web browsing","B","MCP standardizes how AI models connect to external tools and resources (files, databases, APIs), providing a secure, consistent interface across different applications and environments.","MCP Protocol"
"In MCP, what are the key components: clients, servers, and tools?","AI application (client) wants to use a code editor (server) that provides tools like file reading, writing, and execution. Need standardized communication protocol.","A) All components are identical","B) Clients are AI applications, servers provide capabilities, tools are specific functions exposed by servers","C) Only clients matter","D) Components are randomly assigned","B","MCP architecture: Clients (AI applications) connect to Servers (capability providers) which expose Tools (specific functions like file operations, API calls) through a standardized protocol.","MCP Protocol"
"What are the security considerations when implementing MCP servers?","MCP server provides file system access to AI applications. Need to prevent unauthorized access to sensitive files while allowing legitimate operations.","A) Security is not important","B) Implement authentication, authorization, sandboxing, and audit logging for safe tool access","C) Allow unrestricted access","D) Security is automatic","B","MCP security requires: authentication (verify client identity), authorization (control access permissions), sandboxing (limit tool capabilities), and audit logging (track all operations).","MCP Protocol"
"How does MCP enable composability of AI tools and services?","Building AI assistant that needs: web search + document analysis + code execution + email sending. Each capability is provided by different MCP servers.","A) MCP prevents tool combination","B) MCP allows AI applications to connect to multiple specialized servers, combining different capabilities seamlessly","C) Only single tools are supported","D) Composability is not possible","B","MCP enables composability by allowing clients to connect to multiple servers simultaneously, combining different tools (search, analysis, execution) into comprehensive AI workflows.","MCP Protocol"
"What are the benefits of standardizing AI tool interfaces through MCP?","Currently: each AI tool has custom APIs. Developers must learn multiple interfaces. Tools can't easily work together across different AI applications.","A) Standardization creates more complexity","B) Standardization reduces integration effort, improves interoperability, and enables tool reuse across applications","C) Custom interfaces are always better","D) Standardization is unnecessary","B","MCP standardization reduces development effort (learn once, use everywhere), improves interoperability between tools, and enables tool ecosystem growth through consistent interfaces.","MCP Protocol"
"When deploying machine learning models to production, what are the key considerations for model serving?","Trained image classification model needs to serve 1000 requests/second with <100ms latency. Model file is 500MB. Need reliability and cost efficiency.","A) Only accuracy matters in production","B) Consider latency, throughput, resource usage, scalability, monitoring, and cost","C) Deployment is identical to training","D) Model performance doesn't matter","B","Production serving requires: low latency, high throughput, efficient resource usage, auto-scaling, health monitoring, error handling, and cost optimization while maintaining model accuracy.","LLM Deployment"
"What is the difference between batch and real-time inference in ML deployment?","E-commerce company needs: 1) Product recommendations for website users (immediate), 2) Monthly customer segmentation analysis (can wait hours).","A) Batch and real-time are identical","B) Real-time serves individual requests immediately, batch processes large datasets offline","C) Batch is always faster","D) Real-time is always better","B","Real-time inference serves individual requests with low latency requirements. Batch inference processes large datasets efficiently offline when immediate response isn't needed.","LLM Deployment"
"How do you handle model versioning and rollback in production ML systems?","Deployed model v1.2 serving traffic. Model v1.3 shows better accuracy in testing but causes increased latency in production. Need to safely deploy and potentially rollback.","A) Always deploy immediately without testing","B) Use canary deployments, A/B testing, and maintain rollback capabilities for safe model updates","C) Never update models","D) Rollback is not necessary","B","Safe model deployment uses: canary releases (gradual traffic shift), A/B testing (compare performance), blue-green deployments, and automated rollback procedures for production stability.","LLM Deployment"
"What are the challenges and solutions for serving large language models (LLMs) in production?","Serving 70B parameter model: high memory requirements, expensive GPU costs, variable request lengths, potential generation latency issues.","A) LLMs don't need special considerations","B) Use model quantization, batching, caching, distributed serving, and efficient hardware for large model deployment","C) LLMs cannot be deployed","D) Only small models work in production","B","LLM serving challenges: memory requirements, latency, cost. Solutions include: quantization (reduce precision), dynamic batching, KV caching, distributed inference, and specialized hardware.","LLM Deployment"
"How do you monitor and maintain model performance in production?","Deployed fraud detection model initially had 90% precision. After 6 months, precision dropped to 75%. Model hasn't changed, but fraud patterns evolved.","A) Model monitoring is unnecessary","B) Monitor data drift, model drift, performance metrics, and implement retraining pipelines","C) Models never degrade","D) Manual monitoring is sufficient","B","Production ML monitoring requires: data drift detection (input distribution changes), model drift monitoring (performance degradation), automated alerts, and retraining pipelines to maintain performance.","Production ML"
"What is A/B testing in the context of ML model deployment and why is it important?","Testing new recommendation algorithm: 50% users see recommendations from model A, 50% from model B. Measuring click-through rates to determine which performs better.","A) A/B testing is only for web design","B) A/B testing compares model performance on real user traffic to make data-driven deployment decisions","C) A/B testing slows down deployment","D) Random assignment is not important","B","A/B testing splits real traffic between model versions to compare performance metrics (accuracy, user engagement, business KPIs) and make evidence-based decisions about model deployment.","Production ML"
"How do you handle concept drift in production ML systems?","Customer behavior prediction model trained pre-COVID shows declining accuracy. Shopping patterns, economic factors, and preferences have shifted significantly.","A) Concept drift is not a real problem","B) Detect distribution changes, retrain models on recent data, and implement adaptive learning systems","C) Old models always work forever","D) Manual updates are sufficient","B","Concept drift occurs when the relationship between features and targets changes. Solutions: drift detection algorithms, automated retraining on recent data, and adaptive learning systems.","Production ML"
"What are the security considerations for ML models in production?","Deployed image classification API: adversarial examples could fool the model, training data might contain sensitive information, model could be stolen through API queries.","A) ML models have no security risks","B) Protect against adversarial attacks, data privacy breaches, model stealing, and implement secure API design","C) Security is only for traditional software","D) Open access is always better","B","ML security includes: adversarial robustness (defend against crafted inputs), data privacy (protect training data), model protection (prevent theft), and secure API design.","Production ML"
"In AutoML systems, what aspects of the machine learning pipeline can be automated?","Data scientist spends weeks on: feature engineering, model selection, hyperparameter tuning, architecture search. Want to automate repetitive tasks while maintaining quality.","A) AutoML cannot automate anything","B) AutoML can automate feature engineering, model selection, hyperparameter optimization, and architecture search","C) Only hyperparameters can be automated","D) Automation reduces model quality","B","AutoML automates: data preprocessing, feature engineering, model selection, hyperparameter optimization, neural architecture search, and model evaluation - reducing manual effort while maintaining performance.","Production ML"
"What is the difference between supervised, unsupervised, and reinforcement learning paradigms?","Three scenarios: 1) Email spam detection with labeled examples, 2) Customer segmentation without predefined groups, 3) Game-playing AI learning from wins/losses.","A) All paradigms are identical","B) Supervised uses labeled data, unsupervised finds patterns in unlabeled data, RL learns from rewards/penalties","C) Only supervised learning works","D) Labels are always available","B","Supervised learning uses input-output pairs. Unsupervised learning discovers patterns in data without labels. Reinforcement learning learns optimal actions through reward/penalty feedback.","ML Fundamentals"
"What is cross-validation and why is it important for model evaluation?","Training sentiment classifier on 10K reviews. Single train-test split might by chance put all positive reviews in training set and negative in test set.","A) Cross-validation is unnecessary","B) Cross-validation provides more robust performance estimates by training/testing on multiple data splits","C) Single split is always sufficient","D) Cross-validation reduces accuracy","B","Cross-validation trains/tests on multiple data splits, providing more reliable performance estimates, reducing variance from random splits, and better detecting overfitting.","Model Evaluation"
"What is the bias-variance tradeoff in machine learning?","Comparing simple linear regression vs. complex neural network on house price prediction: Linear model underfits (high bias), complex model overfits to training noise (high variance).","A) Bias and variance are unrelated","B) High bias models underfit, high variance models overfit; optimal models balance both","C) High bias is always better","D) High variance is always better","B","Bias-variance tradeoff: High bias (underfitting) misses true patterns. High variance (overfitting) captures noise. Optimal models balance both for best generalization.","ML Fundamentals"
"How do you choose between different machine learning algorithms for a given problem?","Problem: predict customer churn with 100K samples, 50 features, mix of categorical and numerical data. Need interpretable model with good performance.","A) Always use the most complex algorithm","B) Consider data size, feature types, interpretability needs, performance requirements, and computational constraints","C) Random selection works best","D) Only neural networks should be used","B","Algorithm selection depends on: dataset size, feature types, interpretability requirements, performance needs, computational budget, and training time constraints.","Model Selection"
"What is feature engineering and why is it important?","Raw e-commerce data: user_age=25, purchase_date='2024-01-15', item_price=49.99. Want to predict repeat purchases but need better features.","A) Raw features are always optimal","B) Feature engineering creates more informative features from raw data to improve model performance","C) Feature engineering is unnecessary","D) More features always help","B","Feature engineering transforms raw data into informative features: age groups, days_since_purchase, price_bins, interaction features - often improving model performance more than algorithm choice.","Data Preprocessing"
"What is regularization and how does it help prevent overfitting?","Training polynomial regression: without regularization, model perfectly fits training data (including noise) but fails on validation set.","A) Regularization increases overfitting","B) Regularization adds penalty terms to loss function to constrain model complexity and improve generalization","C) Regularization is only for neural networks","D) Regularization reduces training accuracy","B","Regularization (L1, L2, dropout) adds penalties for model complexity, constraining weights to prevent overfitting and improve generalization to unseen data.","Overfitting/Underfitting"
"How do you handle imbalanced datasets in classification?","Credit card fraud detection: 99.9% normal transactions, 0.1% fraud. Standard accuracy metric and equal class weights will bias toward majority class.","A) Imbalanced data is not a problem","B) Use appropriate metrics (precision/recall), resampling techniques, cost-sensitive learning, or ensemble methods","C) Ignore minority class","D) Only accuracy matters","B","Imbalanced data solutions: use precision/recall/F1 metrics, oversample minority class (SMOTE), undersample majority class, cost-sensitive learning, or ensemble methods.","Model Selection"
"What is ensemble learning and when is it beneficial?","Individual models: Decision Tree (85% accuracy), SVM (87% accuracy), Random Forest (89% accuracy). Combining them might achieve better performance than any single model.","A) Ensemble learning always reduces performance","B) Ensemble learning combines multiple models to achieve better performance than individual models","C) Only single models should be used","D) Ensemble learning is too complex","B","Ensemble learning combines multiple models (voting, bagging, boosting) to reduce variance, bias, and improve generalization beyond individual model performance.","Ensemble Methods"
"What is the difference between bagging and boosting ensemble methods?","Random Forest (bagging): trains multiple trees on different data subsets independently. AdaBoost (boosting): trains models sequentially, focusing on previous errors.","A) Bagging and boosting are identical","B) Bagging trains models independently in parallel, boosting trains models sequentially focusing on errors","C) Boosting is always better","D) Bagging is always better","B","Bagging (Bootstrap AGGregating) trains models independently on different data subsets. Boosting trains models sequentially, with each model focusing on previous models' errors.","Ensemble Methods"
"How do you evaluate and interpret machine learning model results?","Binary classifier results: TP=850, TN=9500, FP=100, FN=50. Need to understand model performance and business impact.","A) Only accuracy matters","B) Use multiple metrics (precision, recall, F1, ROC-AUC), confusion matrix, and consider business impact","C) Single metric is sufficient","D) Evaluation is unnecessary","B","Comprehensive evaluation uses: accuracy, precision, recall, F1-score, ROC-AUC, confusion matrix, and business-specific metrics to understand model performance and impact.","Model Evaluation"